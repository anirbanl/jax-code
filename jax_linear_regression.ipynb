{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jax_linear_regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnxmYO4QW_qV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "892fc4ae-e044-4d1f-e3b6-ea0f8dea091b"
      },
      "source": [
        "# TF CODE\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\n",
        "x_data = np.random.rand(100).astype(np.float32)\n",
        "y_data = x_data * 0.1 + 0.3\n",
        "\n",
        "# Try to find values for W and b that compute y_data = W * x_data + b\n",
        "# (We know that W should be 0.1 and b 0.3, but Tensorflow will\n",
        "# figure that out for us.)\n",
        "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
        "b = tf.Variable(tf.zeros([1]))\n",
        "y = W * x_data + b\n",
        "\n",
        "# Minimize the mean squared errors.\n",
        "loss = tf.reduce_mean(tf.square(y - y_data))\n",
        "optimizer = tf.train.AdamOptimizer(0.5)\n",
        "train = optimizer.minimize(loss)\n",
        "\n",
        "# Before starting, initialize the variables.  We will 'run' this first.\n",
        "init = tf.initialize_all_variables()\n",
        "\n",
        "# Launch the graph.\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "\n",
        "# Fit the line.\n",
        "for step in range(201):\n",
        "    sess.run(train)\n",
        "    if step % 20 == 0:\n",
        "        print(step, sess.run(W), sess.run(b))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [0.60629356] [0.49999976]\n",
            "20 [0.01879729] [0.27725616]\n",
            "40 [0.07610643] [0.27667654]\n",
            "60 [0.11405155] [0.30311984]\n",
            "80 [0.10251226] [0.30413422]\n",
            "100 [0.10005254] [0.30157742]\n",
            "120 [0.10037202] [0.30064023]\n",
            "140 [0.0999805] [0.29993704]\n",
            "160 [0.10001576] [0.29996943]\n",
            "180 [0.10000985] [0.29998872]\n",
            "200 [0.10001373] [0.30000582]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHLqB2HcXpt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from jax import jit, grad, vmap, random\n",
        "from functools import partial\n",
        "import jax\n",
        "import jax.numpy as np\n",
        "from jax.experimental import stax # neural network library\n",
        "from jax.experimental.stax import Conv, Dense, MaxPool, Relu, Flatten, LogSoftmax, LeakyRelu, Dropout # neural network layers\n",
        "import matplotlib.pyplot as plt # visualization\n",
        "import numpy as onp\n",
        "from jax.experimental import optimizers\n",
        "from jax.tree_util import tree_multimap  # Element-wise manipulation of collections of numpy arrays"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57wMm-6CX1o-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generate data\n",
        "rng = random.PRNGKey(1)\n",
        "x_data = random.normal(rng, (100,1))\n",
        "y_data = x_data * 0.1 + 0.3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIyKl8DLZRVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define model\n",
        "net_init, net_apply = stax.serial(Dense(1))\n",
        "in_shape = (-1, 1,)\n",
        "out_shape, net_params = net_init(rng, in_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MwoVbnuZiUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define losses and optimizers\n",
        "def loss(params, inputs, targets):\n",
        "    # Computes average loss for the batch\n",
        "    predictions = net_apply(params, inputs)\n",
        "    return np.mean((targets - predictions)**2)\n",
        "\n",
        "opt_init, opt_update, get_params = optimizers.adam(step_size=0.5)  # this LR seems to be better than 1e-2 and 1e-4\n",
        "out_shape, net_params = net_init(rng, in_shape)\n",
        "opt_state = opt_init(net_params)\n",
        "\n",
        "@jit\n",
        "def step(i, opt_state, x, y):\n",
        "    p = get_params(opt_state)\n",
        "    g = grad(loss)(p, x, y)\n",
        "    l = loss(p, x, y)\n",
        "    return opt_update(i, g, opt_state), l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq_SeR66as7p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ee40cdab-7661-4621-fba4-2be84c6ac726"
      },
      "source": [
        "#Training\n",
        "losses=[]\n",
        "for i in range(201):\n",
        "    opt_state, l = step(i, opt_state, x_data, y_data)\n",
        "    losses.append(l)\n",
        "    if i % 20 == 0:\n",
        "        print(get_params(opt_state))\n",
        "net_params=get_params(opt_state)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(DeviceArray([[0.3528281]], dtype=float32), DeviceArray([0.49513587], dtype=float32))]\n",
            "[(DeviceArray([[0.09138642]], dtype=float32), DeviceArray([0.39634892], dtype=float32))]\n",
            "[(DeviceArray([[0.09057112]], dtype=float32), DeviceArray([0.3366886], dtype=float32))]\n",
            "[(DeviceArray([[0.12353643]], dtype=float32), DeviceArray([0.30404165], dtype=float32))]\n",
            "[(DeviceArray([[0.10185698]], dtype=float32), DeviceArray([0.29825112], dtype=float32))]\n",
            "[(DeviceArray([[0.09979293]], dtype=float32), DeviceArray([0.29922235], dtype=float32))]\n",
            "[(DeviceArray([[0.10034811]], dtype=float32), DeviceArray([0.30057368], dtype=float32))]\n",
            "[(DeviceArray([[0.10039034]], dtype=float32), DeviceArray([0.30027398], dtype=float32))]\n",
            "[(DeviceArray([[0.10002711]], dtype=float32), DeviceArray([0.3000829], dtype=float32))]\n",
            "[(DeviceArray([[0.09994931]], dtype=float32), DeviceArray([0.30000454], dtype=float32))]\n",
            "[(DeviceArray([[0.10001832]], dtype=float32), DeviceArray([0.29999167], dtype=float32))]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}