{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jax_qlearning_cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anirbanl/jax-code/blob/master/rlflax/qlearning/jax_flax_dueling_dqn_cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJIO3iDY31YN"
      },
      "source": [
        "# Based on sources:\n",
        "\n",
        "1. https://markelsanz14.medium.com/introduction-to-reinforcement-learning-part-4-double-dqn-and-dueling-dqn-b349c9a61ea1\n",
        "2. https://towardsdatascience.com/dueling-deep-q-networks-81ffab672751\n",
        "3. https://medium.com/@parsa_h_m/deep-reinforcement-learning-dqn-double-dqn-dueling-dqn-noisy-dqn-and-dqn-with-prioritized-551f621a9823\n",
        "4. https://github.com/higgsfield/RL-Adventure/blob/master/3.dueling%20dqn.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_jS9qg6EG4l",
        "outputId": "55793d3a-cde1-4db2-dbad-3179eba4a08a"
      },
      "source": [
        "!pip install jax jaxlib flax"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (0.2.13)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (0.1.66+cuda110)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.7/dist-packages (0.3.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from jax) (1.19.5)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.12)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.4.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax) (1.0.2)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (from flax) (0.0.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax) (3.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax) (1.15.0)\n",
            "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax->flax) (0.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.8.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.11.1)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.1.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt1r4--nEPJm"
      },
      "source": [
        "import gym\n",
        "gym.logger.set_level(40) # suppress warnings (please remove if gives error)\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import jax\n",
        "import jax.numpy as jp\n",
        "from jax.ops import index, index_add, index_update\n",
        "from jax import jit, grad, vmap, random, jacrev, jacobian, jacfwd, value_and_grad\n",
        "from functools import partial\n",
        "from jax.tree_util import tree_multimap  # Element-wise manipulation of collections of numpy arrays\n",
        "from flax import linen as nn           # The Linen API\n",
        "from flax.training import train_state  # Useful dataclass to keep train state\n",
        "import optax                           # Optimizers\n",
        "from typing import Sequence, Type"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK9XAT81EWW0"
      },
      "source": [
        "from collections import deque\n",
        "class Memory():\n",
        "    def __init__(self, max_size = 1000):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "    \n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "            \n",
        "    def sample(self, key, batch_size):\n",
        "        key, _ = jax.random.split(key)\n",
        "        idx = jax.random.choice(key,\n",
        "                               jp.arange(len(self.buffer)), \n",
        "                               shape=(batch_size, ))\n",
        "        # print(f\"\\nIds:{jp.mean(idx)}\\n\")\n",
        "        return [self.buffer[ii] for ii in idx]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtUX4AXiFWVK"
      },
      "source": [
        "train_episodes = 1000          # max number of episodes to learn from\n",
        "max_steps = 200                # max steps in an episode\n",
        "gamma = 0.99                   # future reward discount\n",
        "\n",
        "# Exploration parameters\n",
        "explore_start = 1.0            # exploration probability at start\n",
        "explore_stop = 0.01            # minimum exploration probability \n",
        "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
        "\n",
        "# Network parameters\n",
        "hidden_size = 64               # number of units in each Q-network hidden layer\n",
        "learning_rate = 1e-4         # Q-network learning rate\n",
        "\n",
        "# Memory parameters\n",
        "memory_size = 10000            # memory capacity\n",
        "batch_size = 20                # experience mini-batch size\n",
        "pretrain_length = batch_size   # number experiences to pretrain the memory"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSJfnThHFouD"
      },
      "source": [
        "#Define Q-network\n",
        "class QNetwork:\n",
        "    def __init__(self, rng, env, learning_rate=0.01, state_size=4, \n",
        "                 action_size=2, hidden_size=10, \n",
        "                 name='QNetwork'):\n",
        "        self.key = rng\n",
        "        self.env = env\n",
        "        # print(f\"QNetwork rng:{rng}\")\n",
        "\n",
        "        class Sequential(nn.Module):\n",
        "            layers: Sequence[nn.Module]\n",
        "\n",
        "            @nn.compact\n",
        "            def __call__(self, x):\n",
        "                for layer in self.layers:\n",
        "  \t                x = layer(x)\n",
        "                return x\n",
        "\n",
        "        class DuelingDQN(nn.Module):\n",
        "\n",
        "            def setup(self):\n",
        "                self.feature_model = Sequential(\n",
        "                    layers = [nn.Dense(hidden_size), nn.relu])\n",
        "                self.advantage_model = Sequential(\n",
        "                    layers = [nn.Dense(hidden_size), nn.relu, nn.Dense(action_size)])\n",
        "                self.value_model = Sequential(\n",
        "                    layers = [nn.Dense(hidden_size), nn.relu, nn.Dense(1)])\n",
        "\n",
        "            def __call__(self, x):\n",
        "                x = self.feature_model(x)\n",
        "                advantage = self.advantage_model(x)\n",
        "                value = self.value_model(x)\n",
        "                return value + advantage - jp.mean(advantage)\n",
        "\n",
        "        def create_train_state(rng, learning_rate, input_size):\n",
        "            \"\"\"Creates initial `TrainState`.\"\"\"\n",
        "            model = DuelingDQN()\n",
        "            params = model.init(rng, jp.ones((input_size, )))#['params']\n",
        "            # print(f\"Params:{params}\")\n",
        "            tx = optax.adam(learning_rate)\n",
        "            return train_state.TrainState.create(\n",
        "                apply_fn=model.apply, params=params, tx=tx)\n",
        "\n",
        "        self.ts = create_train_state(rng, learning_rate, state_size)\n",
        "\n",
        "        @jit\n",
        "        def train_step(ts, inputs, actions, targets):\n",
        "\n",
        "            def loss_fun(params, inputs, actions, targets):\n",
        "                output = ts.apply_fn(params, inputs)\n",
        "                selectedq = jp.sum(actions*output, axis=-1)\n",
        "                diff = selectedq - jax.lax.stop_gradient(targets)\n",
        "                return jp.mean(diff**2)\n",
        "\n",
        "            loss, g = value_and_grad(loss_fun)(ts.params, inputs, actions, targets)\n",
        "            return ts.apply_gradients(grads=g), loss\n",
        "\n",
        "        self.train_fn = train_step\n",
        "\n",
        "\n",
        "    def act(self, state, explore_p):\n",
        "        self.key, _ = jax.random.split(self.key)\n",
        "        # print(f\"Act key:{self.key}\")\n",
        "        uf = jax.random.uniform(self.key, (1,), minval=0.0, maxval=1.0)[0]\n",
        "        if explore_p > uf:\n",
        "            # Make a random action\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            # Get action from Q-network\n",
        "            qvalues = self.ts.apply_fn(self.ts.params, state)\n",
        "            action = jp.argmax(qvalues).item()\n",
        "        return action\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0fPvqRcLdf4"
      },
      "source": [
        "def init_memory(env):\n",
        "    # Initialize the simulation\n",
        "    env.reset()\n",
        "    # Take one random step to get the pole and cart moving\n",
        "    state, reward, done, _ = env.step(env.action_space.sample())\n",
        "    # print(f\"@@@@@@ Env init state:{state} @@@@@@\")\n",
        "\n",
        "    memory = Memory(max_size=memory_size)\n",
        "\n",
        "    # Make a bunch of random actions and store the experiences\n",
        "    for ii in range(pretrain_length):\n",
        "        # Uncomment the line below to watch the simulation\n",
        "        # env.render()\n",
        "\n",
        "        # Make a random action\n",
        "        action = env.action_space.sample()\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        # print(f\"@@@@@@ Env action:{action} @@@@@@\")\n",
        "        # print(f\"@@@@@@ Env next state:{next_state} @@@@@@\")\n",
        "\n",
        "        if done:\n",
        "            # The simulation fails so no next state\n",
        "            next_state = jp.zeros(state.shape)\n",
        "            # Add experience to memory\n",
        "            memory.add((state, action, reward, next_state))\n",
        "            \n",
        "            # Start new episode\n",
        "            env.reset()\n",
        "            # Take one random step to get the pole and cart moving\n",
        "            state, reward, done, _ = env.step(env.action_space.sample())\n",
        "        else:\n",
        "            # Add experience to memory\n",
        "            memory.add((state, action, reward, next_state))\n",
        "            state = next_state\n",
        "    return memory, state"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vpr3LNUMkaG"
      },
      "source": [
        "# Now train with experiences\n",
        "def one_hot(x, k, dtype=jp.float32):\n",
        "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
        "  return jp.array(x[:, None] == jp.arange(k), dtype)\n",
        "\n",
        "\n",
        "def train(rng, env, mainQN):\n",
        "    # print(f\"train rng:{rng}\")\n",
        "    rewards_list = []    \n",
        "    step = 0\n",
        "    memory, state = init_memory(env)\n",
        "    for ep in range(1, train_episodes):\n",
        "        total_reward = 0\n",
        "        t = 0\n",
        "        while t < max_steps:\n",
        "            step += 1\n",
        "            # Uncomment this next line to watch the training\n",
        "            # env.render() \n",
        "            \n",
        "            # Explore or Exploit\n",
        "            explore_p = explore_stop + (explore_start - explore_stop)*jp.exp(-decay_rate*step) \n",
        "            action = mainQN.act(state, explore_p)\n",
        "            \n",
        "            # Take action, get new state and reward\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            # print(f\"State:{state}\")\n",
        "            # print(f\"Reward:{reward}\")\n",
        "            # print(f\"Action:{action}\")\n",
        "            # print(f\"Done:{done}\\n\")\n",
        "\n",
        "            total_reward += reward\n",
        "            \n",
        "            if done:\n",
        "                # the episode ends so no next state\n",
        "                next_state = jp.zeros(state.shape)\n",
        "                t = max_steps\n",
        "                \n",
        "                print('Episode: {}'.format(ep),\n",
        "                    'Total reward: {}'.format(total_reward),\n",
        "                    'Training loss: {:.4f}'.format(loss),\n",
        "                    'Explore P: {:.4f}'.format(explore_p))\n",
        "                rewards_list.append((ep, total_reward))\n",
        "                \n",
        "                # Add experience to memory\n",
        "                memory.add((state, action, reward, next_state))\n",
        "                \n",
        "                # Start new episode\n",
        "                env.reset()\n",
        "                # Take one random step to get the pole and cart moving\n",
        "                state, reward, done, _ = env.step(env.action_space.sample())\n",
        "\n",
        "            else:\n",
        "                # Add experience to memory\n",
        "                memory.add((state, action, reward, next_state))\n",
        "                state = next_state\n",
        "                t += 1\n",
        "            \n",
        "            # Sample mini-batch from memory\n",
        "            batch = memory.sample(rng, batch_size)\n",
        "            states = jp.array([each[0] for each in batch])\n",
        "            actions = one_hot(jp.array([each[1] for each in batch]), 2)\n",
        "            rewards = jp.array([each[2] for each in batch])\n",
        "            next_states = jp.array([each[3] for each in batch])\n",
        "            \n",
        "            # Train network\n",
        "            target_Qs = mainQN.ts.apply_fn(mainQN.ts.params, next_states)\n",
        "            \n",
        "            # Set target_Qs to 0 for states where episode ends\n",
        "            episode_ends = (next_states == jp.zeros(states[0].shape)).all(axis=1)\n",
        "            new_target_Qs = index_update(target_Qs, index[episode_ends], (0, 0))\n",
        "            target_Qs = new_target_Qs\n",
        "            \n",
        "            targets = rewards + gamma * jp.max(target_Qs, axis=1)\n",
        "            # print(states.shape, targets.shape, targets)\n",
        "            mainQN.ts, loss = mainQN.train_fn(mainQN.ts, states, actions, targets)\n",
        "\n",
        "    return rewards_list\n",
        "\n",
        "def plot_scores(rewards_list):\n",
        "    def running_mean(x, N):\n",
        "        cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
        "        return (cumsum[N:] - cumsum[:-N]) / N\n",
        "    eps, rews = np.array(rewards_list).T\n",
        "    smoothed_rews = running_mean(rews, 10)\n",
        "    plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
        "    plt.plot(eps, rews, color='grey', alpha=0.3)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.show()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeurKJ6oNcZF",
        "outputId": "93c8f8c0-63ff-4838-805e-7ecb3675f3d2"
      },
      "source": [
        "def main():\n",
        "    seed = 0\n",
        "    env = gym.make('CartPole-v0')\n",
        "    env.seed(seed)\n",
        "    env.action_space.seed(seed)\n",
        "    print('observation space:', env.observation_space)\n",
        "    print('action space:', env.action_space.n)\n",
        "    rng = jax.random.PRNGKey(seed)\n",
        "    mainQN = QNetwork(rng, env, name='main', hidden_size=hidden_size, learning_rate=learning_rate)\n",
        "    rewards_list = train(rng, env, mainQN)\n",
        "    plot_scores(rewards_list)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
            "action space: 2\n",
            "Episode: 1 Total reward: 31.0 Training loss: 1.0755 Explore P: 0.9969\n",
            "Episode: 2 Total reward: 54.0 Training loss: 1.1272 Explore P: 0.9916\n",
            "Episode: 3 Total reward: 28.0 Training loss: 1.0321 Explore P: 0.9889\n",
            "Episode: 4 Total reward: 21.0 Training loss: 1.0920 Explore P: 0.9868\n",
            "Episode: 5 Total reward: 8.0 Training loss: 1.2192 Explore P: 0.9860\n",
            "Episode: 6 Total reward: 13.0 Training loss: 1.1947 Explore P: 0.9848\n",
            "Episode: 7 Total reward: 20.0 Training loss: 1.0323 Explore P: 0.9828\n",
            "Episode: 8 Total reward: 35.0 Training loss: 1.4423 Explore P: 0.9794\n",
            "Episode: 9 Total reward: 27.0 Training loss: 1.3803 Explore P: 0.9768\n",
            "Episode: 10 Total reward: 22.0 Training loss: 1.1606 Explore P: 0.9747\n",
            "Episode: 11 Total reward: 15.0 Training loss: 1.8741 Explore P: 0.9732\n",
            "Episode: 12 Total reward: 28.0 Training loss: 1.6710 Explore P: 0.9705\n",
            "Episode: 13 Total reward: 10.0 Training loss: 1.8584 Explore P: 0.9696\n",
            "Episode: 14 Total reward: 16.0 Training loss: 1.7576 Explore P: 0.9681\n",
            "Episode: 15 Total reward: 10.0 Training loss: 2.0772 Explore P: 0.9671\n",
            "Episode: 16 Total reward: 21.0 Training loss: 2.0026 Explore P: 0.9651\n",
            "Episode: 17 Total reward: 15.0 Training loss: 6.0408 Explore P: 0.9637\n",
            "Episode: 18 Total reward: 10.0 Training loss: 4.2056 Explore P: 0.9627\n",
            "Episode: 19 Total reward: 33.0 Training loss: 2.7284 Explore P: 0.9596\n",
            "Episode: 20 Total reward: 21.0 Training loss: 4.8622 Explore P: 0.9576\n",
            "Episode: 21 Total reward: 8.0 Training loss: 9.5753 Explore P: 0.9568\n",
            "Episode: 22 Total reward: 21.0 Training loss: 8.5751 Explore P: 0.9548\n",
            "Episode: 23 Total reward: 10.0 Training loss: 2.3432 Explore P: 0.9539\n",
            "Episode: 24 Total reward: 15.0 Training loss: 3.3483 Explore P: 0.9525\n",
            "Episode: 25 Total reward: 10.0 Training loss: 4.0226 Explore P: 0.9515\n",
            "Episode: 26 Total reward: 12.0 Training loss: 5.0237 Explore P: 0.9504\n",
            "Episode: 27 Total reward: 16.0 Training loss: 2.9378 Explore P: 0.9489\n",
            "Episode: 28 Total reward: 16.0 Training loss: 15.4454 Explore P: 0.9474\n",
            "Episode: 29 Total reward: 15.0 Training loss: 8.9545 Explore P: 0.9460\n",
            "Episode: 30 Total reward: 14.0 Training loss: 4.1658 Explore P: 0.9447\n",
            "Episode: 31 Total reward: 9.0 Training loss: 6.9405 Explore P: 0.9438\n",
            "Episode: 32 Total reward: 16.0 Training loss: 6.1465 Explore P: 0.9423\n",
            "Episode: 33 Total reward: 19.0 Training loss: 4.3574 Explore P: 0.9406\n",
            "Episode: 34 Total reward: 15.0 Training loss: 16.8464 Explore P: 0.9392\n",
            "Episode: 35 Total reward: 9.0 Training loss: 13.9244 Explore P: 0.9383\n",
            "Episode: 36 Total reward: 12.0 Training loss: 4.2801 Explore P: 0.9372\n",
            "Episode: 37 Total reward: 11.0 Training loss: 23.3024 Explore P: 0.9362\n",
            "Episode: 38 Total reward: 34.0 Training loss: 5.9704 Explore P: 0.9331\n",
            "Episode: 39 Total reward: 23.0 Training loss: 7.4357 Explore P: 0.9309\n",
            "Episode: 40 Total reward: 18.0 Training loss: 12.0221 Explore P: 0.9293\n",
            "Episode: 41 Total reward: 9.0 Training loss: 8.5438 Explore P: 0.9285\n",
            "Episode: 42 Total reward: 13.0 Training loss: 46.0423 Explore P: 0.9273\n",
            "Episode: 43 Total reward: 20.0 Training loss: 3.9695 Explore P: 0.9254\n",
            "Episode: 44 Total reward: 27.0 Training loss: 31.2436 Explore P: 0.9230\n",
            "Episode: 45 Total reward: 10.0 Training loss: 16.4892 Explore P: 0.9221\n",
            "Episode: 46 Total reward: 17.0 Training loss: 15.9647 Explore P: 0.9205\n",
            "Episode: 47 Total reward: 25.0 Training loss: 5.5154 Explore P: 0.9182\n",
            "Episode: 48 Total reward: 22.0 Training loss: 4.9542 Explore P: 0.9162\n",
            "Episode: 49 Total reward: 14.0 Training loss: 9.8756 Explore P: 0.9150\n",
            "Episode: 50 Total reward: 22.0 Training loss: 8.3530 Explore P: 0.9130\n",
            "Episode: 51 Total reward: 11.0 Training loss: 8.4110 Explore P: 0.9120\n",
            "Episode: 52 Total reward: 16.0 Training loss: 19.8922 Explore P: 0.9105\n",
            "Episode: 53 Total reward: 19.0 Training loss: 18.5638 Explore P: 0.9088\n",
            "Episode: 54 Total reward: 19.0 Training loss: 9.5262 Explore P: 0.9071\n",
            "Episode: 55 Total reward: 24.0 Training loss: 95.5796 Explore P: 0.9050\n",
            "Episode: 56 Total reward: 32.0 Training loss: 7.0350 Explore P: 0.9021\n",
            "Episode: 57 Total reward: 17.0 Training loss: 45.8084 Explore P: 0.9006\n",
            "Episode: 58 Total reward: 9.0 Training loss: 9.5456 Explore P: 0.8998\n",
            "Episode: 59 Total reward: 14.0 Training loss: 45.3930 Explore P: 0.8986\n",
            "Episode: 60 Total reward: 14.0 Training loss: 41.4355 Explore P: 0.8973\n",
            "Episode: 61 Total reward: 45.0 Training loss: 19.8576 Explore P: 0.8933\n",
            "Episode: 62 Total reward: 11.0 Training loss: 57.4208 Explore P: 0.8924\n",
            "Episode: 63 Total reward: 14.0 Training loss: 27.6897 Explore P: 0.8911\n",
            "Episode: 64 Total reward: 9.0 Training loss: 12.1536 Explore P: 0.8903\n",
            "Episode: 65 Total reward: 46.0 Training loss: 51.4933 Explore P: 0.8863\n",
            "Episode: 66 Total reward: 23.0 Training loss: 14.5171 Explore P: 0.8843\n",
            "Episode: 67 Total reward: 26.0 Training loss: 35.6388 Explore P: 0.8820\n",
            "Episode: 68 Total reward: 33.0 Training loss: 27.0655 Explore P: 0.8791\n",
            "Episode: 69 Total reward: 20.0 Training loss: 115.1167 Explore P: 0.8774\n",
            "Episode: 70 Total reward: 9.0 Training loss: 12.1869 Explore P: 0.8766\n",
            "Episode: 71 Total reward: 15.0 Training loss: 87.4874 Explore P: 0.8753\n",
            "Episode: 72 Total reward: 12.0 Training loss: 34.4301 Explore P: 0.8743\n",
            "Episode: 73 Total reward: 18.0 Training loss: 132.2488 Explore P: 0.8727\n",
            "Episode: 74 Total reward: 16.0 Training loss: 13.2809 Explore P: 0.8714\n",
            "Episode: 75 Total reward: 32.0 Training loss: 15.2042 Explore P: 0.8686\n",
            "Episode: 76 Total reward: 12.0 Training loss: 27.4722 Explore P: 0.8676\n",
            "Episode: 77 Total reward: 16.0 Training loss: 38.4908 Explore P: 0.8662\n",
            "Episode: 78 Total reward: 18.0 Training loss: 50.5850 Explore P: 0.8647\n",
            "Episode: 79 Total reward: 12.0 Training loss: 54.5844 Explore P: 0.8636\n",
            "Episode: 80 Total reward: 18.0 Training loss: 104.7400 Explore P: 0.8621\n",
            "Episode: 81 Total reward: 14.0 Training loss: 32.3186 Explore P: 0.8609\n",
            "Episode: 82 Total reward: 32.0 Training loss: 52.8981 Explore P: 0.8582\n",
            "Episode: 83 Total reward: 22.0 Training loss: 130.4665 Explore P: 0.8563\n",
            "Episode: 84 Total reward: 21.0 Training loss: 40.7504 Explore P: 0.8546\n",
            "Episode: 85 Total reward: 15.0 Training loss: 61.0801 Explore P: 0.8533\n",
            "Episode: 86 Total reward: 21.0 Training loss: 16.6459 Explore P: 0.8515\n",
            "Episode: 87 Total reward: 15.0 Training loss: 44.5080 Explore P: 0.8503\n",
            "Episode: 88 Total reward: 12.0 Training loss: 73.9039 Explore P: 0.8492\n",
            "Episode: 89 Total reward: 8.0 Training loss: 96.2312 Explore P: 0.8486\n",
            "Episode: 90 Total reward: 9.0 Training loss: 13.0917 Explore P: 0.8478\n",
            "Episode: 91 Total reward: 26.0 Training loss: 71.5490 Explore P: 0.8456\n",
            "Episode: 92 Total reward: 13.0 Training loss: 54.5336 Explore P: 0.8446\n",
            "Episode: 93 Total reward: 11.0 Training loss: 98.8805 Explore P: 0.8436\n",
            "Episode: 94 Total reward: 15.0 Training loss: 43.3911 Explore P: 0.8424\n",
            "Episode: 95 Total reward: 58.0 Training loss: 89.4039 Explore P: 0.8376\n",
            "Episode: 96 Total reward: 12.0 Training loss: 14.1275 Explore P: 0.8366\n",
            "Episode: 97 Total reward: 13.0 Training loss: 108.8643 Explore P: 0.8355\n",
            "Episode: 98 Total reward: 19.0 Training loss: 11.2162 Explore P: 0.8339\n",
            "Episode: 99 Total reward: 13.0 Training loss: 69.3505 Explore P: 0.8329\n",
            "Episode: 100 Total reward: 13.0 Training loss: 41.6780 Explore P: 0.8318\n",
            "Episode: 101 Total reward: 20.0 Training loss: 160.2904 Explore P: 0.8302\n",
            "Episode: 102 Total reward: 29.0 Training loss: 12.9990 Explore P: 0.8278\n",
            "Episode: 103 Total reward: 18.0 Training loss: 15.7034 Explore P: 0.8263\n",
            "Episode: 104 Total reward: 15.0 Training loss: 15.5320 Explore P: 0.8251\n",
            "Episode: 105 Total reward: 16.0 Training loss: 15.9746 Explore P: 0.8238\n",
            "Episode: 106 Total reward: 12.0 Training loss: 81.7001 Explore P: 0.8228\n",
            "Episode: 107 Total reward: 23.0 Training loss: 198.6173 Explore P: 0.8209\n",
            "Episode: 108 Total reward: 10.0 Training loss: 107.2417 Explore P: 0.8201\n",
            "Episode: 109 Total reward: 13.0 Training loss: 84.6466 Explore P: 0.8191\n",
            "Episode: 110 Total reward: 27.0 Training loss: 15.8842 Explore P: 0.8169\n",
            "Episode: 111 Total reward: 14.0 Training loss: 88.5134 Explore P: 0.8158\n",
            "Episode: 112 Total reward: 12.0 Training loss: 17.6661 Explore P: 0.8148\n",
            "Episode: 113 Total reward: 10.0 Training loss: 19.8821 Explore P: 0.8140\n",
            "Episode: 114 Total reward: 12.0 Training loss: 158.6693 Explore P: 0.8130\n",
            "Episode: 115 Total reward: 22.0 Training loss: 59.3522 Explore P: 0.8113\n",
            "Episode: 116 Total reward: 12.0 Training loss: 183.1293 Explore P: 0.8103\n",
            "Episode: 117 Total reward: 18.0 Training loss: 82.7120 Explore P: 0.8089\n",
            "Episode: 118 Total reward: 15.0 Training loss: 118.8136 Explore P: 0.8077\n",
            "Episode: 119 Total reward: 13.0 Training loss: 150.5477 Explore P: 0.8066\n",
            "Episode: 120 Total reward: 12.0 Training loss: 18.0232 Explore P: 0.8057\n",
            "Episode: 121 Total reward: 13.0 Training loss: 55.9372 Explore P: 0.8047\n",
            "Episode: 122 Total reward: 12.0 Training loss: 16.7726 Explore P: 0.8037\n",
            "Episode: 123 Total reward: 10.0 Training loss: 69.7663 Explore P: 0.8029\n",
            "Episode: 124 Total reward: 8.0 Training loss: 19.4779 Explore P: 0.8023\n",
            "Episode: 125 Total reward: 9.0 Training loss: 118.2486 Explore P: 0.8016\n",
            "Episode: 126 Total reward: 14.0 Training loss: 88.7116 Explore P: 0.8005\n",
            "Episode: 127 Total reward: 35.0 Training loss: 105.4413 Explore P: 0.7977\n",
            "Episode: 128 Total reward: 8.0 Training loss: 109.5703 Explore P: 0.7971\n",
            "Episode: 129 Total reward: 30.0 Training loss: 67.2488 Explore P: 0.7947\n",
            "Episode: 130 Total reward: 13.0 Training loss: 18.5895 Explore P: 0.7937\n",
            "Episode: 131 Total reward: 13.0 Training loss: 65.1766 Explore P: 0.7927\n",
            "Episode: 132 Total reward: 12.0 Training loss: 64.8752 Explore P: 0.7917\n",
            "Episode: 133 Total reward: 11.0 Training loss: 104.4153 Explore P: 0.7909\n",
            "Episode: 134 Total reward: 18.0 Training loss: 151.9736 Explore P: 0.7895\n",
            "Episode: 135 Total reward: 14.0 Training loss: 16.9946 Explore P: 0.7884\n",
            "Episode: 136 Total reward: 14.0 Training loss: 96.9553 Explore P: 0.7873\n",
            "Episode: 137 Total reward: 14.0 Training loss: 87.7824 Explore P: 0.7862\n",
            "Episode: 138 Total reward: 16.0 Training loss: 164.7950 Explore P: 0.7850\n",
            "Episode: 139 Total reward: 8.0 Training loss: 148.3525 Explore P: 0.7843\n",
            "Episode: 140 Total reward: 16.0 Training loss: 20.0282 Explore P: 0.7831\n",
            "Episode: 141 Total reward: 12.0 Training loss: 176.9337 Explore P: 0.7822\n",
            "Episode: 142 Total reward: 14.0 Training loss: 204.6931 Explore P: 0.7811\n",
            "Episode: 143 Total reward: 11.0 Training loss: 266.2122 Explore P: 0.7802\n",
            "Episode: 144 Total reward: 8.0 Training loss: 72.9248 Explore P: 0.7796\n",
            "Episode: 145 Total reward: 23.0 Training loss: 66.8903 Explore P: 0.7779\n",
            "Episode: 146 Total reward: 17.0 Training loss: 269.4144 Explore P: 0.7766\n",
            "Episode: 147 Total reward: 9.0 Training loss: 14.1246 Explore P: 0.7759\n",
            "Episode: 148 Total reward: 24.0 Training loss: 157.7933 Explore P: 0.7740\n",
            "Episode: 149 Total reward: 9.0 Training loss: 15.1248 Explore P: 0.7733\n",
            "Episode: 150 Total reward: 8.0 Training loss: 23.5831 Explore P: 0.7727\n",
            "Episode: 151 Total reward: 20.0 Training loss: 158.2263 Explore P: 0.7712\n",
            "Episode: 152 Total reward: 17.0 Training loss: 245.9387 Explore P: 0.7699\n",
            "Episode: 153 Total reward: 12.0 Training loss: 225.4515 Explore P: 0.7690\n",
            "Episode: 154 Total reward: 14.0 Training loss: 11.0395 Explore P: 0.7679\n",
            "Episode: 155 Total reward: 11.0 Training loss: 317.9433 Explore P: 0.7671\n",
            "Episode: 156 Total reward: 25.0 Training loss: 130.5743 Explore P: 0.7652\n",
            "Episode: 157 Total reward: 11.0 Training loss: 143.9607 Explore P: 0.7644\n",
            "Episode: 158 Total reward: 17.0 Training loss: 245.5929 Explore P: 0.7631\n",
            "Episode: 159 Total reward: 11.0 Training loss: 85.0881 Explore P: 0.7623\n",
            "Episode: 160 Total reward: 9.0 Training loss: 13.6741 Explore P: 0.7616\n",
            "Episode: 161 Total reward: 14.0 Training loss: 138.2179 Explore P: 0.7605\n",
            "Episode: 162 Total reward: 10.0 Training loss: 16.0900 Explore P: 0.7598\n",
            "Episode: 163 Total reward: 12.0 Training loss: 490.5493 Explore P: 0.7589\n",
            "Episode: 164 Total reward: 12.0 Training loss: 67.5322 Explore P: 0.7580\n",
            "Episode: 165 Total reward: 11.0 Training loss: 95.5696 Explore P: 0.7572\n",
            "Episode: 166 Total reward: 21.0 Training loss: 88.2668 Explore P: 0.7556\n",
            "Episode: 167 Total reward: 23.0 Training loss: 89.8502 Explore P: 0.7539\n",
            "Episode: 168 Total reward: 13.0 Training loss: 235.2129 Explore P: 0.7529\n",
            "Episode: 169 Total reward: 15.0 Training loss: 89.4484 Explore P: 0.7518\n",
            "Episode: 170 Total reward: 13.0 Training loss: 78.2813 Explore P: 0.7509\n",
            "Episode: 171 Total reward: 12.0 Training loss: 12.6795 Explore P: 0.7500\n",
            "Episode: 172 Total reward: 12.0 Training loss: 116.8647 Explore P: 0.7491\n",
            "Episode: 173 Total reward: 13.0 Training loss: 94.7707 Explore P: 0.7481\n",
            "Episode: 174 Total reward: 10.0 Training loss: 45.6404 Explore P: 0.7474\n",
            "Episode: 175 Total reward: 12.0 Training loss: 15.8704 Explore P: 0.7465\n",
            "Episode: 176 Total reward: 11.0 Training loss: 311.6327 Explore P: 0.7457\n",
            "Episode: 177 Total reward: 21.0 Training loss: 12.2308 Explore P: 0.7441\n",
            "Episode: 178 Total reward: 15.0 Training loss: 75.9743 Explore P: 0.7430\n",
            "Episode: 179 Total reward: 19.0 Training loss: 35.8957 Explore P: 0.7417\n",
            "Episode: 180 Total reward: 8.0 Training loss: 74.9411 Explore P: 0.7411\n",
            "Episode: 181 Total reward: 31.0 Training loss: 35.4993 Explore P: 0.7388\n",
            "Episode: 182 Total reward: 11.0 Training loss: 46.7199 Explore P: 0.7380\n",
            "Episode: 183 Total reward: 10.0 Training loss: 266.9874 Explore P: 0.7373\n",
            "Episode: 184 Total reward: 12.0 Training loss: 12.0092 Explore P: 0.7364\n",
            "Episode: 185 Total reward: 13.0 Training loss: 82.4687 Explore P: 0.7355\n",
            "Episode: 186 Total reward: 15.0 Training loss: 81.3803 Explore P: 0.7344\n",
            "Episode: 187 Total reward: 21.0 Training loss: 141.4990 Explore P: 0.7329\n",
            "Episode: 188 Total reward: 11.0 Training loss: 111.6231 Explore P: 0.7321\n",
            "Episode: 189 Total reward: 10.0 Training loss: 101.1819 Explore P: 0.7313\n",
            "Episode: 190 Total reward: 10.0 Training loss: 12.4651 Explore P: 0.7306\n",
            "Episode: 191 Total reward: 11.0 Training loss: 73.3631 Explore P: 0.7298\n",
            "Episode: 192 Total reward: 19.0 Training loss: 44.4552 Explore P: 0.7285\n",
            "Episode: 193 Total reward: 18.0 Training loss: 13.1584 Explore P: 0.7272\n",
            "Episode: 194 Total reward: 10.0 Training loss: 133.9329 Explore P: 0.7264\n",
            "Episode: 195 Total reward: 12.0 Training loss: 30.6269 Explore P: 0.7256\n",
            "Episode: 196 Total reward: 20.0 Training loss: 151.3402 Explore P: 0.7242\n",
            "Episode: 197 Total reward: 22.0 Training loss: 372.6838 Explore P: 0.7226\n",
            "Episode: 198 Total reward: 12.0 Training loss: 15.6944 Explore P: 0.7217\n",
            "Episode: 199 Total reward: 12.0 Training loss: 27.8704 Explore P: 0.7209\n",
            "Episode: 200 Total reward: 11.0 Training loss: 147.3208 Explore P: 0.7201\n",
            "Episode: 201 Total reward: 13.0 Training loss: 11.8997 Explore P: 0.7192\n",
            "Episode: 202 Total reward: 16.0 Training loss: 22.9256 Explore P: 0.7180\n",
            "Episode: 203 Total reward: 8.0 Training loss: 16.2270 Explore P: 0.7175\n",
            "Episode: 204 Total reward: 10.0 Training loss: 15.9270 Explore P: 0.7168\n",
            "Episode: 205 Total reward: 13.0 Training loss: 28.7545 Explore P: 0.7159\n",
            "Episode: 206 Total reward: 14.0 Training loss: 75.4453 Explore P: 0.7149\n",
            "Episode: 207 Total reward: 18.0 Training loss: 130.6152 Explore P: 0.7136\n",
            "Episode: 208 Total reward: 26.0 Training loss: 13.6376 Explore P: 0.7118\n",
            "Episode: 209 Total reward: 10.0 Training loss: 14.3921 Explore P: 0.7111\n",
            "Episode: 210 Total reward: 25.0 Training loss: 128.8119 Explore P: 0.7093\n",
            "Episode: 211 Total reward: 10.0 Training loss: 75.6107 Explore P: 0.7086\n",
            "Episode: 212 Total reward: 16.0 Training loss: 89.7523 Explore P: 0.7075\n",
            "Episode: 213 Total reward: 12.0 Training loss: 6.5170 Explore P: 0.7067\n",
            "Episode: 214 Total reward: 16.0 Training loss: 28.6447 Explore P: 0.7056\n",
            "Episode: 215 Total reward: 10.0 Training loss: 86.5611 Explore P: 0.7049\n",
            "Episode: 216 Total reward: 11.0 Training loss: 22.1482 Explore P: 0.7041\n",
            "Episode: 217 Total reward: 15.0 Training loss: 118.2266 Explore P: 0.7031\n",
            "Episode: 218 Total reward: 23.0 Training loss: 23.2586 Explore P: 0.7015\n",
            "Episode: 219 Total reward: 24.0 Training loss: 22.3258 Explore P: 0.6998\n",
            "Episode: 220 Total reward: 13.0 Training loss: 170.0702 Explore P: 0.6989\n",
            "Episode: 221 Total reward: 18.0 Training loss: 12.6092 Explore P: 0.6977\n",
            "Episode: 222 Total reward: 9.0 Training loss: 30.5247 Explore P: 0.6970\n",
            "Episode: 223 Total reward: 9.0 Training loss: 135.6361 Explore P: 0.6964\n",
            "Episode: 224 Total reward: 16.0 Training loss: 107.8389 Explore P: 0.6953\n",
            "Episode: 225 Total reward: 13.0 Training loss: 7.8606 Explore P: 0.6944\n",
            "Episode: 226 Total reward: 24.0 Training loss: 95.1623 Explore P: 0.6928\n",
            "Episode: 227 Total reward: 11.0 Training loss: 35.3906 Explore P: 0.6921\n",
            "Episode: 228 Total reward: 19.0 Training loss: 101.4112 Explore P: 0.6908\n",
            "Episode: 229 Total reward: 15.0 Training loss: 74.6583 Explore P: 0.6897\n",
            "Episode: 230 Total reward: 10.0 Training loss: 9.2577 Explore P: 0.6891\n",
            "Episode: 231 Total reward: 16.0 Training loss: 112.6891 Explore P: 0.6880\n",
            "Episode: 232 Total reward: 20.0 Training loss: 120.7810 Explore P: 0.6866\n",
            "Episode: 233 Total reward: 15.0 Training loss: 116.8696 Explore P: 0.6856\n",
            "Episode: 234 Total reward: 16.0 Training loss: 13.4013 Explore P: 0.6845\n",
            "Episode: 235 Total reward: 10.0 Training loss: 70.4023 Explore P: 0.6838\n",
            "Episode: 236 Total reward: 14.0 Training loss: 85.5137 Explore P: 0.6829\n",
            "Episode: 237 Total reward: 20.0 Training loss: 95.9422 Explore P: 0.6816\n",
            "Episode: 238 Total reward: 19.0 Training loss: 12.2928 Explore P: 0.6803\n",
            "Episode: 239 Total reward: 27.0 Training loss: 19.9964 Explore P: 0.6785\n",
            "Episode: 240 Total reward: 22.0 Training loss: 11.2954 Explore P: 0.6770\n",
            "Episode: 241 Total reward: 12.0 Training loss: 84.8742 Explore P: 0.6762\n",
            "Episode: 242 Total reward: 38.0 Training loss: 17.4532 Explore P: 0.6737\n",
            "Episode: 243 Total reward: 15.0 Training loss: 62.0362 Explore P: 0.6727\n",
            "Episode: 244 Total reward: 14.0 Training loss: 15.4184 Explore P: 0.6718\n",
            "Episode: 245 Total reward: 13.0 Training loss: 11.1169 Explore P: 0.6709\n",
            "Episode: 246 Total reward: 12.0 Training loss: 104.8097 Explore P: 0.6701\n",
            "Episode: 247 Total reward: 13.0 Training loss: 182.7111 Explore P: 0.6693\n",
            "Episode: 248 Total reward: 16.0 Training loss: 13.0178 Explore P: 0.6682\n",
            "Episode: 249 Total reward: 20.0 Training loss: 6.6594 Explore P: 0.6669\n",
            "Episode: 250 Total reward: 8.0 Training loss: 24.9850 Explore P: 0.6664\n",
            "Episode: 251 Total reward: 10.0 Training loss: 21.2805 Explore P: 0.6657\n",
            "Episode: 252 Total reward: 23.0 Training loss: 12.8752 Explore P: 0.6642\n",
            "Episode: 253 Total reward: 20.0 Training loss: 28.3185 Explore P: 0.6629\n",
            "Episode: 254 Total reward: 14.0 Training loss: 41.1892 Explore P: 0.6620\n",
            "Episode: 255 Total reward: 14.0 Training loss: 20.5845 Explore P: 0.6611\n",
            "Episode: 256 Total reward: 17.0 Training loss: 10.0736 Explore P: 0.6600\n",
            "Episode: 257 Total reward: 15.0 Training loss: 69.4698 Explore P: 0.6590\n",
            "Episode: 258 Total reward: 18.0 Training loss: 7.4842 Explore P: 0.6578\n",
            "Episode: 259 Total reward: 24.0 Training loss: 119.2755 Explore P: 0.6563\n",
            "Episode: 260 Total reward: 8.0 Training loss: 5.7409 Explore P: 0.6557\n",
            "Episode: 261 Total reward: 8.0 Training loss: 52.1806 Explore P: 0.6552\n",
            "Episode: 262 Total reward: 26.0 Training loss: 7.8656 Explore P: 0.6536\n",
            "Episode: 263 Total reward: 12.0 Training loss: 90.9857 Explore P: 0.6528\n",
            "Episode: 264 Total reward: 17.0 Training loss: 21.9224 Explore P: 0.6517\n",
            "Episode: 265 Total reward: 9.0 Training loss: 50.2725 Explore P: 0.6511\n",
            "Episode: 266 Total reward: 21.0 Training loss: 21.6487 Explore P: 0.6498\n",
            "Episode: 267 Total reward: 10.0 Training loss: 57.1788 Explore P: 0.6491\n",
            "Episode: 268 Total reward: 20.0 Training loss: 23.9051 Explore P: 0.6479\n",
            "Episode: 269 Total reward: 13.0 Training loss: 26.0019 Explore P: 0.6470\n",
            "Episode: 270 Total reward: 27.0 Training loss: 55.2685 Explore P: 0.6453\n",
            "Episode: 271 Total reward: 14.0 Training loss: 7.9638 Explore P: 0.6444\n",
            "Episode: 272 Total reward: 16.0 Training loss: 71.8409 Explore P: 0.6434\n",
            "Episode: 273 Total reward: 15.0 Training loss: 110.9884 Explore P: 0.6425\n",
            "Episode: 274 Total reward: 13.0 Training loss: 33.6656 Explore P: 0.6416\n",
            "Episode: 275 Total reward: 15.0 Training loss: 10.7279 Explore P: 0.6407\n",
            "Episode: 276 Total reward: 14.0 Training loss: 93.6442 Explore P: 0.6398\n",
            "Episode: 277 Total reward: 13.0 Training loss: 35.1579 Explore P: 0.6390\n",
            "Episode: 278 Total reward: 11.0 Training loss: 20.3752 Explore P: 0.6383\n",
            "Episode: 279 Total reward: 12.0 Training loss: 34.4020 Explore P: 0.6375\n",
            "Episode: 280 Total reward: 26.0 Training loss: 14.1485 Explore P: 0.6359\n",
            "Episode: 281 Total reward: 12.0 Training loss: 41.2184 Explore P: 0.6352\n",
            "Episode: 282 Total reward: 16.0 Training loss: 35.5979 Explore P: 0.6342\n",
            "Episode: 283 Total reward: 25.0 Training loss: 17.3234 Explore P: 0.6326\n",
            "Episode: 284 Total reward: 62.0 Training loss: 27.1572 Explore P: 0.6288\n",
            "Episode: 285 Total reward: 17.0 Training loss: 51.6527 Explore P: 0.6277\n",
            "Episode: 286 Total reward: 44.0 Training loss: 6.1085 Explore P: 0.6250\n",
            "Episode: 287 Total reward: 35.0 Training loss: 9.0332 Explore P: 0.6228\n",
            "Episode: 288 Total reward: 67.0 Training loss: 10.9942 Explore P: 0.6187\n",
            "Episode: 289 Total reward: 47.0 Training loss: 4.2822 Explore P: 0.6159\n",
            "Episode: 290 Total reward: 61.0 Training loss: 6.1266 Explore P: 0.6122\n",
            "Episode: 291 Total reward: 75.0 Training loss: 29.0466 Explore P: 0.6077\n",
            "Episode: 292 Total reward: 91.0 Training loss: 9.0442 Explore P: 0.6023\n",
            "Episode: 293 Total reward: 73.0 Training loss: 7.4169 Explore P: 0.5980\n",
            "Episode: 294 Total reward: 79.0 Training loss: 49.7825 Explore P: 0.5934\n",
            "Episode: 295 Total reward: 95.0 Training loss: 6.0631 Explore P: 0.5878\n",
            "Episode: 296 Total reward: 30.0 Training loss: 31.8909 Explore P: 0.5861\n",
            "Episode: 297 Total reward: 60.0 Training loss: 8.2777 Explore P: 0.5827\n",
            "Episode: 298 Total reward: 36.0 Training loss: 7.7308 Explore P: 0.5806\n",
            "Episode: 299 Total reward: 130.0 Training loss: 12.6038 Explore P: 0.5732\n",
            "Episode: 300 Total reward: 34.0 Training loss: 68.4217 Explore P: 0.5713\n",
            "Episode: 301 Total reward: 89.0 Training loss: 30.8751 Explore P: 0.5664\n",
            "Episode: 302 Total reward: 55.0 Training loss: 22.0381 Explore P: 0.5633\n",
            "Episode: 303 Total reward: 31.0 Training loss: 8.3702 Explore P: 0.5616\n",
            "Episode: 304 Total reward: 65.0 Training loss: 29.6792 Explore P: 0.5580\n",
            "Episode: 305 Total reward: 107.0 Training loss: 16.6003 Explore P: 0.5522\n",
            "Episode: 306 Total reward: 63.0 Training loss: 13.9589 Explore P: 0.5488\n",
            "Episode: 307 Total reward: 89.0 Training loss: 8.6819 Explore P: 0.5440\n",
            "Episode: 308 Total reward: 35.0 Training loss: 8.4806 Explore P: 0.5421\n",
            "Episode: 309 Total reward: 61.0 Training loss: 4.4746 Explore P: 0.5389\n",
            "Episode: 310 Total reward: 74.0 Training loss: 19.3660 Explore P: 0.5350\n",
            "Episode: 311 Total reward: 48.0 Training loss: 16.9713 Explore P: 0.5325\n",
            "Episode: 312 Total reward: 75.0 Training loss: 13.9742 Explore P: 0.5286\n",
            "Episode: 313 Total reward: 60.0 Training loss: 5.6020 Explore P: 0.5255\n",
            "Episode: 314 Total reward: 150.0 Training loss: 29.5908 Explore P: 0.5178\n",
            "Episode: 315 Total reward: 49.0 Training loss: 38.7815 Explore P: 0.5153\n",
            "Episode: 316 Total reward: 72.0 Training loss: 83.9635 Explore P: 0.5117\n",
            "Episode: 317 Total reward: 31.0 Training loss: 6.8642 Explore P: 0.5101\n",
            "Episode: 318 Total reward: 39.0 Training loss: 48.0404 Explore P: 0.5082\n",
            "Episode: 319 Total reward: 74.0 Training loss: 87.8696 Explore P: 0.5045\n",
            "Episode: 320 Total reward: 113.0 Training loss: 112.3589 Explore P: 0.4990\n",
            "Episode: 321 Total reward: 77.0 Training loss: 33.5696 Explore P: 0.4952\n",
            "Episode: 322 Total reward: 68.0 Training loss: 4.3538 Explore P: 0.4919\n",
            "Episode: 323 Total reward: 67.0 Training loss: 61.3731 Explore P: 0.4887\n",
            "Episode: 324 Total reward: 57.0 Training loss: 8.8041 Explore P: 0.4860\n",
            "Episode: 325 Total reward: 87.0 Training loss: 31.7379 Explore P: 0.4819\n",
            "Episode: 326 Total reward: 62.0 Training loss: 64.5936 Explore P: 0.4790\n",
            "Episode: 327 Total reward: 141.0 Training loss: 39.2064 Explore P: 0.4724\n",
            "Episode: 328 Total reward: 39.0 Training loss: 106.7407 Explore P: 0.4706\n",
            "Episode: 329 Total reward: 67.0 Training loss: 6.9862 Explore P: 0.4675\n",
            "Episode: 330 Total reward: 49.0 Training loss: 7.5195 Explore P: 0.4653\n",
            "Episode: 331 Total reward: 107.0 Training loss: 32.9824 Explore P: 0.4604\n",
            "Episode: 332 Total reward: 89.0 Training loss: 3.5860 Explore P: 0.4564\n",
            "Episode: 333 Total reward: 48.0 Training loss: 5.0560 Explore P: 0.4543\n",
            "Episode: 334 Total reward: 19.0 Training loss: 7.7972 Explore P: 0.4535\n",
            "Episode: 335 Total reward: 93.0 Training loss: 9.2161 Explore P: 0.4494\n",
            "Episode: 336 Total reward: 61.0 Training loss: 26.1010 Explore P: 0.4467\n",
            "Episode: 337 Total reward: 90.0 Training loss: 189.9969 Explore P: 0.4428\n",
            "Episode: 338 Total reward: 100.0 Training loss: 9.0182 Explore P: 0.4385\n",
            "Episode: 339 Total reward: 105.0 Training loss: 3.3818 Explore P: 0.4340\n",
            "Episode: 340 Total reward: 25.0 Training loss: 118.6534 Explore P: 0.4329\n",
            "Episode: 341 Total reward: 42.0 Training loss: 48.6332 Explore P: 0.4312\n",
            "Episode: 342 Total reward: 63.0 Training loss: 14.3316 Explore P: 0.4285\n",
            "Episode: 343 Total reward: 91.0 Training loss: 5.8355 Explore P: 0.4247\n",
            "Episode: 344 Total reward: 88.0 Training loss: 7.9995 Explore P: 0.4211\n",
            "Episode: 345 Total reward: 43.0 Training loss: 9.4002 Explore P: 0.4193\n",
            "Episode: 346 Total reward: 43.0 Training loss: 177.9648 Explore P: 0.4176\n",
            "Episode: 347 Total reward: 48.0 Training loss: 3.4291 Explore P: 0.4156\n",
            "Episode: 348 Total reward: 33.0 Training loss: 97.2509 Explore P: 0.4143\n",
            "Episode: 349 Total reward: 85.0 Training loss: 104.5193 Explore P: 0.4109\n",
            "Episode: 350 Total reward: 66.0 Training loss: 8.5361 Explore P: 0.4082\n",
            "Episode: 351 Total reward: 59.0 Training loss: 2.9458 Explore P: 0.4059\n",
            "Episode: 352 Total reward: 46.0 Training loss: 14.4173 Explore P: 0.4041\n",
            "Episode: 353 Total reward: 67.0 Training loss: 7.0714 Explore P: 0.4014\n",
            "Episode: 354 Total reward: 137.0 Training loss: 94.8029 Explore P: 0.3961\n",
            "Episode: 355 Total reward: 69.0 Training loss: 20.3477 Explore P: 0.3934\n",
            "Episode: 356 Total reward: 118.0 Training loss: 9.8179 Explore P: 0.3890\n",
            "Episode: 357 Total reward: 65.0 Training loss: 178.0732 Explore P: 0.3865\n",
            "Episode: 358 Total reward: 43.0 Training loss: 283.4804 Explore P: 0.3849\n",
            "Episode: 359 Total reward: 77.0 Training loss: 146.9827 Explore P: 0.3820\n",
            "Episode: 360 Total reward: 62.0 Training loss: 11.3548 Explore P: 0.3797\n",
            "Episode: 361 Total reward: 61.0 Training loss: 16.3235 Explore P: 0.3775\n",
            "Episode: 362 Total reward: 143.0 Training loss: 50.9830 Explore P: 0.3722\n",
            "Episode: 363 Total reward: 49.0 Training loss: 17.6038 Explore P: 0.3705\n",
            "Episode: 364 Total reward: 61.0 Training loss: 53.2474 Explore P: 0.3683\n",
            "Episode: 365 Total reward: 96.0 Training loss: 239.9287 Explore P: 0.3649\n",
            "Episode: 366 Total reward: 69.0 Training loss: 11.4703 Explore P: 0.3624\n",
            "Episode: 367 Total reward: 80.0 Training loss: 1.5753 Explore P: 0.3596\n",
            "Episode: 368 Total reward: 109.0 Training loss: 163.3180 Explore P: 0.3558\n",
            "Episode: 369 Total reward: 79.0 Training loss: 21.3826 Explore P: 0.3531\n",
            "Episode: 370 Total reward: 107.0 Training loss: 102.5595 Explore P: 0.3494\n",
            "Episode: 371 Total reward: 94.0 Training loss: 3.5053 Explore P: 0.3463\n",
            "Episode: 372 Total reward: 84.0 Training loss: 57.0843 Explore P: 0.3435\n",
            "Episode: 373 Total reward: 127.0 Training loss: 209.0515 Explore P: 0.3392\n",
            "Episode: 374 Total reward: 56.0 Training loss: 8.8564 Explore P: 0.3374\n",
            "Episode: 375 Total reward: 90.0 Training loss: 16.4218 Explore P: 0.3345\n",
            "Episode: 376 Total reward: 63.0 Training loss: 119.0642 Explore P: 0.3324\n",
            "Episode: 377 Total reward: 99.0 Training loss: 19.6297 Explore P: 0.3293\n",
            "Episode: 378 Total reward: 49.0 Training loss: 250.1615 Explore P: 0.3277\n",
            "Episode: 379 Total reward: 125.0 Training loss: 10.5976 Explore P: 0.3238\n",
            "Episode: 380 Total reward: 71.0 Training loss: 14.5526 Explore P: 0.3215\n",
            "Episode: 381 Total reward: 81.0 Training loss: 74.6047 Explore P: 0.3190\n",
            "Episode: 382 Total reward: 73.0 Training loss: 6.7287 Explore P: 0.3168\n",
            "Episode: 383 Total reward: 42.0 Training loss: 12.0770 Explore P: 0.3155\n",
            "Episode: 384 Total reward: 100.0 Training loss: 49.2029 Explore P: 0.3124\n",
            "Episode: 385 Total reward: 65.0 Training loss: 148.3005 Explore P: 0.3105\n",
            "Episode: 386 Total reward: 46.0 Training loss: 214.9715 Explore P: 0.3091\n",
            "Episode: 387 Total reward: 91.0 Training loss: 11.0625 Explore P: 0.3064\n",
            "Episode: 388 Total reward: 57.0 Training loss: 8.1407 Explore P: 0.3047\n",
            "Episode: 389 Total reward: 51.0 Training loss: 93.8630 Explore P: 0.3032\n",
            "Episode: 390 Total reward: 67.0 Training loss: 1.2898 Explore P: 0.3013\n",
            "Episode: 391 Total reward: 86.0 Training loss: 190.0793 Explore P: 0.2988\n",
            "Episode: 392 Total reward: 108.0 Training loss: 249.4825 Explore P: 0.2957\n",
            "Episode: 393 Total reward: 144.0 Training loss: 6.3971 Explore P: 0.2916\n",
            "Episode: 394 Total reward: 68.0 Training loss: 1.6889 Explore P: 0.2897\n",
            "Episode: 395 Total reward: 77.0 Training loss: 63.8419 Explore P: 0.2875\n",
            "Episode: 396 Total reward: 99.0 Training loss: 56.5454 Explore P: 0.2848\n",
            "Episode: 397 Total reward: 66.0 Training loss: 5.1597 Explore P: 0.2830\n",
            "Episode: 398 Total reward: 77.0 Training loss: 113.9890 Explore P: 0.2809\n",
            "Episode: 399 Total reward: 70.0 Training loss: 12.2591 Explore P: 0.2790\n",
            "Episode: 400 Total reward: 118.0 Training loss: 3.1465 Explore P: 0.2758\n",
            "Episode: 401 Total reward: 89.0 Training loss: 3.8408 Explore P: 0.2735\n",
            "Episode: 402 Total reward: 60.0 Training loss: 32.5871 Explore P: 0.2719\n",
            "Episode: 403 Total reward: 64.0 Training loss: 41.8985 Explore P: 0.2702\n",
            "Episode: 404 Total reward: 63.0 Training loss: 2.9081 Explore P: 0.2686\n",
            "Episode: 405 Total reward: 68.0 Training loss: 3.7469 Explore P: 0.2669\n",
            "Episode: 406 Total reward: 61.0 Training loss: 247.1194 Explore P: 0.2653\n",
            "Episode: 407 Total reward: 72.0 Training loss: 2.2155 Explore P: 0.2635\n",
            "Episode: 408 Total reward: 116.0 Training loss: 213.7331 Explore P: 0.2605\n",
            "Episode: 409 Total reward: 181.0 Training loss: 6.1131 Explore P: 0.2560\n",
            "Episode: 410 Total reward: 77.0 Training loss: 2.4515 Explore P: 0.2542\n",
            "Episode: 411 Total reward: 157.0 Training loss: 231.2870 Explore P: 0.2504\n",
            "Episode: 412 Total reward: 70.0 Training loss: 3.1492 Explore P: 0.2487\n",
            "Episode: 413 Total reward: 83.0 Training loss: 71.4845 Explore P: 0.2467\n",
            "Episode: 414 Total reward: 89.0 Training loss: 1.8685 Explore P: 0.2446\n",
            "Episode: 415 Total reward: 75.0 Training loss: 66.6658 Explore P: 0.2429\n",
            "Episode: 416 Total reward: 133.0 Training loss: 4.7885 Explore P: 0.2398\n",
            "Episode: 417 Total reward: 84.0 Training loss: 3.2942 Explore P: 0.2379\n",
            "Episode: 418 Total reward: 99.0 Training loss: 365.4203 Explore P: 0.2356\n",
            "Episode: 419 Total reward: 80.0 Training loss: 3.1013 Explore P: 0.2338\n",
            "Episode: 420 Total reward: 57.0 Training loss: 1.5906 Explore P: 0.2325\n",
            "Episode: 421 Total reward: 199.0 Training loss: 2.2069 Explore P: 0.2282\n",
            "Episode: 422 Total reward: 81.0 Training loss: 3.9257 Explore P: 0.2264\n",
            "Episode: 423 Total reward: 56.0 Training loss: 4.0081 Explore P: 0.2252\n",
            "Episode: 424 Total reward: 57.0 Training loss: 0.8899 Explore P: 0.2240\n",
            "Episode: 425 Total reward: 94.0 Training loss: 1.7555 Explore P: 0.2220\n",
            "Episode: 426 Total reward: 59.0 Training loss: 49.4344 Explore P: 0.2207\n",
            "Episode: 427 Total reward: 93.0 Training loss: 78.6665 Explore P: 0.2188\n",
            "Episode: 428 Total reward: 151.0 Training loss: 2.7067 Explore P: 0.2156\n",
            "Episode: 429 Total reward: 57.0 Training loss: 275.1800 Explore P: 0.2145\n",
            "Episode: 430 Total reward: 199.0 Training loss: 2.2626 Explore P: 0.2104\n",
            "Episode: 431 Total reward: 126.0 Training loss: 1.3502 Explore P: 0.2079\n",
            "Episode: 432 Total reward: 72.0 Training loss: 1.1084 Explore P: 0.2065\n",
            "Episode: 433 Total reward: 117.0 Training loss: 3.1070 Explore P: 0.2042\n",
            "Episode: 434 Total reward: 159.0 Training loss: 1.0517 Explore P: 0.2012\n",
            "Episode: 435 Total reward: 88.0 Training loss: 30.3432 Explore P: 0.1995\n",
            "Episode: 436 Total reward: 150.0 Training loss: 18.9586 Explore P: 0.1967\n",
            "Episode: 437 Total reward: 79.0 Training loss: 5.7736 Explore P: 0.1952\n",
            "Episode: 438 Total reward: 41.0 Training loss: 2.2865 Explore P: 0.1944\n",
            "Episode: 439 Total reward: 104.0 Training loss: 229.8754 Explore P: 0.1925\n",
            "Episode: 440 Total reward: 139.0 Training loss: 1.4033 Explore P: 0.1900\n",
            "Episode: 441 Total reward: 69.0 Training loss: 2.9050 Explore P: 0.1888\n",
            "Episode: 442 Total reward: 106.0 Training loss: 0.8005 Explore P: 0.1869\n",
            "Episode: 443 Total reward: 106.0 Training loss: 2.6888 Explore P: 0.1850\n",
            "Episode: 444 Total reward: 106.0 Training loss: 1.7195 Explore P: 0.1832\n",
            "Episode: 445 Total reward: 61.0 Training loss: 2.8136 Explore P: 0.1821\n",
            "Episode: 446 Total reward: 77.0 Training loss: 2.2749 Explore P: 0.1808\n",
            "Episode: 447 Total reward: 113.0 Training loss: 1.3553 Explore P: 0.1789\n",
            "Episode: 448 Total reward: 87.0 Training loss: 1.3654 Explore P: 0.1774\n",
            "Episode: 449 Total reward: 76.0 Training loss: 2.0372 Explore P: 0.1762\n",
            "Episode: 450 Total reward: 68.0 Training loss: 1.5544 Explore P: 0.1750\n",
            "Episode: 451 Total reward: 102.0 Training loss: 1.0797 Explore P: 0.1734\n",
            "Episode: 452 Total reward: 65.0 Training loss: 169.9920 Explore P: 0.1723\n",
            "Episode: 453 Total reward: 83.0 Training loss: 2.4859 Explore P: 0.1710\n",
            "Episode: 454 Total reward: 80.0 Training loss: 1.1206 Explore P: 0.1697\n",
            "Episode: 455 Total reward: 129.0 Training loss: 1.7276 Explore P: 0.1676\n",
            "Episode: 456 Total reward: 82.0 Training loss: 2.8923 Explore P: 0.1663\n",
            "Episode: 457 Total reward: 90.0 Training loss: 5.2402 Explore P: 0.1649\n",
            "Episode: 458 Total reward: 102.0 Training loss: 3.5719 Explore P: 0.1634\n",
            "Episode: 459 Total reward: 85.0 Training loss: 1.4720 Explore P: 0.1621\n",
            "Episode: 460 Total reward: 136.0 Training loss: 143.8033 Explore P: 0.1600\n",
            "Episode: 461 Total reward: 97.0 Training loss: 1.0146 Explore P: 0.1586\n",
            "Episode: 462 Total reward: 68.0 Training loss: 1.3819 Explore P: 0.1576\n",
            "Episode: 463 Total reward: 68.0 Training loss: 0.9926 Explore P: 0.1566\n",
            "Episode: 464 Total reward: 199.0 Training loss: 2.4597 Explore P: 0.1537\n",
            "Episode: 465 Total reward: 103.0 Training loss: 1.5896 Explore P: 0.1522\n",
            "Episode: 466 Total reward: 63.0 Training loss: 1.8740 Explore P: 0.1513\n",
            "Episode: 467 Total reward: 87.0 Training loss: 0.9546 Explore P: 0.1501\n",
            "Episode: 468 Total reward: 199.0 Training loss: 97.1151 Explore P: 0.1473\n",
            "Episode: 469 Total reward: 53.0 Training loss: 0.6481 Explore P: 0.1466\n",
            "Episode: 470 Total reward: 128.0 Training loss: 1.6979 Explore P: 0.1449\n",
            "Episode: 471 Total reward: 147.0 Training loss: 3.2691 Explore P: 0.1429\n",
            "Episode: 472 Total reward: 156.0 Training loss: 2.0327 Explore P: 0.1408\n",
            "Episode: 473 Total reward: 98.0 Training loss: 0.7998 Explore P: 0.1396\n",
            "Episode: 474 Total reward: 199.0 Training loss: 1.2833 Explore P: 0.1370\n",
            "Episode: 475 Total reward: 159.0 Training loss: 2.7137 Explore P: 0.1350\n",
            "Episode: 476 Total reward: 68.0 Training loss: 0.4211 Explore P: 0.1342\n",
            "Episode: 477 Total reward: 167.0 Training loss: 0.4153 Explore P: 0.1321\n",
            "Episode: 478 Total reward: 113.0 Training loss: 1.0561 Explore P: 0.1307\n",
            "Episode: 479 Total reward: 95.0 Training loss: 0.4010 Explore P: 0.1296\n",
            "Episode: 480 Total reward: 199.0 Training loss: 1.5296 Explore P: 0.1272\n",
            "Episode: 481 Total reward: 199.0 Training loss: 7.3390 Explore P: 0.1249\n",
            "Episode: 482 Total reward: 79.0 Training loss: 0.9927 Explore P: 0.1240\n",
            "Episode: 483 Total reward: 86.0 Training loss: 0.4015 Explore P: 0.1230\n",
            "Episode: 484 Total reward: 130.0 Training loss: 1.5023 Explore P: 0.1216\n",
            "Episode: 485 Total reward: 199.0 Training loss: 0.9159 Explore P: 0.1194\n",
            "Episode: 486 Total reward: 189.0 Training loss: 0.5827 Explore P: 0.1173\n",
            "Episode: 487 Total reward: 74.0 Training loss: 0.5076 Explore P: 0.1165\n",
            "Episode: 488 Total reward: 73.0 Training loss: 8.5158 Explore P: 0.1158\n",
            "Episode: 489 Total reward: 142.0 Training loss: 0.3730 Explore P: 0.1143\n",
            "Episode: 490 Total reward: 199.0 Training loss: 0.3448 Explore P: 0.1122\n",
            "Episode: 491 Total reward: 96.0 Training loss: 0.3089 Explore P: 0.1112\n",
            "Episode: 492 Total reward: 199.0 Training loss: 0.4425 Explore P: 0.1092\n",
            "Episode: 493 Total reward: 95.0 Training loss: 0.4165 Explore P: 0.1083\n",
            "Episode: 494 Total reward: 199.0 Training loss: 0.8076 Explore P: 0.1064\n",
            "Episode: 495 Total reward: 143.0 Training loss: 3.6747 Explore P: 0.1050\n",
            "Episode: 496 Total reward: 141.0 Training loss: 2.8786 Explore P: 0.1037\n",
            "Episode: 497 Total reward: 199.0 Training loss: 0.4138 Explore P: 0.1018\n",
            "Episode: 498 Total reward: 84.0 Training loss: 1.0848 Explore P: 0.1011\n",
            "Episode: 499 Total reward: 148.0 Training loss: 1.0570 Explore P: 0.0997\n",
            "Episode: 500 Total reward: 98.0 Training loss: 2.8222 Explore P: 0.0988\n",
            "Episode: 501 Total reward: 127.0 Training loss: 1.1648 Explore P: 0.0977\n",
            "Episode: 502 Total reward: 131.0 Training loss: 0.2993 Explore P: 0.0966\n",
            "Episode: 503 Total reward: 128.0 Training loss: 261.2240 Explore P: 0.0955\n",
            "Episode: 504 Total reward: 199.0 Training loss: 0.9057 Explore P: 0.0938\n",
            "Episode: 505 Total reward: 156.0 Training loss: 0.4974 Explore P: 0.0925\n",
            "Episode: 506 Total reward: 149.0 Training loss: 1.6442 Explore P: 0.0913\n",
            "Episode: 507 Total reward: 199.0 Training loss: 0.3917 Explore P: 0.0897\n",
            "Episode: 508 Total reward: 119.0 Training loss: 0.7764 Explore P: 0.0887\n",
            "Episode: 509 Total reward: 169.0 Training loss: 0.8979 Explore P: 0.0874\n",
            "Episode: 510 Total reward: 169.0 Training loss: 0.2502 Explore P: 0.0861\n",
            "Episode: 511 Total reward: 199.0 Training loss: 0.8315 Explore P: 0.0846\n",
            "Episode: 512 Total reward: 199.0 Training loss: 3.6633 Explore P: 0.0831\n",
            "Episode: 513 Total reward: 199.0 Training loss: 0.5259 Explore P: 0.0817\n",
            "Episode: 514 Total reward: 199.0 Training loss: 0.5060 Explore P: 0.0803\n",
            "Episode: 515 Total reward: 199.0 Training loss: 0.4558 Explore P: 0.0789\n",
            "Episode: 516 Total reward: 199.0 Training loss: 1.0904 Explore P: 0.0776\n",
            "Episode: 517 Total reward: 199.0 Training loss: 0.7788 Explore P: 0.0762\n",
            "Episode: 518 Total reward: 199.0 Training loss: 0.6055 Explore P: 0.0749\n",
            "Episode: 519 Total reward: 199.0 Training loss: 0.6366 Explore P: 0.0736\n",
            "Episode: 520 Total reward: 199.0 Training loss: 0.5991 Explore P: 0.0724\n",
            "Episode: 521 Total reward: 199.0 Training loss: 0.6076 Explore P: 0.0712\n",
            "Episode: 522 Total reward: 199.0 Training loss: 0.4421 Explore P: 0.0699\n",
            "Episode: 523 Total reward: 199.0 Training loss: 1.6259 Explore P: 0.0688\n",
            "Episode: 524 Total reward: 199.0 Training loss: 0.7865 Explore P: 0.0676\n",
            "Episode: 525 Total reward: 199.0 Training loss: 0.4167 Explore P: 0.0665\n",
            "Episode: 526 Total reward: 199.0 Training loss: 1.0441 Explore P: 0.0654\n",
            "Episode: 527 Total reward: 199.0 Training loss: 1.1207 Explore P: 0.0643\n",
            "Episode: 528 Total reward: 199.0 Training loss: 1.0468 Explore P: 0.0632\n",
            "Episode: 529 Total reward: 199.0 Training loss: 0.7992 Explore P: 0.0622\n",
            "Episode: 530 Total reward: 199.0 Training loss: 1.0118 Explore P: 0.0611\n",
            "Episode: 531 Total reward: 199.0 Training loss: 1.0139 Explore P: 0.0601\n",
            "Episode: 532 Total reward: 199.0 Training loss: 1.0276 Explore P: 0.0591\n",
            "Episode: 533 Total reward: 199.0 Training loss: 1.5191 Explore P: 0.0582\n",
            "Episode: 534 Total reward: 199.0 Training loss: 1.1233 Explore P: 0.0572\n",
            "Episode: 535 Total reward: 199.0 Training loss: 1.7274 Explore P: 0.0563\n",
            "Episode: 536 Total reward: 199.0 Training loss: 1.6739 Explore P: 0.0554\n",
            "Episode: 537 Total reward: 199.0 Training loss: 2.5195 Explore P: 0.0545\n",
            "Episode: 538 Total reward: 199.0 Training loss: 135.2781 Explore P: 0.0536\n",
            "Episode: 539 Total reward: 199.0 Training loss: 282.2395 Explore P: 0.0527\n",
            "Episode: 540 Total reward: 199.0 Training loss: 3.2145 Explore P: 0.0519\n",
            "Episode: 541 Total reward: 199.0 Training loss: 1.5184 Explore P: 0.0511\n",
            "Episode: 542 Total reward: 199.0 Training loss: 1.0023 Explore P: 0.0503\n",
            "Episode: 543 Total reward: 199.0 Training loss: 305.5758 Explore P: 0.0495\n",
            "Episode: 544 Total reward: 199.0 Training loss: 1.5883 Explore P: 0.0487\n",
            "Episode: 545 Total reward: 199.0 Training loss: 1.4743 Explore P: 0.0479\n",
            "Episode: 546 Total reward: 199.0 Training loss: 1.2454 Explore P: 0.0472\n",
            "Episode: 547 Total reward: 199.0 Training loss: 1.3933 Explore P: 0.0465\n",
            "Episode: 548 Total reward: 199.0 Training loss: 1.5895 Explore P: 0.0457\n",
            "Episode: 549 Total reward: 199.0 Training loss: 1.5565 Explore P: 0.0450\n",
            "Episode: 550 Total reward: 199.0 Training loss: 1.7540 Explore P: 0.0443\n",
            "Episode: 551 Total reward: 199.0 Training loss: 1.7545 Explore P: 0.0437\n",
            "Episode: 552 Total reward: 199.0 Training loss: 1.3735 Explore P: 0.0430\n",
            "Episode: 553 Total reward: 136.0 Training loss: 0.6530 Explore P: 0.0426\n",
            "Episode: 554 Total reward: 199.0 Training loss: 0.9817 Explore P: 0.0419\n",
            "Episode: 555 Total reward: 97.0 Training loss: 0.5164 Explore P: 0.0416\n",
            "Episode: 556 Total reward: 11.0 Training loss: 0.6979 Explore P: 0.0416\n",
            "Episode: 557 Total reward: 11.0 Training loss: 1.2719 Explore P: 0.0415\n",
            "Episode: 558 Total reward: 199.0 Training loss: 1.8209 Explore P: 0.0409\n",
            "Episode: 559 Total reward: 199.0 Training loss: 0.7701 Explore P: 0.0403\n",
            "Episode: 560 Total reward: 199.0 Training loss: 0.5714 Explore P: 0.0397\n",
            "Episode: 561 Total reward: 199.0 Training loss: 0.7436 Explore P: 0.0391\n",
            "Episode: 562 Total reward: 199.0 Training loss: 0.5839 Explore P: 0.0385\n",
            "Episode: 563 Total reward: 199.0 Training loss: 0.3504 Explore P: 0.0380\n",
            "Episode: 564 Total reward: 199.0 Training loss: 0.6594 Explore P: 0.0374\n",
            "Episode: 565 Total reward: 199.0 Training loss: 0.4023 Explore P: 0.0369\n",
            "Episode: 566 Total reward: 199.0 Training loss: 4.5397 Explore P: 0.0364\n",
            "Episode: 567 Total reward: 199.0 Training loss: 0.7095 Explore P: 0.0358\n",
            "Episode: 568 Total reward: 199.0 Training loss: 1.9656 Explore P: 0.0353\n",
            "Episode: 569 Total reward: 199.0 Training loss: 1.4475 Explore P: 0.0348\n",
            "Episode: 570 Total reward: 199.0 Training loss: 0.9138 Explore P: 0.0343\n",
            "Episode: 571 Total reward: 199.0 Training loss: 0.5777 Explore P: 0.0339\n",
            "Episode: 572 Total reward: 199.0 Training loss: 1.5033 Explore P: 0.0334\n",
            "Episode: 573 Total reward: 199.0 Training loss: 0.4918 Explore P: 0.0329\n",
            "Episode: 574 Total reward: 199.0 Training loss: 0.7453 Explore P: 0.0325\n",
            "Episode: 575 Total reward: 199.0 Training loss: 4.4111 Explore P: 0.0320\n",
            "Episode: 576 Total reward: 199.0 Training loss: 1.3122 Explore P: 0.0316\n",
            "Episode: 577 Total reward: 199.0 Training loss: 0.9622 Explore P: 0.0312\n",
            "Episode: 578 Total reward: 199.0 Training loss: 0.4231 Explore P: 0.0308\n",
            "Episode: 579 Total reward: 199.0 Training loss: 0.5384 Explore P: 0.0304\n",
            "Episode: 580 Total reward: 199.0 Training loss: 0.6881 Explore P: 0.0300\n",
            "Episode: 581 Total reward: 199.0 Training loss: 1.1688 Explore P: 0.0296\n",
            "Episode: 582 Total reward: 199.0 Training loss: 1.4271 Explore P: 0.0292\n",
            "Episode: 583 Total reward: 199.0 Training loss: 2.0036 Explore P: 0.0288\n",
            "Episode: 584 Total reward: 199.0 Training loss: 3.2270 Explore P: 0.0284\n",
            "Episode: 585 Total reward: 199.0 Training loss: 3.9170 Explore P: 0.0281\n",
            "Episode: 586 Total reward: 19.0 Training loss: 3.4369 Explore P: 0.0280\n",
            "Episode: 587 Total reward: 12.0 Training loss: 4.5900 Explore P: 0.0280\n",
            "Episode: 588 Total reward: 17.0 Training loss: 6.5103 Explore P: 0.0280\n",
            "Episode: 589 Total reward: 199.0 Training loss: 2.2805 Explore P: 0.0276\n",
            "Episode: 590 Total reward: 199.0 Training loss: 3.0682 Explore P: 0.0273\n",
            "Episode: 591 Total reward: 199.0 Training loss: 3.5854 Explore P: 0.0269\n",
            "Episode: 592 Total reward: 11.0 Training loss: 4.2831 Explore P: 0.0269\n",
            "Episode: 593 Total reward: 11.0 Training loss: 2.3879 Explore P: 0.0269\n",
            "Episode: 594 Total reward: 199.0 Training loss: 6.4287 Explore P: 0.0266\n",
            "Episode: 595 Total reward: 11.0 Training loss: 4.6519 Explore P: 0.0265\n",
            "Episode: 596 Total reward: 12.0 Training loss: 5.2677 Explore P: 0.0265\n",
            "Episode: 597 Total reward: 10.0 Training loss: 4.5194 Explore P: 0.0265\n",
            "Episode: 598 Total reward: 8.0 Training loss: 4.5015 Explore P: 0.0265\n",
            "Episode: 599 Total reward: 11.0 Training loss: 440.4348 Explore P: 0.0265\n",
            "Episode: 600 Total reward: 8.0 Training loss: 1353.7697 Explore P: 0.0265\n",
            "Episode: 601 Total reward: 8.0 Training loss: 4.0825 Explore P: 0.0265\n",
            "Episode: 602 Total reward: 11.0 Training loss: 4.7424 Explore P: 0.0264\n",
            "Episode: 603 Total reward: 10.0 Training loss: 4.8746 Explore P: 0.0264\n",
            "Episode: 604 Total reward: 11.0 Training loss: 5.7881 Explore P: 0.0264\n",
            "Episode: 605 Total reward: 9.0 Training loss: 1506.4843 Explore P: 0.0264\n",
            "Episode: 606 Total reward: 13.0 Training loss: 10.9128 Explore P: 0.0264\n",
            "Episode: 607 Total reward: 10.0 Training loss: 10.4778 Explore P: 0.0263\n",
            "Episode: 608 Total reward: 9.0 Training loss: 264.7539 Explore P: 0.0263\n",
            "Episode: 609 Total reward: 10.0 Training loss: 8.7213 Explore P: 0.0263\n",
            "Episode: 610 Total reward: 13.0 Training loss: 7.3166 Explore P: 0.0263\n",
            "Episode: 611 Total reward: 10.0 Training loss: 42.1162 Explore P: 0.0263\n",
            "Episode: 612 Total reward: 10.0 Training loss: 7.7779 Explore P: 0.0263\n",
            "Episode: 613 Total reward: 10.0 Training loss: 9.8508 Explore P: 0.0262\n",
            "Episode: 614 Total reward: 8.0 Training loss: 7.5503 Explore P: 0.0262\n",
            "Episode: 615 Total reward: 11.0 Training loss: 16.1736 Explore P: 0.0262\n",
            "Episode: 616 Total reward: 9.0 Training loss: 8.1205 Explore P: 0.0262\n",
            "Episode: 617 Total reward: 13.0 Training loss: 4.2153 Explore P: 0.0262\n",
            "Episode: 618 Total reward: 7.0 Training loss: 5.5490 Explore P: 0.0262\n",
            "Episode: 619 Total reward: 8.0 Training loss: 4.2373 Explore P: 0.0262\n",
            "Episode: 620 Total reward: 9.0 Training loss: 1377.9202 Explore P: 0.0261\n",
            "Episode: 621 Total reward: 8.0 Training loss: 3.4765 Explore P: 0.0261\n",
            "Episode: 622 Total reward: 12.0 Training loss: 5.6831 Explore P: 0.0261\n",
            "Episode: 623 Total reward: 9.0 Training loss: 3.7647 Explore P: 0.0261\n",
            "Episode: 624 Total reward: 10.0 Training loss: 5.1277 Explore P: 0.0261\n",
            "Episode: 625 Total reward: 9.0 Training loss: 3.3140 Explore P: 0.0261\n",
            "Episode: 626 Total reward: 11.0 Training loss: 4.5649 Explore P: 0.0260\n",
            "Episode: 627 Total reward: 7.0 Training loss: 9.7226 Explore P: 0.0260\n",
            "Episode: 628 Total reward: 10.0 Training loss: 185.5013 Explore P: 0.0260\n",
            "Episode: 629 Total reward: 11.0 Training loss: 4.7156 Explore P: 0.0260\n",
            "Episode: 630 Total reward: 9.0 Training loss: 3.9771 Explore P: 0.0260\n",
            "Episode: 631 Total reward: 56.0 Training loss: 5.7770 Explore P: 0.0259\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}