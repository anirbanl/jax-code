{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jax_double_dqn_cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anirbanl/jax-code/blob/master/rlflax/qlearning/jax_double_dqn_cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6rIyAnYHQvM"
      },
      "source": [
        "Based on sources:\n",
        "1. https://arxiv.org/pdf/1509.06461.pdf\n",
        "2. https://github.com/higgsfield/RL-Adventure/blob/master/2.double%20dqn.ipynb\n",
        "3. https://medium.com/@parsa_h_m/deep-reinforcement-learning-dqn-double-dqn-dueling-dqn-noisy-dqn-and-dqn-with-prioritized-551f621a9823"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_jS9qg6EG4l",
        "outputId": "36a278e3-360a-43d3-a498-0db3c53085d9"
      },
      "source": [
        "!pip install jax jaxlib flax"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (0.2.13)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (0.1.66+cuda110)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.7/dist-packages (0.3.4)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from jax) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax) (0.12.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.12)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.4.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax) (1.0.2)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (from flax) (0.0.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax) (3.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax) (1.15.0)\n",
            "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax->flax) (0.0.7)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.8.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.11.1)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.1.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt1r4--nEPJm"
      },
      "source": [
        "import gym\n",
        "gym.logger.set_level(40) # suppress warnings (please remove if gives error)\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import jax\n",
        "import jax.numpy as jp\n",
        "from jax.ops import index, index_add, index_update\n",
        "from jax import jit, grad, vmap, random, jacrev, jacobian, jacfwd, value_and_grad\n",
        "from functools import partial\n",
        "from jax.tree_util import tree_multimap  # Element-wise manipulation of collections of numpy arrays\n",
        "from flax import linen as nn           # The Linen API\n",
        "from flax.training import train_state  # Useful dataclass to keep train state\n",
        "import optax                           # Optimizers\n",
        "from typing import Sequence\n",
        "import copy"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK9XAT81EWW0"
      },
      "source": [
        "from collections import deque\n",
        "class Memory():\n",
        "    def __init__(self, max_size = 1000):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "    \n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "            \n",
        "    def sample(self, key, batch_size):\n",
        "        key, _ = jax.random.split(key)\n",
        "        idx = jax.random.choice(key,\n",
        "                               jp.arange(len(self.buffer)), \n",
        "                               shape=(batch_size, ))\n",
        "        return [self.buffer[ii] for ii in idx]"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtUX4AXiFWVK"
      },
      "source": [
        "train_episodes = 1000          # max number of episodes to learn from\n",
        "max_steps = 200                # max steps in an episode\n",
        "gamma = 0.99                   # future reward discount\n",
        "update_target_every = 20       # Update target Q model every this episodes\n",
        "\n",
        "# Exploration parameters\n",
        "explore_start = 1.0            # exploration probability at start\n",
        "explore_stop = 0.01            # minimum exploration probability \n",
        "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
        "\n",
        "# Network parameters\n",
        "hidden_size = 64               # number of units in each Q-network hidden layer\n",
        "learning_rate = 1e-4         # Q-network learning rate\n",
        "\n",
        "# Memory parameters\n",
        "memory_size = 10000            # memory capacity\n",
        "batch_size = 20                # experience mini-batch size\n",
        "pretrain_length = batch_size   # number experiences to pretrain the memory"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSJfnThHFouD"
      },
      "source": [
        "#Define Q-network\n",
        "class QNetwork:\n",
        "    def __init__(self, rng, env, learning_rate=0.01, state_size=4, \n",
        "                 action_size=2, hidden_size=10, \n",
        "                 name='QNetwork'):\n",
        "        self.key = rng\n",
        "        self.env = env\n",
        "\n",
        "        class Model(nn.Module):\n",
        "            features: Sequence[int]\n",
        "\n",
        "            @nn.compact\n",
        "            def __call__(self, x):\n",
        "                x = nn.relu(nn.Dense(self.features[0])(x))\n",
        "                x = nn.relu(nn.Dense(self.features[1])(x))\n",
        "                x = nn.Dense(self.features[2])(x)\n",
        "                return x\n",
        "\n",
        "        def create_train_state(rng, learning_rate, s_size, h_size, a_size):\n",
        "            \"\"\"Creates initial `TrainState`.\"\"\"\n",
        "            model = Model(features=[hidden_size, hidden_size, a_size])\n",
        "            params = model.init(rng, jp.ones((s_size, )))#['params']\n",
        "            tx = optax.adam(learning_rate)\n",
        "            return train_state.TrainState.create(\n",
        "                apply_fn=model.apply, params=params, tx=tx)\n",
        "\n",
        "        self.ts = create_train_state(rng, learning_rate, state_size, hidden_size, action_size)\n",
        "\n",
        "        # @ jit\n",
        "        def train_step(ts, inputs, actions, targets):\n",
        "\n",
        "            def loss_fun(params, inputs, actions, targets):\n",
        "                output = ts.apply_fn(params, inputs)\n",
        "                selectedq = jp.sum(actions*output, axis=-1)\n",
        "                # print(f\"qvalues:{selectedq}\")\n",
        "                # print(f\"targets:{targets}\")\n",
        "                diff = selectedq - jax.lax.stop_gradient(targets)\n",
        "                # print(f\"diff:{diff}\")\n",
        "                # print(f\"diffsqsum:{jp.sum(diff**2)}\")\n",
        "                return jp.mean(diff**2)\n",
        "\n",
        "            loss, g = value_and_grad(loss_fun)(ts.params, inputs, actions, targets)\n",
        "            return ts.apply_gradients(grads=g), loss\n",
        "\n",
        "        self.train_fn = train_step\n",
        "\n",
        "\n",
        "    def act(self, state, explore_p):\n",
        "        self.key, _ = jax.random.split(self.key)\n",
        "        uf = jax.random.uniform(self.key, (1,), minval=0.0, maxval=1.0)[0]\n",
        "        if explore_p > uf:\n",
        "            # Make a random action\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            # Get action from Q-network\n",
        "            qvalues = self.ts.apply_fn(self.ts.params, state)\n",
        "            action = jp.argmax(qvalues).item()\n",
        "        return action\n",
        "\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0fPvqRcLdf4"
      },
      "source": [
        "def init_memory(env):\n",
        "    # Initialize the simulation\n",
        "    env.reset()\n",
        "    # Take one random step to get the pole and cart moving\n",
        "    state, reward, done, _ = env.step(env.action_space.sample())\n",
        "\n",
        "    memory = Memory(max_size=memory_size)\n",
        "\n",
        "    # Make a bunch of random actions and store the experiences\n",
        "    for ii in range(pretrain_length):\n",
        "        # Uncomment the line below to watch the simulation\n",
        "        # env.render()\n",
        "\n",
        "        # Make a random action\n",
        "        action = env.action_space.sample()\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        if done:\n",
        "            # The simulation fails so no next state\n",
        "            next_state = jp.zeros(state.shape)\n",
        "            # Add experience to memory\n",
        "            memory.add((state, action, reward, next_state))\n",
        "            \n",
        "            # Start new episode\n",
        "            env.reset()\n",
        "            # Take one random step to get the pole and cart moving\n",
        "            state, reward, done, _ = env.step(env.action_space.sample())\n",
        "        else:\n",
        "            # Add experience to memory\n",
        "            memory.add((state, action, reward, next_state))\n",
        "            state = next_state\n",
        "    return memory, state"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vpr3LNUMkaG"
      },
      "source": [
        "# Now train with experiences\n",
        "def one_hot(x, k, dtype=jp.float32):\n",
        "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
        "  return jp.array(x[:, None] == jp.arange(k), dtype)\n",
        "\n",
        "\n",
        "def train(rng, env, mainQN):\n",
        "    rewards_list = []    \n",
        "    step = 0\n",
        "    memory, state = init_memory(env)\n",
        "    current_params = mainQN.ts.params\n",
        "    target_params = copy.deepcopy(mainQN.ts.params)\n",
        "    for ep in range(1, train_episodes):\n",
        "        total_reward = 0\n",
        "        t = 0\n",
        "        while t < max_steps:\n",
        "            step += 1\n",
        "            # Uncomment this next line to watch the training\n",
        "            # env.render() \n",
        "            \n",
        "            # Explore or Exploit\n",
        "            explore_p = explore_stop + (explore_start - explore_stop)*jp.exp(-decay_rate*step) \n",
        "            action = mainQN.act(state, explore_p)\n",
        "            \n",
        "            # Take action, get new state and reward\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            total_reward += reward\n",
        "            \n",
        "            if done:\n",
        "                # the episode ends so no next state\n",
        "                next_state = jp.zeros(state.shape)\n",
        "                t = max_steps\n",
        "                \n",
        "                print('Episode: {}'.format(ep),\n",
        "                    'Total reward: {}'.format(total_reward),\n",
        "                    'Training loss: {:.4f}'.format(loss),\n",
        "                    'Explore P: {:.4f}'.format(explore_p))\n",
        "                rewards_list.append((ep, total_reward))\n",
        "                \n",
        "                # Add experience to memory\n",
        "                memory.add((state, action, reward, next_state))\n",
        "                \n",
        "                # Start new episode\n",
        "                env.reset()\n",
        "                # Take one random step to get the pole and cart moving\n",
        "                state, reward, done, _ = env.step(env.action_space.sample())\n",
        "\n",
        "            else:\n",
        "                # Add experience to memory\n",
        "                memory.add((state, action, reward, next_state))\n",
        "                state = next_state\n",
        "                t += 1\n",
        "            \n",
        "            # Sample mini-batch from memory\n",
        "            batch = memory.sample(rng, batch_size)\n",
        "            states = jp.array([each[0] for each in batch])\n",
        "            actions = one_hot(jp.array([each[1] for each in batch]), 2)\n",
        "            rewards = jp.array([each[2] for each in batch])\n",
        "            next_states = jp.array([each[3] for each in batch])\n",
        "            \n",
        "            # Train network\n",
        "            current_Qs = mainQN.ts.apply_fn(current_params, next_states)\n",
        "            target_Qs = mainQN.ts.apply_fn(target_params, next_states)\n",
        "            \n",
        "            # Set target_Qs to 0 for states where episode ends\n",
        "            episode_ends = (next_states == jp.zeros(states[0].shape)).all(axis=1)\n",
        "            new_target_Qs = index_update(target_Qs, index[episode_ends], (0, 0))\n",
        "            target_Qs = new_target_Qs\n",
        "            \n",
        "            max_current_Qs_indices = jp.argmax(current_Qs, axis=-1)\n",
        "            targets = rewards + gamma * jp.take_along_axis(target_Qs, max_current_Qs_indices[..., None], axis=-1).squeeze(axis=-1)\n",
        "            # print(states.shape, targets.shape, targets)\n",
        "            mainQN.ts, loss = mainQN.train_fn(mainQN.ts, states, actions, targets)\n",
        "            current_params = mainQN.ts.params\n",
        "\n",
        "        if ep % update_target_every == 0:\n",
        "            target_params = copy.deepcopy(mainQN.ts.params)\n",
        "            print('***** Updated target QNetwork *****')\n",
        "\n",
        "    return rewards_list\n",
        "\n",
        "def plot_scores(rewards_list):\n",
        "    def running_mean(x, N):\n",
        "        cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
        "        return (cumsum[N:] - cumsum[:-N]) / N\n",
        "    eps, rews = np.array(rewards_list).T\n",
        "    smoothed_rews = running_mean(rews, 10)\n",
        "    plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
        "    plt.plot(eps, rews, color='grey', alpha=0.3)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.show()"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeurKJ6oNcZF",
        "outputId": "bdf7a7a4-6751-4b0a-d725-a84e68c910f1"
      },
      "source": [
        "def main():\n",
        "    seed = 0\n",
        "    env = gym.make('CartPole-v0')\n",
        "    env.seed(seed)\n",
        "    env.action_space.seed(seed)\n",
        "    print('observation space:', env.observation_space)\n",
        "    print('action space:', env.action_space.n)\n",
        "    rng = jax.random.PRNGKey(seed)\n",
        "    mainQN = QNetwork(rng, env, name='main', hidden_size=hidden_size, learning_rate=learning_rate)\n",
        "    rewards_list = train(rng, env, mainQN)\n",
        "    plot_scores(rewards_list)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
            "action space: 2\n",
            "Episode: 1 Total reward: 31.0 Training loss: 1.5479 Explore P: 0.9969\n",
            "Episode: 2 Total reward: 54.0 Training loss: 0.9487 Explore P: 0.9916\n",
            "Episode: 3 Total reward: 28.0 Training loss: 0.9809 Explore P: 0.9889\n",
            "Episode: 4 Total reward: 21.0 Training loss: 0.6974 Explore P: 0.9868\n",
            "Episode: 5 Total reward: 8.0 Training loss: 0.6927 Explore P: 0.9860\n",
            "Episode: 6 Total reward: 13.0 Training loss: 0.9124 Explore P: 0.9848\n",
            "Episode: 7 Total reward: 20.0 Training loss: 0.5460 Explore P: 0.9828\n",
            "Episode: 8 Total reward: 35.0 Training loss: 0.4937 Explore P: 0.9794\n",
            "Episode: 9 Total reward: 27.0 Training loss: 0.4373 Explore P: 0.9768\n",
            "Episode: 10 Total reward: 22.0 Training loss: 0.2370 Explore P: 0.9747\n",
            "Episode: 11 Total reward: 15.0 Training loss: 0.3728 Explore P: 0.9732\n",
            "Episode: 12 Total reward: 28.0 Training loss: 0.3618 Explore P: 0.9705\n",
            "Episode: 13 Total reward: 10.0 Training loss: 0.3282 Explore P: 0.9696\n",
            "Episode: 14 Total reward: 16.0 Training loss: 0.2790 Explore P: 0.9681\n",
            "Episode: 15 Total reward: 10.0 Training loss: 0.2375 Explore P: 0.9671\n",
            "Episode: 16 Total reward: 21.0 Training loss: 0.2656 Explore P: 0.9651\n",
            "Episode: 17 Total reward: 15.0 Training loss: 0.9342 Explore P: 0.9637\n",
            "Episode: 18 Total reward: 10.0 Training loss: 0.6732 Explore P: 0.9627\n",
            "Episode: 19 Total reward: 33.0 Training loss: 0.1871 Explore P: 0.9596\n",
            "Episode: 20 Total reward: 26.0 Training loss: 0.2495 Explore P: 0.9571\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 21 Total reward: 17.0 Training loss: 1.0015 Explore P: 0.9555\n",
            "Episode: 22 Total reward: 11.0 Training loss: 0.8516 Explore P: 0.9545\n",
            "Episode: 23 Total reward: 23.0 Training loss: 0.8972 Explore P: 0.9523\n",
            "Episode: 24 Total reward: 14.0 Training loss: 0.7223 Explore P: 0.9510\n",
            "Episode: 25 Total reward: 18.0 Training loss: 0.5316 Explore P: 0.9493\n",
            "Episode: 26 Total reward: 27.0 Training loss: 0.5526 Explore P: 0.9467\n",
            "Episode: 27 Total reward: 17.0 Training loss: 0.4953 Explore P: 0.9451\n",
            "Episode: 28 Total reward: 31.0 Training loss: 1.3345 Explore P: 0.9423\n",
            "Episode: 29 Total reward: 22.0 Training loss: 0.3722 Explore P: 0.9402\n",
            "Episode: 30 Total reward: 12.0 Training loss: 0.4071 Explore P: 0.9391\n",
            "Episode: 31 Total reward: 33.0 Training loss: 0.3971 Explore P: 0.9360\n",
            "Episode: 32 Total reward: 12.0 Training loss: 0.4032 Explore P: 0.9349\n",
            "Episode: 33 Total reward: 24.0 Training loss: 1.6486 Explore P: 0.9327\n",
            "Episode: 34 Total reward: 34.0 Training loss: 1.1113 Explore P: 0.9296\n",
            "Episode: 35 Total reward: 17.0 Training loss: 0.1281 Explore P: 0.9280\n",
            "Episode: 36 Total reward: 19.0 Training loss: 0.2125 Explore P: 0.9263\n",
            "Episode: 37 Total reward: 11.0 Training loss: 0.5802 Explore P: 0.9253\n",
            "Episode: 38 Total reward: 29.0 Training loss: 0.2562 Explore P: 0.9226\n",
            "Episode: 39 Total reward: 12.0 Training loss: 0.6421 Explore P: 0.9215\n",
            "Episode: 40 Total reward: 15.0 Training loss: 0.1537 Explore P: 0.9201\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 41 Total reward: 9.0 Training loss: 3.4693 Explore P: 0.9193\n",
            "Episode: 42 Total reward: 16.0 Training loss: 1.0210 Explore P: 0.9179\n",
            "Episode: 43 Total reward: 22.0 Training loss: 0.9385 Explore P: 0.9159\n",
            "Episode: 44 Total reward: 34.0 Training loss: 0.2720 Explore P: 0.9128\n",
            "Episode: 45 Total reward: 12.0 Training loss: 0.2427 Explore P: 0.9117\n",
            "Episode: 46 Total reward: 16.0 Training loss: 0.2517 Explore P: 0.9103\n",
            "Episode: 47 Total reward: 12.0 Training loss: 0.3471 Explore P: 0.9092\n",
            "Episode: 48 Total reward: 12.0 Training loss: 0.3350 Explore P: 0.9081\n",
            "Episode: 49 Total reward: 25.0 Training loss: 0.7561 Explore P: 0.9059\n",
            "Episode: 50 Total reward: 20.0 Training loss: 1.9285 Explore P: 0.9041\n",
            "Episode: 51 Total reward: 71.0 Training loss: 0.3771 Explore P: 0.8978\n",
            "Episode: 52 Total reward: 23.0 Training loss: 0.1884 Explore P: 0.8957\n",
            "Episode: 53 Total reward: 9.0 Training loss: 3.7008 Explore P: 0.8949\n",
            "Episode: 54 Total reward: 22.0 Training loss: 0.1795 Explore P: 0.8930\n",
            "Episode: 55 Total reward: 18.0 Training loss: 1.8170 Explore P: 0.8914\n",
            "Episode: 56 Total reward: 15.0 Training loss: 0.1471 Explore P: 0.8901\n",
            "Episode: 57 Total reward: 13.0 Training loss: 1.7301 Explore P: 0.8889\n",
            "Episode: 58 Total reward: 15.0 Training loss: 0.1005 Explore P: 0.8876\n",
            "Episode: 59 Total reward: 16.0 Training loss: 1.4592 Explore P: 0.8862\n",
            "Episode: 60 Total reward: 10.0 Training loss: 0.6156 Explore P: 0.8853\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 61 Total reward: 11.0 Training loss: 1.8389 Explore P: 0.8844\n",
            "Episode: 62 Total reward: 13.0 Training loss: 1.3982 Explore P: 0.8832\n",
            "Episode: 63 Total reward: 15.0 Training loss: 0.8267 Explore P: 0.8819\n",
            "Episode: 64 Total reward: 11.0 Training loss: 0.4564 Explore P: 0.8810\n",
            "Episode: 65 Total reward: 25.0 Training loss: 0.3699 Explore P: 0.8788\n",
            "Episode: 66 Total reward: 10.0 Training loss: 0.3042 Explore P: 0.8779\n",
            "Episode: 67 Total reward: 22.0 Training loss: 1.3115 Explore P: 0.8760\n",
            "Episode: 68 Total reward: 9.0 Training loss: 2.8739 Explore P: 0.8752\n",
            "Episode: 69 Total reward: 8.0 Training loss: 0.7313 Explore P: 0.8745\n",
            "Episode: 70 Total reward: 25.0 Training loss: 2.4882 Explore P: 0.8724\n",
            "Episode: 71 Total reward: 13.0 Training loss: 0.4874 Explore P: 0.8713\n",
            "Episode: 72 Total reward: 21.0 Training loss: 2.4984 Explore P: 0.8695\n",
            "Episode: 73 Total reward: 14.0 Training loss: 1.5154 Explore P: 0.8683\n",
            "Episode: 74 Total reward: 35.0 Training loss: 0.8718 Explore P: 0.8653\n",
            "Episode: 75 Total reward: 14.0 Training loss: 0.1453 Explore P: 0.8641\n",
            "Episode: 76 Total reward: 13.0 Training loss: 2.5370 Explore P: 0.8630\n",
            "Episode: 77 Total reward: 11.0 Training loss: 0.1301 Explore P: 0.8620\n",
            "Episode: 78 Total reward: 14.0 Training loss: 1.0688 Explore P: 0.8608\n",
            "Episode: 79 Total reward: 45.0 Training loss: 3.6868 Explore P: 0.8570\n",
            "Episode: 80 Total reward: 9.0 Training loss: 1.6677 Explore P: 0.8562\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 81 Total reward: 15.0 Training loss: 1.3944 Explore P: 0.8550\n",
            "Episode: 82 Total reward: 24.0 Training loss: 0.7109 Explore P: 0.8529\n",
            "Episode: 83 Total reward: 17.0 Training loss: 2.8962 Explore P: 0.8515\n",
            "Episode: 84 Total reward: 13.0 Training loss: 0.3004 Explore P: 0.8504\n",
            "Episode: 85 Total reward: 16.0 Training loss: 1.8605 Explore P: 0.8491\n",
            "Episode: 86 Total reward: 17.0 Training loss: 3.5993 Explore P: 0.8477\n",
            "Episode: 87 Total reward: 10.0 Training loss: 1.6290 Explore P: 0.8468\n",
            "Episode: 88 Total reward: 34.0 Training loss: 4.8412 Explore P: 0.8440\n",
            "Episode: 89 Total reward: 13.0 Training loss: 3.7504 Explore P: 0.8429\n",
            "Episode: 90 Total reward: 19.0 Training loss: 2.7724 Explore P: 0.8413\n",
            "Episode: 91 Total reward: 9.0 Training loss: 7.3289 Explore P: 0.8406\n",
            "Episode: 92 Total reward: 50.0 Training loss: 1.5082 Explore P: 0.8364\n",
            "Episode: 93 Total reward: 16.0 Training loss: 2.6346 Explore P: 0.8351\n",
            "Episode: 94 Total reward: 17.0 Training loss: 1.2149 Explore P: 0.8337\n",
            "Episode: 95 Total reward: 16.0 Training loss: 0.2431 Explore P: 0.8324\n",
            "Episode: 96 Total reward: 9.0 Training loss: 0.1393 Explore P: 0.8316\n",
            "Episode: 97 Total reward: 22.0 Training loss: 3.6281 Explore P: 0.8298\n",
            "Episode: 98 Total reward: 46.0 Training loss: 0.1616 Explore P: 0.8261\n",
            "Episode: 99 Total reward: 17.0 Training loss: 0.6472 Explore P: 0.8247\n",
            "Episode: 100 Total reward: 15.0 Training loss: 5.1726 Explore P: 0.8235\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 101 Total reward: 36.0 Training loss: 1.7897 Explore P: 0.8205\n",
            "Episode: 102 Total reward: 10.0 Training loss: 3.5810 Explore P: 0.8197\n",
            "Episode: 103 Total reward: 11.0 Training loss: 0.3499 Explore P: 0.8188\n",
            "Episode: 104 Total reward: 11.0 Training loss: 6.2774 Explore P: 0.8180\n",
            "Episode: 105 Total reward: 9.0 Training loss: 0.2864 Explore P: 0.8172\n",
            "Episode: 106 Total reward: 17.0 Training loss: 4.5845 Explore P: 0.8159\n",
            "Episode: 107 Total reward: 17.0 Training loss: 6.2026 Explore P: 0.8145\n",
            "Episode: 108 Total reward: 19.0 Training loss: 1.6036 Explore P: 0.8130\n",
            "Episode: 109 Total reward: 12.0 Training loss: 1.6324 Explore P: 0.8120\n",
            "Episode: 110 Total reward: 14.0 Training loss: 2.3140 Explore P: 0.8109\n",
            "Episode: 111 Total reward: 11.0 Training loss: 1.4090 Explore P: 0.8100\n",
            "Episode: 112 Total reward: 16.0 Training loss: 2.8112 Explore P: 0.8087\n",
            "Episode: 113 Total reward: 10.0 Training loss: 0.2630 Explore P: 0.8079\n",
            "Episode: 114 Total reward: 10.0 Training loss: 1.4565 Explore P: 0.8071\n",
            "Episode: 115 Total reward: 8.0 Training loss: 1.8213 Explore P: 0.8065\n",
            "Episode: 116 Total reward: 16.0 Training loss: 0.2670 Explore P: 0.8052\n",
            "Episode: 117 Total reward: 9.0 Training loss: 3.4283 Explore P: 0.8045\n",
            "Episode: 118 Total reward: 19.0 Training loss: 2.4825 Explore P: 0.8030\n",
            "Episode: 119 Total reward: 21.0 Training loss: 0.1777 Explore P: 0.8013\n",
            "Episode: 120 Total reward: 12.0 Training loss: 0.1666 Explore P: 0.8004\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 121 Total reward: 10.0 Training loss: 1.6641 Explore P: 0.7996\n",
            "Episode: 122 Total reward: 11.0 Training loss: 3.6331 Explore P: 0.7987\n",
            "Episode: 123 Total reward: 9.0 Training loss: 2.0355 Explore P: 0.7980\n",
            "Episode: 124 Total reward: 10.0 Training loss: 4.1114 Explore P: 0.7972\n",
            "Episode: 125 Total reward: 15.0 Training loss: 0.3721 Explore P: 0.7960\n",
            "Episode: 126 Total reward: 24.0 Training loss: 0.2550 Explore P: 0.7942\n",
            "Episode: 127 Total reward: 22.0 Training loss: 4.7521 Explore P: 0.7924\n",
            "Episode: 128 Total reward: 19.0 Training loss: 0.3531 Explore P: 0.7909\n",
            "Episode: 129 Total reward: 26.0 Training loss: 6.0258 Explore P: 0.7889\n",
            "Episode: 130 Total reward: 17.0 Training loss: 2.3470 Explore P: 0.7876\n",
            "Episode: 131 Total reward: 10.0 Training loss: 5.4554 Explore P: 0.7868\n",
            "Episode: 132 Total reward: 9.0 Training loss: 0.5298 Explore P: 0.7861\n",
            "Episode: 133 Total reward: 17.0 Training loss: 2.2050 Explore P: 0.7848\n",
            "Episode: 134 Total reward: 11.0 Training loss: 2.4860 Explore P: 0.7839\n",
            "Episode: 135 Total reward: 12.0 Training loss: 2.5731 Explore P: 0.7830\n",
            "Episode: 136 Total reward: 12.0 Training loss: 2.1748 Explore P: 0.7821\n",
            "Episode: 137 Total reward: 14.0 Training loss: 3.6842 Explore P: 0.7810\n",
            "Episode: 138 Total reward: 17.0 Training loss: 3.2968 Explore P: 0.7797\n",
            "Episode: 139 Total reward: 10.0 Training loss: 4.1779 Explore P: 0.7789\n",
            "Episode: 140 Total reward: 12.0 Training loss: 0.2595 Explore P: 0.7780\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 141 Total reward: 23.0 Training loss: 2.3714 Explore P: 0.7762\n",
            "Episode: 142 Total reward: 13.0 Training loss: 6.9786 Explore P: 0.7753\n",
            "Episode: 143 Total reward: 53.0 Training loss: 0.3313 Explore P: 0.7712\n",
            "Episode: 144 Total reward: 10.0 Training loss: 3.7369 Explore P: 0.7704\n",
            "Episode: 145 Total reward: 24.0 Training loss: 0.2483 Explore P: 0.7686\n",
            "Episode: 146 Total reward: 17.0 Training loss: 0.3459 Explore P: 0.7673\n",
            "Episode: 147 Total reward: 12.0 Training loss: 2.2585 Explore P: 0.7664\n",
            "Episode: 148 Total reward: 25.0 Training loss: 4.8632 Explore P: 0.7645\n",
            "Episode: 149 Total reward: 15.0 Training loss: 2.0962 Explore P: 0.7634\n",
            "Episode: 150 Total reward: 11.0 Training loss: 0.4489 Explore P: 0.7626\n",
            "Episode: 151 Total reward: 10.0 Training loss: 2.1531 Explore P: 0.7618\n",
            "Episode: 152 Total reward: 25.0 Training loss: 8.3269 Explore P: 0.7599\n",
            "Episode: 153 Total reward: 11.0 Training loss: 0.4383 Explore P: 0.7591\n",
            "Episode: 154 Total reward: 9.0 Training loss: 2.2787 Explore P: 0.7585\n",
            "Episode: 155 Total reward: 10.0 Training loss: 2.0391 Explore P: 0.7577\n",
            "Episode: 156 Total reward: 8.0 Training loss: 0.1774 Explore P: 0.7571\n",
            "Episode: 157 Total reward: 14.0 Training loss: 0.4433 Explore P: 0.7561\n",
            "Episode: 158 Total reward: 19.0 Training loss: 4.4253 Explore P: 0.7546\n",
            "Episode: 159 Total reward: 28.0 Training loss: 0.3880 Explore P: 0.7526\n",
            "Episode: 160 Total reward: 25.0 Training loss: 4.2276 Explore P: 0.7507\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 161 Total reward: 9.0 Training loss: 2.8465 Explore P: 0.7500\n",
            "Episode: 162 Total reward: 11.0 Training loss: 4.2343 Explore P: 0.7492\n",
            "Episode: 163 Total reward: 19.0 Training loss: 0.3779 Explore P: 0.7478\n",
            "Episode: 164 Total reward: 17.0 Training loss: 0.3577 Explore P: 0.7466\n",
            "Episode: 165 Total reward: 25.0 Training loss: 7.4496 Explore P: 0.7447\n",
            "Episode: 166 Total reward: 32.0 Training loss: 0.2983 Explore P: 0.7424\n",
            "Episode: 167 Total reward: 11.0 Training loss: 3.8215 Explore P: 0.7416\n",
            "Episode: 168 Total reward: 18.0 Training loss: 5.7432 Explore P: 0.7403\n",
            "Episode: 169 Total reward: 14.0 Training loss: 4.4585 Explore P: 0.7392\n",
            "Episode: 170 Total reward: 11.0 Training loss: 7.4181 Explore P: 0.7384\n",
            "Episode: 171 Total reward: 10.0 Training loss: 10.4153 Explore P: 0.7377\n",
            "Episode: 172 Total reward: 24.0 Training loss: 0.3569 Explore P: 0.7360\n",
            "Episode: 173 Total reward: 11.0 Training loss: 2.4835 Explore P: 0.7352\n",
            "Episode: 174 Total reward: 9.0 Training loss: 1.9875 Explore P: 0.7345\n",
            "Episode: 175 Total reward: 11.0 Training loss: 2.0935 Explore P: 0.7337\n",
            "Episode: 176 Total reward: 28.0 Training loss: 2.7163 Explore P: 0.7317\n",
            "Episode: 177 Total reward: 11.0 Training loss: 2.1274 Explore P: 0.7309\n",
            "Episode: 178 Total reward: 12.0 Training loss: 5.2460 Explore P: 0.7300\n",
            "Episode: 179 Total reward: 79.0 Training loss: 1.4475 Explore P: 0.7244\n",
            "Episode: 180 Total reward: 12.0 Training loss: 0.1855 Explore P: 0.7235\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 181 Total reward: 14.0 Training loss: 0.8751 Explore P: 0.7225\n",
            "Episode: 182 Total reward: 32.0 Training loss: 0.1741 Explore P: 0.7202\n",
            "Episode: 183 Total reward: 11.0 Training loss: 2.0055 Explore P: 0.7195\n",
            "Episode: 184 Total reward: 21.0 Training loss: 4.0259 Explore P: 0.7180\n",
            "Episode: 185 Total reward: 17.0 Training loss: 4.2331 Explore P: 0.7168\n",
            "Episode: 186 Total reward: 19.0 Training loss: 3.5432 Explore P: 0.7154\n",
            "Episode: 187 Total reward: 31.0 Training loss: 2.9962 Explore P: 0.7132\n",
            "Episode: 188 Total reward: 29.0 Training loss: 1.5014 Explore P: 0.7112\n",
            "Episode: 189 Total reward: 27.0 Training loss: 4.7442 Explore P: 0.7093\n",
            "Episode: 190 Total reward: 9.0 Training loss: 1.5603 Explore P: 0.7087\n",
            "Episode: 191 Total reward: 12.0 Training loss: 8.0005 Explore P: 0.7079\n",
            "Episode: 192 Total reward: 18.0 Training loss: 6.8649 Explore P: 0.7066\n",
            "Episode: 193 Total reward: 19.0 Training loss: 5.9371 Explore P: 0.7053\n",
            "Episode: 194 Total reward: 28.0 Training loss: 2.8543 Explore P: 0.7033\n",
            "Episode: 195 Total reward: 13.0 Training loss: 3.7130 Explore P: 0.7024\n",
            "Episode: 196 Total reward: 35.0 Training loss: 4.0005 Explore P: 0.7000\n",
            "Episode: 197 Total reward: 13.0 Training loss: 6.7240 Explore P: 0.6991\n",
            "Episode: 198 Total reward: 25.0 Training loss: 2.8996 Explore P: 0.6974\n",
            "Episode: 199 Total reward: 15.0 Training loss: 3.1783 Explore P: 0.6964\n",
            "Episode: 200 Total reward: 14.0 Training loss: 4.3902 Explore P: 0.6954\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 201 Total reward: 13.0 Training loss: 0.7886 Explore P: 0.6945\n",
            "Episode: 202 Total reward: 20.0 Training loss: 3.9331 Explore P: 0.6931\n",
            "Episode: 203 Total reward: 15.0 Training loss: 4.5004 Explore P: 0.6921\n",
            "Episode: 204 Total reward: 55.0 Training loss: 6.6305 Explore P: 0.6884\n",
            "Episode: 205 Total reward: 55.0 Training loss: 0.4322 Explore P: 0.6847\n",
            "Episode: 206 Total reward: 48.0 Training loss: 0.3509 Explore P: 0.6814\n",
            "Episode: 207 Total reward: 63.0 Training loss: 5.4299 Explore P: 0.6772\n",
            "Episode: 208 Total reward: 83.0 Training loss: 1.2485 Explore P: 0.6717\n",
            "Episode: 209 Total reward: 24.0 Training loss: 2.1400 Explore P: 0.6701\n",
            "Episode: 210 Total reward: 34.0 Training loss: 0.3654 Explore P: 0.6679\n",
            "Episode: 211 Total reward: 54.0 Training loss: 1.9311 Explore P: 0.6643\n",
            "Episode: 212 Total reward: 23.0 Training loss: 0.5212 Explore P: 0.6628\n",
            "Episode: 213 Total reward: 72.0 Training loss: 0.3976 Explore P: 0.6581\n",
            "Episode: 214 Total reward: 13.0 Training loss: 1.7729 Explore P: 0.6573\n",
            "Episode: 215 Total reward: 59.0 Training loss: 3.2167 Explore P: 0.6535\n",
            "Episode: 216 Total reward: 34.0 Training loss: 0.3744 Explore P: 0.6513\n",
            "Episode: 217 Total reward: 46.0 Training loss: 0.2988 Explore P: 0.6484\n",
            "Episode: 218 Total reward: 28.0 Training loss: 3.8573 Explore P: 0.6466\n",
            "Episode: 219 Total reward: 16.0 Training loss: 0.2844 Explore P: 0.6456\n",
            "Episode: 220 Total reward: 16.0 Training loss: 1.5182 Explore P: 0.6445\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 221 Total reward: 34.0 Training loss: 0.2792 Explore P: 0.6424\n",
            "Episode: 222 Total reward: 38.0 Training loss: 0.2450 Explore P: 0.6400\n",
            "Episode: 223 Total reward: 49.0 Training loss: 0.2093 Explore P: 0.6369\n",
            "Episode: 224 Total reward: 23.0 Training loss: 5.3178 Explore P: 0.6355\n",
            "Episode: 225 Total reward: 58.0 Training loss: 0.5920 Explore P: 0.6319\n",
            "Episode: 226 Total reward: 53.0 Training loss: 1.4339 Explore P: 0.6286\n",
            "Episode: 227 Total reward: 13.0 Training loss: 7.2642 Explore P: 0.6278\n",
            "Episode: 228 Total reward: 22.0 Training loss: 4.5221 Explore P: 0.6264\n",
            "Episode: 229 Total reward: 56.0 Training loss: 1.1649 Explore P: 0.6230\n",
            "Episode: 230 Total reward: 70.0 Training loss: 2.1494 Explore P: 0.6187\n",
            "Episode: 231 Total reward: 32.0 Training loss: 0.3021 Explore P: 0.6167\n",
            "Episode: 232 Total reward: 85.0 Training loss: 2.3080 Explore P: 0.6116\n",
            "Episode: 233 Total reward: 18.0 Training loss: 6.4625 Explore P: 0.6105\n",
            "Episode: 234 Total reward: 67.0 Training loss: 0.2543 Explore P: 0.6065\n",
            "Episode: 235 Total reward: 36.0 Training loss: 0.3172 Explore P: 0.6044\n",
            "Episode: 236 Total reward: 20.0 Training loss: 10.3709 Explore P: 0.6032\n",
            "Episode: 237 Total reward: 27.0 Training loss: 2.8022 Explore P: 0.6016\n",
            "Episode: 238 Total reward: 36.0 Training loss: 0.8379 Explore P: 0.5995\n",
            "Episode: 239 Total reward: 152.0 Training loss: 1.9294 Explore P: 0.5906\n",
            "Episode: 240 Total reward: 65.0 Training loss: 0.5408 Explore P: 0.5868\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 241 Total reward: 52.0 Training loss: 1.0372 Explore P: 0.5838\n",
            "Episode: 242 Total reward: 54.0 Training loss: 3.2588 Explore P: 0.5807\n",
            "Episode: 243 Total reward: 93.0 Training loss: 2.2569 Explore P: 0.5754\n",
            "Episode: 244 Total reward: 39.0 Training loss: 0.1364 Explore P: 0.5732\n",
            "Episode: 245 Total reward: 25.0 Training loss: 9.9078 Explore P: 0.5718\n",
            "Episode: 246 Total reward: 114.0 Training loss: 2.6757 Explore P: 0.5655\n",
            "Episode: 247 Total reward: 30.0 Training loss: 0.3443 Explore P: 0.5638\n",
            "Episode: 248 Total reward: 24.0 Training loss: 3.6223 Explore P: 0.5625\n",
            "Episode: 249 Total reward: 199.0 Training loss: 0.4041 Explore P: 0.5516\n",
            "Episode: 250 Total reward: 102.0 Training loss: 2.2741 Explore P: 0.5461\n",
            "Episode: 251 Total reward: 43.0 Training loss: 4.8664 Explore P: 0.5438\n",
            "Episode: 252 Total reward: 87.0 Training loss: 9.1476 Explore P: 0.5392\n",
            "Episode: 253 Total reward: 57.0 Training loss: 7.3045 Explore P: 0.5362\n",
            "Episode: 254 Total reward: 75.0 Training loss: 0.3436 Explore P: 0.5322\n",
            "Episode: 255 Total reward: 99.0 Training loss: 1.2284 Explore P: 0.5271\n",
            "Episode: 256 Total reward: 35.0 Training loss: 0.3833 Explore P: 0.5253\n",
            "Episode: 257 Total reward: 115.0 Training loss: 0.2726 Explore P: 0.5194\n",
            "Episode: 258 Total reward: 152.0 Training loss: 4.3342 Explore P: 0.5117\n",
            "Episode: 259 Total reward: 88.0 Training loss: 0.3882 Explore P: 0.5073\n",
            "Episode: 260 Total reward: 199.0 Training loss: 3.8039 Explore P: 0.4975\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 261 Total reward: 41.0 Training loss: 0.2770 Explore P: 0.4955\n",
            "Episode: 262 Total reward: 124.0 Training loss: 0.3536 Explore P: 0.4895\n",
            "Episode: 263 Total reward: 117.0 Training loss: 1.6598 Explore P: 0.4840\n",
            "Episode: 264 Total reward: 195.0 Training loss: 0.2917 Explore P: 0.4748\n",
            "Episode: 265 Total reward: 70.0 Training loss: 7.5505 Explore P: 0.4716\n",
            "Episode: 266 Total reward: 156.0 Training loss: 0.5907 Explore P: 0.4644\n",
            "Episode: 267 Total reward: 123.0 Training loss: 20.6189 Explore P: 0.4589\n",
            "Episode: 268 Total reward: 137.0 Training loss: 0.3323 Explore P: 0.4527\n",
            "Episode: 269 Total reward: 114.0 Training loss: 6.9316 Explore P: 0.4477\n",
            "Episode: 270 Total reward: 118.0 Training loss: 0.1970 Explore P: 0.4426\n",
            "Episode: 271 Total reward: 111.0 Training loss: 10.6242 Explore P: 0.4378\n",
            "Episode: 272 Total reward: 76.0 Training loss: 0.2620 Explore P: 0.4346\n",
            "Episode: 273 Total reward: 113.0 Training loss: 1.3542 Explore P: 0.4298\n",
            "Episode: 274 Total reward: 97.0 Training loss: 0.3365 Explore P: 0.4258\n",
            "Episode: 275 Total reward: 167.0 Training loss: 2.5289 Explore P: 0.4189\n",
            "Episode: 276 Total reward: 157.0 Training loss: 0.3398 Explore P: 0.4125\n",
            "Episode: 277 Total reward: 110.0 Training loss: 1.1169 Explore P: 0.4081\n",
            "Episode: 278 Total reward: 41.0 Training loss: 4.2716 Explore P: 0.4065\n",
            "Episode: 279 Total reward: 125.0 Training loss: 0.2771 Explore P: 0.4015\n",
            "Episode: 280 Total reward: 119.0 Training loss: 0.2334 Explore P: 0.3969\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 281 Total reward: 124.0 Training loss: 2.7688 Explore P: 0.3921\n",
            "Episode: 282 Total reward: 75.0 Training loss: 10.0331 Explore P: 0.3893\n",
            "Episode: 283 Total reward: 105.0 Training loss: 62.7206 Explore P: 0.3853\n",
            "Episode: 284 Total reward: 156.0 Training loss: 0.8720 Explore P: 0.3795\n",
            "Episode: 285 Total reward: 152.0 Training loss: 0.1878 Explore P: 0.3739\n",
            "Episode: 286 Total reward: 93.0 Training loss: 0.2287 Explore P: 0.3706\n",
            "Episode: 287 Total reward: 134.0 Training loss: 1.9920 Explore P: 0.3658\n",
            "Episode: 288 Total reward: 185.0 Training loss: 0.1768 Explore P: 0.3593\n",
            "Episode: 289 Total reward: 98.0 Training loss: 2.6163 Explore P: 0.3558\n",
            "Episode: 290 Total reward: 114.0 Training loss: 0.1446 Explore P: 0.3519\n",
            "Episode: 291 Total reward: 88.0 Training loss: 5.6970 Explore P: 0.3489\n",
            "Episode: 292 Total reward: 56.0 Training loss: 7.5906 Explore P: 0.3470\n",
            "Episode: 293 Total reward: 114.0 Training loss: 0.3808 Explore P: 0.3432\n",
            "Episode: 294 Total reward: 199.0 Training loss: 1.1614 Explore P: 0.3367\n",
            "Episode: 295 Total reward: 115.0 Training loss: 10.2594 Explore P: 0.3329\n",
            "Episode: 296 Total reward: 48.0 Training loss: 0.2281 Explore P: 0.3314\n",
            "Episode: 297 Total reward: 117.0 Training loss: 0.4009 Explore P: 0.3276\n",
            "Episode: 298 Total reward: 147.0 Training loss: 0.2588 Explore P: 0.3230\n",
            "Episode: 299 Total reward: 121.0 Training loss: 5.5791 Explore P: 0.3192\n",
            "Episode: 300 Total reward: 159.0 Training loss: 0.4138 Explore P: 0.3144\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 301 Total reward: 199.0 Training loss: 0.2050 Explore P: 0.3084\n",
            "Episode: 302 Total reward: 117.0 Training loss: 0.2332 Explore P: 0.3049\n",
            "Episode: 303 Total reward: 199.0 Training loss: 0.1927 Explore P: 0.2991\n",
            "Episode: 304 Total reward: 166.0 Training loss: 0.2169 Explore P: 0.2943\n",
            "Episode: 305 Total reward: 169.0 Training loss: 22.6144 Explore P: 0.2896\n",
            "Episode: 306 Total reward: 199.0 Training loss: 0.2395 Explore P: 0.2840\n",
            "Episode: 307 Total reward: 107.0 Training loss: 7.7668 Explore P: 0.2811\n",
            "Episode: 308 Total reward: 199.0 Training loss: 28.2745 Explore P: 0.2758\n",
            "Episode: 309 Total reward: 199.0 Training loss: 4.4059 Explore P: 0.2706\n",
            "Episode: 310 Total reward: 152.0 Training loss: 0.1930 Explore P: 0.2666\n",
            "Episode: 311 Total reward: 165.0 Training loss: 31.5958 Explore P: 0.2624\n",
            "Episode: 312 Total reward: 199.0 Training loss: 0.2907 Explore P: 0.2574\n",
            "Episode: 313 Total reward: 162.0 Training loss: 5.9811 Explore P: 0.2535\n",
            "Episode: 314 Total reward: 189.0 Training loss: 0.4307 Explore P: 0.2489\n",
            "Episode: 315 Total reward: 176.0 Training loss: 0.3271 Explore P: 0.2447\n",
            "Episode: 316 Total reward: 199.0 Training loss: 0.2401 Explore P: 0.2401\n",
            "Episode: 317 Total reward: 189.0 Training loss: 0.3920 Explore P: 0.2358\n",
            "Episode: 318 Total reward: 197.0 Training loss: 0.3971 Explore P: 0.2314\n",
            "Episode: 319 Total reward: 199.0 Training loss: 0.0778 Explore P: 0.2270\n",
            "Episode: 320 Total reward: 136.0 Training loss: 0.0723 Explore P: 0.2241\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 321 Total reward: 199.0 Training loss: 0.2902 Explore P: 0.2199\n",
            "Episode: 322 Total reward: 199.0 Training loss: 6.7694 Explore P: 0.2158\n",
            "Episode: 323 Total reward: 187.0 Training loss: 0.2993 Explore P: 0.2119\n",
            "Episode: 324 Total reward: 199.0 Training loss: 0.0917 Explore P: 0.2080\n",
            "Episode: 325 Total reward: 183.0 Training loss: 0.2044 Explore P: 0.2044\n",
            "Episode: 326 Total reward: 180.0 Training loss: 0.2233 Explore P: 0.2009\n",
            "Episode: 327 Total reward: 199.0 Training loss: 0.2110 Explore P: 0.1971\n",
            "Episode: 328 Total reward: 199.0 Training loss: 0.0956 Explore P: 0.1935\n",
            "Episode: 329 Total reward: 199.0 Training loss: 0.0388 Explore P: 0.1898\n",
            "Episode: 330 Total reward: 199.0 Training loss: 0.5502 Explore P: 0.1863\n",
            "Episode: 331 Total reward: 199.0 Training loss: 0.0360 Explore P: 0.1828\n",
            "Episode: 332 Total reward: 199.0 Training loss: 0.1184 Explore P: 0.1794\n",
            "Episode: 333 Total reward: 199.0 Training loss: 0.1305 Explore P: 0.1761\n",
            "Episode: 334 Total reward: 199.0 Training loss: 0.0494 Explore P: 0.1728\n",
            "Episode: 335 Total reward: 199.0 Training loss: 0.1909 Explore P: 0.1696\n",
            "Episode: 336 Total reward: 199.0 Training loss: 0.0937 Explore P: 0.1665\n",
            "Episode: 337 Total reward: 199.0 Training loss: 0.0703 Explore P: 0.1634\n",
            "Episode: 338 Total reward: 199.0 Training loss: 0.0396 Explore P: 0.1604\n",
            "Episode: 339 Total reward: 199.0 Training loss: 0.2331 Explore P: 0.1574\n",
            "Episode: 340 Total reward: 199.0 Training loss: 0.0798 Explore P: 0.1545\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 341 Total reward: 199.0 Training loss: 0.2570 Explore P: 0.1516\n",
            "Episode: 342 Total reward: 199.0 Training loss: 0.0799 Explore P: 0.1489\n",
            "Episode: 343 Total reward: 199.0 Training loss: 0.0332 Explore P: 0.1461\n",
            "Episode: 344 Total reward: 199.0 Training loss: 0.0420 Explore P: 0.1434\n",
            "Episode: 345 Total reward: 199.0 Training loss: 0.0499 Explore P: 0.1408\n",
            "Episode: 346 Total reward: 199.0 Training loss: 0.0456 Explore P: 0.1382\n",
            "Episode: 347 Total reward: 199.0 Training loss: 0.0513 Explore P: 0.1357\n",
            "Episode: 348 Total reward: 199.0 Training loss: 0.1010 Explore P: 0.1332\n",
            "Episode: 349 Total reward: 199.0 Training loss: 0.1035 Explore P: 0.1308\n",
            "Episode: 350 Total reward: 199.0 Training loss: 0.0562 Explore P: 0.1284\n",
            "Episode: 351 Total reward: 199.0 Training loss: 0.0864 Explore P: 0.1261\n",
            "Episode: 352 Total reward: 199.0 Training loss: 0.0602 Explore P: 0.1238\n",
            "Episode: 353 Total reward: 199.0 Training loss: 0.0616 Explore P: 0.1216\n",
            "Episode: 354 Total reward: 199.0 Training loss: 0.0263 Explore P: 0.1194\n",
            "Episode: 355 Total reward: 199.0 Training loss: 0.0537 Explore P: 0.1172\n",
            "Episode: 356 Total reward: 199.0 Training loss: 0.1040 Explore P: 0.1151\n",
            "Episode: 357 Total reward: 199.0 Training loss: 0.0619 Explore P: 0.1130\n",
            "Episode: 358 Total reward: 199.0 Training loss: 0.0296 Explore P: 0.1110\n",
            "Episode: 359 Total reward: 199.0 Training loss: 0.0682 Explore P: 0.1090\n",
            "Episode: 360 Total reward: 199.0 Training loss: 0.0239 Explore P: 0.1070\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 361 Total reward: 199.0 Training loss: 0.0875 Explore P: 0.1051\n",
            "Episode: 362 Total reward: 199.0 Training loss: 0.0225 Explore P: 0.1033\n",
            "Episode: 363 Total reward: 199.0 Training loss: 0.0332 Explore P: 0.1014\n",
            "Episode: 364 Total reward: 199.0 Training loss: 0.0207 Explore P: 0.0996\n",
            "Episode: 365 Total reward: 199.0 Training loss: 0.0312 Explore P: 0.0979\n",
            "Episode: 366 Total reward: 199.0 Training loss: 0.0382 Explore P: 0.0961\n",
            "Episode: 367 Total reward: 199.0 Training loss: 0.0454 Explore P: 0.0944\n",
            "Episode: 368 Total reward: 199.0 Training loss: 0.0315 Explore P: 0.0928\n",
            "Episode: 369 Total reward: 199.0 Training loss: 0.1287 Explore P: 0.0911\n",
            "Episode: 370 Total reward: 199.0 Training loss: 0.0316 Explore P: 0.0895\n",
            "Episode: 371 Total reward: 199.0 Training loss: 0.0465 Explore P: 0.0880\n",
            "Episode: 372 Total reward: 199.0 Training loss: 0.0182 Explore P: 0.0864\n",
            "Episode: 373 Total reward: 199.0 Training loss: 0.0292 Explore P: 0.0849\n",
            "Episode: 374 Total reward: 199.0 Training loss: 0.0463 Explore P: 0.0834\n",
            "Episode: 375 Total reward: 199.0 Training loss: 0.0214 Explore P: 0.0820\n",
            "Episode: 376 Total reward: 199.0 Training loss: 0.0381 Explore P: 0.0806\n",
            "Episode: 377 Total reward: 199.0 Training loss: 0.0263 Explore P: 0.0792\n",
            "Episode: 378 Total reward: 199.0 Training loss: 0.0290 Explore P: 0.0778\n",
            "Episode: 379 Total reward: 199.0 Training loss: 0.0516 Explore P: 0.0765\n",
            "Episode: 380 Total reward: 199.0 Training loss: 0.0348 Explore P: 0.0752\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 381 Total reward: 199.0 Training loss: 0.0271 Explore P: 0.0739\n",
            "Episode: 382 Total reward: 199.0 Training loss: 0.0438 Explore P: 0.0726\n",
            "Episode: 383 Total reward: 199.0 Training loss: 0.0290 Explore P: 0.0714\n",
            "Episode: 384 Total reward: 199.0 Training loss: 0.0313 Explore P: 0.0702\n",
            "Episode: 385 Total reward: 199.0 Training loss: 0.0249 Explore P: 0.0690\n",
            "Episode: 386 Total reward: 199.0 Training loss: 0.0228 Explore P: 0.0678\n",
            "Episode: 387 Total reward: 199.0 Training loss: 0.0215 Explore P: 0.0667\n",
            "Episode: 388 Total reward: 199.0 Training loss: 0.0321 Explore P: 0.0656\n",
            "Episode: 389 Total reward: 199.0 Training loss: 0.0418 Explore P: 0.0645\n",
            "Episode: 390 Total reward: 199.0 Training loss: 0.0367 Explore P: 0.0634\n",
            "Episode: 391 Total reward: 199.0 Training loss: 0.0219 Explore P: 0.0624\n",
            "Episode: 392 Total reward: 199.0 Training loss: 0.0380 Explore P: 0.0613\n",
            "Episode: 393 Total reward: 199.0 Training loss: 0.0241 Explore P: 0.0603\n",
            "Episode: 394 Total reward: 199.0 Training loss: 0.0191 Explore P: 0.0593\n",
            "Episode: 395 Total reward: 199.0 Training loss: 0.0252 Explore P: 0.0584\n",
            "Episode: 396 Total reward: 199.0 Training loss: 0.0420 Explore P: 0.0574\n",
            "Episode: 397 Total reward: 199.0 Training loss: 0.0113 Explore P: 0.0565\n",
            "Episode: 398 Total reward: 199.0 Training loss: 0.0192 Explore P: 0.0556\n",
            "Episode: 399 Total reward: 199.0 Training loss: 0.0122 Explore P: 0.0547\n",
            "Episode: 400 Total reward: 199.0 Training loss: 0.0251 Explore P: 0.0538\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 401 Total reward: 199.0 Training loss: 0.0314 Explore P: 0.0529\n",
            "Episode: 402 Total reward: 199.0 Training loss: 0.0178 Explore P: 0.0521\n",
            "Episode: 403 Total reward: 199.0 Training loss: 0.0294 Explore P: 0.0512\n",
            "Episode: 404 Total reward: 199.0 Training loss: 0.0165 Explore P: 0.0504\n",
            "Episode: 405 Total reward: 199.0 Training loss: 0.0451 Explore P: 0.0496\n",
            "Episode: 406 Total reward: 199.0 Training loss: 0.0251 Explore P: 0.0489\n",
            "Episode: 407 Total reward: 199.0 Training loss: 0.0181 Explore P: 0.0481\n",
            "Episode: 408 Total reward: 199.0 Training loss: 0.0521 Explore P: 0.0473\n",
            "Episode: 409 Total reward: 199.0 Training loss: 0.0217 Explore P: 0.0466\n",
            "Episode: 410 Total reward: 199.0 Training loss: 0.0180 Explore P: 0.0459\n",
            "Episode: 411 Total reward: 199.0 Training loss: 0.0076 Explore P: 0.0452\n",
            "Episode: 412 Total reward: 199.0 Training loss: 0.0313 Explore P: 0.0445\n",
            "Episode: 413 Total reward: 199.0 Training loss: 0.0314 Explore P: 0.0438\n",
            "Episode: 414 Total reward: 199.0 Training loss: 0.0076 Explore P: 0.0431\n",
            "Episode: 415 Total reward: 199.0 Training loss: 0.0092 Explore P: 0.0425\n",
            "Episode: 416 Total reward: 199.0 Training loss: 0.0323 Explore P: 0.0418\n",
            "Episode: 417 Total reward: 199.0 Training loss: 0.0432 Explore P: 0.0412\n",
            "Episode: 418 Total reward: 199.0 Training loss: 0.0220 Explore P: 0.0406\n",
            "Episode: 419 Total reward: 199.0 Training loss: 0.0123 Explore P: 0.0400\n",
            "Episode: 420 Total reward: 199.0 Training loss: 0.0396 Explore P: 0.0394\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 421 Total reward: 199.0 Training loss: 0.0271 Explore P: 0.0388\n",
            "Episode: 422 Total reward: 199.0 Training loss: 0.0271 Explore P: 0.0383\n",
            "Episode: 423 Total reward: 199.0 Training loss: 0.0301 Explore P: 0.0377\n",
            "Episode: 424 Total reward: 199.0 Training loss: 0.0286 Explore P: 0.0372\n",
            "Episode: 425 Total reward: 199.0 Training loss: 0.0565 Explore P: 0.0366\n",
            "Episode: 426 Total reward: 199.0 Training loss: 0.0459 Explore P: 0.0361\n",
            "Episode: 427 Total reward: 199.0 Training loss: 0.0433 Explore P: 0.0356\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}