{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jax_double_dqn_cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNB4yy0v2T3PBMZX2Bo21mu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anirbanl/jax-code/blob/master/rlflax/jax_double_dqn_cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6rIyAnYHQvM"
      },
      "source": [
        "Based on sources:\n",
        "1. https://arxiv.org/pdf/1509.06461.pdf\n",
        "2. https://github.com/higgsfield/RL-Adventure/blob/master/2.double%20dqn.ipynb\n",
        "3. https://medium.com/@parsa_h_m/deep-reinforcement-learning-dqn-double-dqn-dueling-dqn-noisy-dqn-and-dqn-with-prioritized-551f621a9823"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_jS9qg6EG4l",
        "outputId": "ac238cd1-c931-4a5e-8e67-cc7cc31d1aa6"
      },
      "source": [
        "!pip install jax jaxlib flax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (0.2.13)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (0.1.66+cuda110)\n",
            "Collecting flax\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/21/21ca1f4831ac24646578d2545c4db9a8369b9da4a4b7dcf067feee312b45/flax-0.3.4-py3-none-any.whl (183kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from jax) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax) (0.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.4.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.12)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax) (1.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax) (3.2.2)\n",
            "Collecting optax\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/04/464fa1d12562d191196f2f7f8112d65e22eaaa9a7e2b599f298aeba2ce27/optax-0.0.8-py3-none-any.whl (113kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 26.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.4.7)\n",
            "Collecting chex>=0.0.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/b9/445eb59ec23249acffc5322c79b07e20b12dbff45b9c1da6cdae9e947685/chex-0.0.7-py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.11.1)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.1.6)\n",
            "Installing collected packages: chex, optax, flax\n",
            "Successfully installed chex-0.0.7 flax-0.3.4 optax-0.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt1r4--nEPJm"
      },
      "source": [
        "import gym\n",
        "gym.logger.set_level(40) # suppress warnings (please remove if gives error)\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import jax\n",
        "import jax.numpy as jp\n",
        "from jax.ops import index, index_add, index_update\n",
        "from jax import jit, grad, vmap, random, jacrev, jacobian, jacfwd, value_and_grad\n",
        "from functools import partial\n",
        "from jax.tree_util import tree_multimap  # Element-wise manipulation of collections of numpy arrays\n",
        "from flax import linen as nn           # The Linen API\n",
        "from flax.training import train_state  # Useful dataclass to keep train state\n",
        "import optax                           # Optimizers\n",
        "from typing import Sequence\n",
        "import copy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK9XAT81EWW0"
      },
      "source": [
        "from collections import deque\n",
        "class Memory():\n",
        "    def __init__(self, max_size = 1000):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "    \n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "            \n",
        "    def sample(self, key, batch_size):\n",
        "        key, _ = jax.random.split(key)\n",
        "        idx = jax.random.choice(key,\n",
        "                               jp.arange(len(self.buffer)), \n",
        "                               shape=(batch_size, ))\n",
        "        return [self.buffer[ii] for ii in idx]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtUX4AXiFWVK"
      },
      "source": [
        "train_episodes = 1000          # max number of episodes to learn from\n",
        "max_steps = 200                # max steps in an episode\n",
        "gamma = 0.99                   # future reward discount\n",
        "update_target_every = 20       # Update target Q model every this episodes\n",
        "\n",
        "# Exploration parameters\n",
        "explore_start = 1.0            # exploration probability at start\n",
        "explore_stop = 0.01            # minimum exploration probability \n",
        "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
        "\n",
        "# Network parameters\n",
        "hidden_size = 64               # number of units in each Q-network hidden layer\n",
        "learning_rate = 1e-3         # Q-network learning rate\n",
        "\n",
        "# Memory parameters\n",
        "memory_size = 10000            # memory capacity\n",
        "batch_size = 20                # experience mini-batch size\n",
        "pretrain_length = batch_size   # number experiences to pretrain the memory"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSJfnThHFouD"
      },
      "source": [
        "#Define Q-network\n",
        "class QNetwork:\n",
        "    def __init__(self, rng, env, learning_rate=0.01, state_size=4, \n",
        "                 action_size=2, hidden_size=10, \n",
        "                 name='QNetwork'):\n",
        "        self.key = rng\n",
        "        self.env = env\n",
        "\n",
        "        class Model(nn.Module):\n",
        "            features: Sequence[int]\n",
        "\n",
        "            @nn.compact\n",
        "            def __call__(self, x):\n",
        "                x = nn.relu(nn.Dense(self.features[0])(x))\n",
        "                x = nn.relu(nn.Dense(self.features[1])(x))\n",
        "                x = nn.Dense(self.features[2])(x)\n",
        "                return x\n",
        "\n",
        "        def create_train_state(rng, learning_rate, s_size, h_size, a_size):\n",
        "            \"\"\"Creates initial `TrainState`.\"\"\"\n",
        "            model = Model(features=[hidden_size, hidden_size, a_size])\n",
        "            params = model.init(rng, jp.ones((s_size, )))#['params']\n",
        "            tx = optax.adam(learning_rate)\n",
        "            return train_state.TrainState.create(\n",
        "                apply_fn=model.apply, params=params, tx=tx)\n",
        "\n",
        "        self.ts = create_train_state(rng, learning_rate, state_size, hidden_size, action_size)\n",
        "\n",
        "        @jit\n",
        "        def train_step(ts, inputs, actions, targets):\n",
        "\n",
        "            def loss_fun(params, inputs, actions, targets):\n",
        "                output = ts.apply_fn(params, inputs)\n",
        "                selectedq = jp.sum(actions*output, axis=-1)\n",
        "                diff = selectedq - jax.lax.stop_gradient(targets)\n",
        "                return jp.mean(diff**2)\n",
        "\n",
        "            loss, g = value_and_grad(loss_fun)(ts.params, inputs, actions, targets)\n",
        "            return ts.apply_gradients(grads=g), loss\n",
        "\n",
        "        self.train_fn = train_step\n",
        "\n",
        "\n",
        "    def act(self, state, explore_p):\n",
        "        self.key, _ = jax.random.split(self.key)\n",
        "        uf = jax.random.uniform(self.key, (1,), minval=0.0, maxval=1.0)[0]\n",
        "        if explore_p > uf:\n",
        "            # Make a random action\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            # Get action from Q-network\n",
        "            qvalues = self.ts.apply_fn(self.ts.params, state)\n",
        "            action = jp.argmax(qvalues).item()\n",
        "        return action\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0fPvqRcLdf4"
      },
      "source": [
        "def init_memory(env):\n",
        "    # Initialize the simulation\n",
        "    env.reset()\n",
        "    # Take one random step to get the pole and cart moving\n",
        "    state, reward, done, _ = env.step(env.action_space.sample())\n",
        "\n",
        "    memory = Memory(max_size=memory_size)\n",
        "\n",
        "    # Make a bunch of random actions and store the experiences\n",
        "    for ii in range(pretrain_length):\n",
        "        # Uncomment the line below to watch the simulation\n",
        "        # env.render()\n",
        "\n",
        "        # Make a random action\n",
        "        action = env.action_space.sample()\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        if done:\n",
        "            # The simulation fails so no next state\n",
        "            next_state = jp.zeros(state.shape)\n",
        "            # Add experience to memory\n",
        "            memory.add((state, action, reward, next_state))\n",
        "            \n",
        "            # Start new episode\n",
        "            env.reset()\n",
        "            # Take one random step to get the pole and cart moving\n",
        "            state, reward, done, _ = env.step(env.action_space.sample())\n",
        "        else:\n",
        "            # Add experience to memory\n",
        "            memory.add((state, action, reward, next_state))\n",
        "            state = next_state\n",
        "    return memory, state"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vpr3LNUMkaG"
      },
      "source": [
        "# Now train with experiences\n",
        "def one_hot(x, k, dtype=jp.float32):\n",
        "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
        "  return jp.array(x[:, None] == jp.arange(k), dtype)\n",
        "\n",
        "\n",
        "def train(rng, env, mainQN):\n",
        "    rewards_list = []    \n",
        "    step = 0\n",
        "    memory, state = init_memory(env)\n",
        "    current_params = mainQN.ts.params\n",
        "    target_params = copy.deepcopy(mainQN.ts.params)\n",
        "    for ep in range(1, train_episodes):\n",
        "        total_reward = 0\n",
        "        t = 0\n",
        "        while t < max_steps:\n",
        "            step += 1\n",
        "            # Uncomment this next line to watch the training\n",
        "            # env.render() \n",
        "            \n",
        "            # Explore or Exploit\n",
        "            explore_p = explore_stop + (explore_start - explore_stop)*jp.exp(-decay_rate*step) \n",
        "            action = mainQN.act(state, explore_p)\n",
        "            \n",
        "            # Take action, get new state and reward\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            total_reward += reward\n",
        "            \n",
        "            if done:\n",
        "                # the episode ends so no next state\n",
        "                next_state = jp.zeros(state.shape)\n",
        "                t = max_steps\n",
        "                \n",
        "                print('Episode: {}'.format(ep),\n",
        "                    'Total reward: {}'.format(total_reward),\n",
        "                    'Training loss: {:.4f}'.format(loss),\n",
        "                    'Explore P: {:.4f}'.format(explore_p))\n",
        "                rewards_list.append((ep, total_reward))\n",
        "                \n",
        "                # Add experience to memory\n",
        "                memory.add((state, action, reward, next_state))\n",
        "                \n",
        "                # Start new episode\n",
        "                env.reset()\n",
        "                # Take one random step to get the pole and cart moving\n",
        "                state, reward, done, _ = env.step(env.action_space.sample())\n",
        "\n",
        "            else:\n",
        "                # Add experience to memory\n",
        "                memory.add((state, action, reward, next_state))\n",
        "                state = next_state\n",
        "                t += 1\n",
        "            \n",
        "            # Sample mini-batch from memory\n",
        "            batch = memory.sample(rng, batch_size)\n",
        "            states = jp.array([each[0] for each in batch])\n",
        "            actions = one_hot(jp.array([each[1] for each in batch]), 2)\n",
        "            rewards = jp.array([each[2] for each in batch])\n",
        "            next_states = jp.array([each[3] for each in batch])\n",
        "            \n",
        "            # Train network\n",
        "            current_Qs = mainQN.ts.apply_fn(current_params, next_states)\n",
        "            target_Qs = mainQN.ts.apply_fn(target_params, next_states)\n",
        "            \n",
        "            # Set target_Qs to 0 for states where episode ends\n",
        "            episode_ends = (next_states == jp.zeros(states[0].shape)).all(axis=1)\n",
        "            new_target_Qs = index_update(target_Qs, index[episode_ends], (0, 0))\n",
        "            target_Qs = new_target_Qs\n",
        "            \n",
        "            max_current_Qs_indices = jp.argmax(current_Qs, axis=-1)\n",
        "            targets = rewards + gamma * jp.take_along_axis(target_Qs, max_current_Qs_indices[..., None], axis=-1)\n",
        "            # print(states.shape, targets.shape, targets)\n",
        "            mainQN.ts, loss = mainQN.train_fn(mainQN.ts, states, actions, targets)\n",
        "            current_params = mainQN.ts.params\n",
        "\n",
        "        if ep % update_target_every == 0:\n",
        "            target_params = copy.deepcopy(mainQN.ts.params)\n",
        "            print('***** Updated target QNetwork *****')\n",
        "\n",
        "    return rewards_list\n",
        "\n",
        "def plot_scores(rewards_list):\n",
        "    def running_mean(x, N):\n",
        "        cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
        "        return (cumsum[N:] - cumsum[:-N]) / N\n",
        "    eps, rews = np.array(rewards_list).T\n",
        "    smoothed_rews = running_mean(rews, 10)\n",
        "    plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
        "    plt.plot(eps, rews, color='grey', alpha=0.3)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.show()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PeurKJ6oNcZF",
        "outputId": "8cf77df9-d413-405d-8afe-ff8547a3f88c"
      },
      "source": [
        "def main():\n",
        "    seed = 0\n",
        "    env = gym.make('CartPole-v0')\n",
        "    env.seed(seed)\n",
        "    env.action_space.seed(seed)\n",
        "    print('observation space:', env.observation_space)\n",
        "    print('action space:', env.action_space.n)\n",
        "    rng = jax.random.PRNGKey(seed)\n",
        "    mainQN = QNetwork(rng, env, name='main', hidden_size=hidden_size, learning_rate=learning_rate)\n",
        "    rewards_list = train(rng, env, mainQN)\n",
        "    plot_scores(rewards_list)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
            "action space: 2\n",
            "Episode: 1 Total reward: 31.0 Training loss: 0.8628 Explore P: 0.9969\n",
            "Episode: 2 Total reward: 54.0 Training loss: 0.2319 Explore P: 0.9916\n",
            "Episode: 3 Total reward: 28.0 Training loss: 0.3584 Explore P: 0.9889\n",
            "Episode: 4 Total reward: 21.0 Training loss: 0.1620 Explore P: 0.9868\n",
            "Episode: 5 Total reward: 8.0 Training loss: 0.2605 Explore P: 0.9860\n",
            "Episode: 6 Total reward: 13.0 Training loss: 0.4306 Explore P: 0.9848\n",
            "Episode: 7 Total reward: 20.0 Training loss: 0.2264 Explore P: 0.9828\n",
            "Episode: 8 Total reward: 35.0 Training loss: 0.1583 Explore P: 0.9794\n",
            "Episode: 9 Total reward: 27.0 Training loss: 0.1401 Explore P: 0.9768\n",
            "Episode: 10 Total reward: 22.0 Training loss: 0.1885 Explore P: 0.9747\n",
            "Episode: 11 Total reward: 15.0 Training loss: 0.2600 Explore P: 0.9732\n",
            "Episode: 12 Total reward: 28.0 Training loss: 0.1787 Explore P: 0.9705\n",
            "Episode: 13 Total reward: 10.0 Training loss: 0.2290 Explore P: 0.9696\n",
            "Episode: 14 Total reward: 16.0 Training loss: 0.1715 Explore P: 0.9681\n",
            "Episode: 15 Total reward: 13.0 Training loss: 0.1211 Explore P: 0.9668\n",
            "Episode: 16 Total reward: 15.0 Training loss: 0.2673 Explore P: 0.9654\n",
            "Episode: 17 Total reward: 16.0 Training loss: 0.2285 Explore P: 0.9638\n",
            "Episode: 18 Total reward: 23.0 Training loss: 0.1450 Explore P: 0.9617\n",
            "Episode: 19 Total reward: 27.0 Training loss: 0.2295 Explore P: 0.9591\n",
            "Episode: 20 Total reward: 17.0 Training loss: 0.1388 Explore P: 0.9575\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 21 Total reward: 9.0 Training loss: 0.1267 Explore P: 0.9566\n",
            "Episode: 22 Total reward: 46.0 Training loss: 0.0115 Explore P: 0.9523\n",
            "Episode: 23 Total reward: 14.0 Training loss: 0.0106 Explore P: 0.9510\n",
            "Episode: 24 Total reward: 13.0 Training loss: 0.1689 Explore P: 0.9497\n",
            "Episode: 25 Total reward: 16.0 Training loss: 0.0074 Explore P: 0.9482\n",
            "Episode: 26 Total reward: 18.0 Training loss: 0.0860 Explore P: 0.9466\n",
            "Episode: 27 Total reward: 19.0 Training loss: 0.0949 Explore P: 0.9448\n",
            "Episode: 28 Total reward: 25.0 Training loss: 0.0895 Explore P: 0.9424\n",
            "Episode: 29 Total reward: 61.0 Training loss: 0.0863 Explore P: 0.9368\n",
            "Episode: 30 Total reward: 11.0 Training loss: 0.0097 Explore P: 0.9358\n",
            "Episode: 31 Total reward: 13.0 Training loss: 0.0915 Explore P: 0.9345\n",
            "Episode: 32 Total reward: 14.0 Training loss: 0.0986 Explore P: 0.9333\n",
            "Episode: 33 Total reward: 12.0 Training loss: 0.0864 Explore P: 0.9321\n",
            "Episode: 34 Total reward: 9.0 Training loss: 0.0914 Explore P: 0.9313\n",
            "Episode: 35 Total reward: 22.0 Training loss: 0.0071 Explore P: 0.9293\n",
            "Episode: 36 Total reward: 9.0 Training loss: 0.0220 Explore P: 0.9285\n",
            "Episode: 37 Total reward: 48.0 Training loss: 0.0094 Explore P: 0.9241\n",
            "Episode: 38 Total reward: 18.0 Training loss: 0.0060 Explore P: 0.9224\n",
            "Episode: 39 Total reward: 9.0 Training loss: 0.0116 Explore P: 0.9216\n",
            "Episode: 40 Total reward: 14.0 Training loss: 0.0029 Explore P: 0.9203\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 41 Total reward: 18.0 Training loss: 0.5058 Explore P: 0.9187\n",
            "Episode: 42 Total reward: 21.0 Training loss: 0.2663 Explore P: 0.9168\n",
            "Episode: 43 Total reward: 15.0 Training loss: 0.0030 Explore P: 0.9154\n",
            "Episode: 44 Total reward: 12.0 Training loss: 0.2785 Explore P: 0.9143\n",
            "Episode: 45 Total reward: 10.0 Training loss: 0.0030 Explore P: 0.9134\n",
            "Episode: 46 Total reward: 18.0 Training loss: 0.7350 Explore P: 0.9118\n",
            "Episode: 47 Total reward: 26.0 Training loss: 0.2620 Explore P: 0.9095\n",
            "Episode: 48 Total reward: 41.0 Training loss: 0.0248 Explore P: 0.9058\n",
            "Episode: 49 Total reward: 44.0 Training loss: 0.4892 Explore P: 0.9019\n",
            "Episode: 50 Total reward: 24.0 Training loss: 0.0628 Explore P: 0.8997\n",
            "Episode: 51 Total reward: 10.0 Training loss: 0.0056 Explore P: 0.8988\n",
            "Episode: 52 Total reward: 37.0 Training loss: 0.2558 Explore P: 0.8955\n",
            "Episode: 53 Total reward: 19.0 Training loss: 0.0076 Explore P: 0.8939\n",
            "Episode: 54 Total reward: 8.0 Training loss: 0.2537 Explore P: 0.8932\n",
            "Episode: 55 Total reward: 15.0 Training loss: 0.2554 Explore P: 0.8918\n",
            "Episode: 56 Total reward: 16.0 Training loss: 0.2835 Explore P: 0.8904\n",
            "Episode: 57 Total reward: 13.0 Training loss: 0.0243 Explore P: 0.8893\n",
            "Episode: 58 Total reward: 44.0 Training loss: 0.2579 Explore P: 0.8854\n",
            "Episode: 59 Total reward: 18.0 Training loss: 0.2560 Explore P: 0.8838\n",
            "Episode: 60 Total reward: 13.0 Training loss: 0.0041 Explore P: 0.8827\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 61 Total reward: 11.0 Training loss: 0.5982 Explore P: 0.8818\n",
            "Episode: 62 Total reward: 13.0 Training loss: 0.5070 Explore P: 0.8806\n",
            "Episode: 63 Total reward: 31.0 Training loss: 0.5086 Explore P: 0.8779\n",
            "Episode: 64 Total reward: 25.0 Training loss: 0.9873 Explore P: 0.8758\n",
            "Episode: 65 Total reward: 17.0 Training loss: 1.4221 Explore P: 0.8743\n",
            "Episode: 66 Total reward: 22.0 Training loss: 0.9576 Explore P: 0.8724\n",
            "Episode: 67 Total reward: 12.0 Training loss: 0.0299 Explore P: 0.8714\n",
            "Episode: 68 Total reward: 23.0 Training loss: 0.4999 Explore P: 0.8694\n",
            "Episode: 69 Total reward: 26.0 Training loss: 0.5055 Explore P: 0.8671\n",
            "Episode: 70 Total reward: 15.0 Training loss: 0.0144 Explore P: 0.8659\n",
            "Episode: 71 Total reward: 11.0 Training loss: 0.5107 Explore P: 0.8649\n",
            "Episode: 72 Total reward: 17.0 Training loss: 0.0522 Explore P: 0.8635\n",
            "Episode: 73 Total reward: 29.0 Training loss: 0.0190 Explore P: 0.8610\n",
            "Episode: 74 Total reward: 11.0 Training loss: 0.9978 Explore P: 0.8601\n",
            "Episode: 75 Total reward: 11.0 Training loss: 0.5050 Explore P: 0.8591\n",
            "Episode: 76 Total reward: 15.0 Training loss: 0.0543 Explore P: 0.8579\n",
            "Episode: 77 Total reward: 46.0 Training loss: 0.0304 Explore P: 0.8540\n",
            "Episode: 78 Total reward: 46.0 Training loss: 1.0036 Explore P: 0.8501\n",
            "Episode: 79 Total reward: 28.0 Training loss: 0.0085 Explore P: 0.8477\n",
            "Episode: 80 Total reward: 15.0 Training loss: 0.4998 Explore P: 0.8465\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 81 Total reward: 14.0 Training loss: 0.0775 Explore P: 0.8453\n",
            "Episode: 82 Total reward: 18.0 Training loss: 0.8450 Explore P: 0.8438\n",
            "Episode: 83 Total reward: 45.0 Training loss: 0.7849 Explore P: 0.8401\n",
            "Episode: 84 Total reward: 35.0 Training loss: 0.7894 Explore P: 0.8372\n",
            "Episode: 85 Total reward: 10.0 Training loss: 1.4822 Explore P: 0.8363\n",
            "Episode: 86 Total reward: 9.0 Training loss: 0.7864 Explore P: 0.8356\n",
            "Episode: 87 Total reward: 14.0 Training loss: 0.7969 Explore P: 0.8344\n",
            "Episode: 88 Total reward: 16.0 Training loss: 0.0124 Explore P: 0.8331\n",
            "Episode: 89 Total reward: 36.0 Training loss: 0.0496 Explore P: 0.8302\n",
            "Episode: 90 Total reward: 30.0 Training loss: 0.0316 Explore P: 0.8277\n",
            "Episode: 91 Total reward: 28.0 Training loss: 1.5187 Explore P: 0.8254\n",
            "Episode: 92 Total reward: 11.0 Training loss: 0.7768 Explore P: 0.8245\n",
            "Episode: 93 Total reward: 15.0 Training loss: 0.0565 Explore P: 0.8233\n",
            "Episode: 94 Total reward: 24.0 Training loss: 0.0320 Explore P: 0.8214\n",
            "Episode: 95 Total reward: 31.0 Training loss: 0.0812 Explore P: 0.8188\n",
            "Episode: 96 Total reward: 14.0 Training loss: 2.1473 Explore P: 0.8177\n",
            "Episode: 97 Total reward: 15.0 Training loss: 0.7997 Explore P: 0.8165\n",
            "Episode: 98 Total reward: 28.0 Training loss: 0.7843 Explore P: 0.8142\n",
            "Episode: 99 Total reward: 14.0 Training loss: 0.0517 Explore P: 0.8131\n",
            "Episode: 100 Total reward: 30.0 Training loss: 0.7856 Explore P: 0.8107\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 101 Total reward: 11.0 Training loss: 2.3639 Explore P: 0.8098\n",
            "Episode: 102 Total reward: 45.0 Training loss: 0.1351 Explore P: 0.8062\n",
            "Episode: 103 Total reward: 11.0 Training loss: 1.1195 Explore P: 0.8054\n",
            "Episode: 104 Total reward: 13.0 Training loss: 1.1172 Explore P: 0.8043\n",
            "Episode: 105 Total reward: 9.0 Training loss: 1.1146 Explore P: 0.8036\n",
            "Episode: 106 Total reward: 10.0 Training loss: 1.1936 Explore P: 0.8028\n",
            "Episode: 107 Total reward: 11.0 Training loss: 2.0856 Explore P: 0.8020\n",
            "Episode: 108 Total reward: 39.0 Training loss: 2.1803 Explore P: 0.7989\n",
            "Episode: 109 Total reward: 20.0 Training loss: 3.3746 Explore P: 0.7973\n",
            "Episode: 110 Total reward: 12.0 Training loss: 2.1456 Explore P: 0.7964\n",
            "Episode: 111 Total reward: 20.0 Training loss: 1.1348 Explore P: 0.7948\n",
            "Episode: 112 Total reward: 19.0 Training loss: 1.1178 Explore P: 0.7933\n",
            "Episode: 113 Total reward: 30.0 Training loss: 0.0318 Explore P: 0.7909\n",
            "Episode: 114 Total reward: 10.0 Training loss: 0.0051 Explore P: 0.7902\n",
            "Episode: 115 Total reward: 14.0 Training loss: 0.1276 Explore P: 0.7891\n",
            "Episode: 116 Total reward: 11.0 Training loss: 0.0257 Explore P: 0.7882\n",
            "Episode: 117 Total reward: 20.0 Training loss: 0.2517 Explore P: 0.7867\n",
            "Episode: 118 Total reward: 19.0 Training loss: 2.1006 Explore P: 0.7852\n",
            "Episode: 119 Total reward: 11.0 Training loss: 0.0363 Explore P: 0.7843\n",
            "Episode: 120 Total reward: 23.0 Training loss: 1.1125 Explore P: 0.7826\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 121 Total reward: 51.0 Training loss: 0.0826 Explore P: 0.7786\n",
            "Episode: 122 Total reward: 13.0 Training loss: 1.5278 Explore P: 0.7776\n",
            "Episode: 123 Total reward: 14.0 Training loss: 1.5045 Explore P: 0.7766\n",
            "Episode: 124 Total reward: 10.0 Training loss: 2.9315 Explore P: 0.7758\n",
            "Episode: 125 Total reward: 37.0 Training loss: 1.5145 Explore P: 0.7730\n",
            "Episode: 126 Total reward: 16.0 Training loss: 1.4890 Explore P: 0.7717\n",
            "Episode: 127 Total reward: 17.0 Training loss: 1.5027 Explore P: 0.7704\n",
            "Episode: 128 Total reward: 20.0 Training loss: 0.1603 Explore P: 0.7689\n",
            "Episode: 129 Total reward: 13.0 Training loss: 1.4986 Explore P: 0.7679\n",
            "Episode: 130 Total reward: 32.0 Training loss: 0.0362 Explore P: 0.7655\n",
            "Episode: 131 Total reward: 29.0 Training loss: 1.4971 Explore P: 0.7633\n",
            "Episode: 132 Total reward: 16.0 Training loss: 0.2051 Explore P: 0.7621\n",
            "Episode: 133 Total reward: 12.0 Training loss: 0.0389 Explore P: 0.7612\n",
            "Episode: 134 Total reward: 41.0 Training loss: 1.4884 Explore P: 0.7582\n",
            "Episode: 135 Total reward: 22.0 Training loss: 1.5237 Explore P: 0.7565\n",
            "Episode: 136 Total reward: 16.0 Training loss: 1.5250 Explore P: 0.7553\n",
            "Episode: 137 Total reward: 23.0 Training loss: 1.5367 Explore P: 0.7536\n",
            "Episode: 138 Total reward: 14.0 Training loss: 4.2439 Explore P: 0.7526\n",
            "Episode: 139 Total reward: 25.0 Training loss: 2.8784 Explore P: 0.7507\n",
            "Episode: 140 Total reward: 23.0 Training loss: 1.5005 Explore P: 0.7490\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 141 Total reward: 8.0 Training loss: 4.9237 Explore P: 0.7484\n",
            "Episode: 142 Total reward: 11.0 Training loss: 0.1067 Explore P: 0.7476\n",
            "Episode: 143 Total reward: 18.0 Training loss: 1.7855 Explore P: 0.7463\n",
            "Episode: 144 Total reward: 12.0 Training loss: 1.7972 Explore P: 0.7454\n",
            "Episode: 145 Total reward: 16.0 Training loss: 3.4461 Explore P: 0.7442\n",
            "Episode: 146 Total reward: 22.0 Training loss: 0.1418 Explore P: 0.7426\n",
            "Episode: 147 Total reward: 11.0 Training loss: 1.8371 Explore P: 0.7418\n",
            "Episode: 148 Total reward: 17.0 Training loss: 1.8288 Explore P: 0.7406\n",
            "Episode: 149 Total reward: 15.0 Training loss: 1.7980 Explore P: 0.7395\n",
            "Episode: 150 Total reward: 14.0 Training loss: 0.1141 Explore P: 0.7384\n",
            "Episode: 151 Total reward: 30.0 Training loss: 1.7978 Explore P: 0.7363\n",
            "Episode: 152 Total reward: 12.0 Training loss: 0.1290 Explore P: 0.7354\n",
            "Episode: 153 Total reward: 45.0 Training loss: 1.8202 Explore P: 0.7321\n",
            "Episode: 154 Total reward: 15.0 Training loss: 1.8130 Explore P: 0.7310\n",
            "Episode: 155 Total reward: 14.0 Training loss: 0.1102 Explore P: 0.7300\n",
            "Episode: 156 Total reward: 14.0 Training loss: 1.8302 Explore P: 0.7290\n",
            "Episode: 157 Total reward: 67.0 Training loss: 1.8088 Explore P: 0.7242\n",
            "Episode: 158 Total reward: 60.0 Training loss: 1.7918 Explore P: 0.7200\n",
            "Episode: 159 Total reward: 17.0 Training loss: 1.8046 Explore P: 0.7188\n",
            "Episode: 160 Total reward: 16.0 Training loss: 1.8007 Explore P: 0.7176\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 161 Total reward: 17.0 Training loss: 0.0318 Explore P: 0.7164\n",
            "Episode: 162 Total reward: 52.0 Training loss: 0.1227 Explore P: 0.7128\n",
            "Episode: 163 Total reward: 22.0 Training loss: 2.1751 Explore P: 0.7112\n",
            "Episode: 164 Total reward: 17.0 Training loss: 2.1393 Explore P: 0.7100\n",
            "Episode: 165 Total reward: 38.0 Training loss: 4.0793 Explore P: 0.7074\n",
            "Episode: 166 Total reward: 12.0 Training loss: 0.0152 Explore P: 0.7065\n",
            "Episode: 167 Total reward: 14.0 Training loss: 2.1448 Explore P: 0.7056\n",
            "Episode: 168 Total reward: 17.0 Training loss: 8.2302 Explore P: 0.7044\n",
            "Episode: 169 Total reward: 10.0 Training loss: 0.1530 Explore P: 0.7037\n",
            "Episode: 170 Total reward: 32.0 Training loss: 0.1076 Explore P: 0.7015\n",
            "Episode: 171 Total reward: 23.0 Training loss: 4.2903 Explore P: 0.6999\n",
            "Episode: 172 Total reward: 22.0 Training loss: 8.0452 Explore P: 0.6984\n",
            "Episode: 173 Total reward: 14.0 Training loss: 0.3409 Explore P: 0.6974\n",
            "Episode: 174 Total reward: 32.0 Training loss: 4.0420 Explore P: 0.6952\n",
            "Episode: 175 Total reward: 17.0 Training loss: 0.1742 Explore P: 0.6940\n",
            "Episode: 176 Total reward: 17.0 Training loss: 0.0183 Explore P: 0.6929\n",
            "Episode: 177 Total reward: 8.0 Training loss: 2.1591 Explore P: 0.6923\n",
            "Episode: 178 Total reward: 13.0 Training loss: 2.1504 Explore P: 0.6914\n",
            "Episode: 179 Total reward: 16.0 Training loss: 2.1368 Explore P: 0.6903\n",
            "Episode: 180 Total reward: 27.0 Training loss: 0.1612 Explore P: 0.6885\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 181 Total reward: 12.0 Training loss: 0.0514 Explore P: 0.6877\n",
            "Episode: 182 Total reward: 14.0 Training loss: 0.1162 Explore P: 0.6868\n",
            "Episode: 183 Total reward: 12.0 Training loss: 2.5961 Explore P: 0.6859\n",
            "Episode: 184 Total reward: 19.0 Training loss: 0.2354 Explore P: 0.6847\n",
            "Episode: 185 Total reward: 12.0 Training loss: 0.1388 Explore P: 0.6838\n",
            "Episode: 186 Total reward: 12.0 Training loss: 2.5960 Explore P: 0.6830\n",
            "Episode: 187 Total reward: 15.0 Training loss: 0.0068 Explore P: 0.6820\n",
            "Episode: 188 Total reward: 14.0 Training loss: 2.5379 Explore P: 0.6811\n",
            "Episode: 189 Total reward: 28.0 Training loss: 7.1045 Explore P: 0.6792\n",
            "Episode: 190 Total reward: 8.0 Training loss: 2.5581 Explore P: 0.6787\n",
            "Episode: 191 Total reward: 33.0 Training loss: 4.9560 Explore P: 0.6765\n",
            "Episode: 192 Total reward: 31.0 Training loss: 0.3019 Explore P: 0.6744\n",
            "Episode: 193 Total reward: 9.0 Training loss: 0.0585 Explore P: 0.6738\n",
            "Episode: 194 Total reward: 17.0 Training loss: 2.5975 Explore P: 0.6727\n",
            "Episode: 195 Total reward: 13.0 Training loss: 2.5522 Explore P: 0.6718\n",
            "Episode: 196 Total reward: 10.0 Training loss: 4.8734 Explore P: 0.6712\n",
            "Episode: 197 Total reward: 18.0 Training loss: 2.6078 Explore P: 0.6700\n",
            "Episode: 198 Total reward: 12.0 Training loss: 2.6244 Explore P: 0.6692\n",
            "Episode: 199 Total reward: 27.0 Training loss: 0.0961 Explore P: 0.6674\n",
            "Episode: 200 Total reward: 10.0 Training loss: 2.6683 Explore P: 0.6668\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 201 Total reward: 11.0 Training loss: 7.7503 Explore P: 0.6660\n",
            "Episode: 202 Total reward: 34.0 Training loss: 5.2779 Explore P: 0.6638\n",
            "Episode: 203 Total reward: 8.0 Training loss: 7.4572 Explore P: 0.6633\n",
            "Episode: 204 Total reward: 19.0 Training loss: 0.4150 Explore P: 0.6620\n",
            "Episode: 205 Total reward: 10.0 Training loss: 0.1365 Explore P: 0.6614\n",
            "Episode: 206 Total reward: 12.0 Training loss: 10.4173 Explore P: 0.6606\n",
            "Episode: 207 Total reward: 15.0 Training loss: 0.1532 Explore P: 0.6596\n",
            "Episode: 208 Total reward: 13.0 Training loss: 2.8043 Explore P: 0.6588\n",
            "Episode: 209 Total reward: 26.0 Training loss: 5.4599 Explore P: 0.6571\n",
            "Episode: 210 Total reward: 11.0 Training loss: 2.7295 Explore P: 0.6564\n",
            "Episode: 211 Total reward: 11.0 Training loss: 0.1443 Explore P: 0.6557\n",
            "Episode: 212 Total reward: 17.0 Training loss: 2.8152 Explore P: 0.6546\n",
            "Episode: 213 Total reward: 10.0 Training loss: 10.8328 Explore P: 0.6539\n",
            "Episode: 214 Total reward: 18.0 Training loss: 0.0517 Explore P: 0.6528\n",
            "Episode: 215 Total reward: 15.0 Training loss: 2.7519 Explore P: 0.6518\n",
            "Episode: 216 Total reward: 7.0 Training loss: 5.2478 Explore P: 0.6514\n",
            "Episode: 217 Total reward: 10.0 Training loss: 0.0177 Explore P: 0.6507\n",
            "Episode: 218 Total reward: 9.0 Training loss: 2.7640 Explore P: 0.6502\n",
            "Episode: 219 Total reward: 12.0 Training loss: 2.9066 Explore P: 0.6494\n",
            "Episode: 220 Total reward: 12.0 Training loss: 2.7491 Explore P: 0.6486\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 221 Total reward: 17.0 Training loss: 0.0375 Explore P: 0.6475\n",
            "Episode: 222 Total reward: 43.0 Training loss: 3.0166 Explore P: 0.6448\n",
            "Episode: 223 Total reward: 13.0 Training loss: 3.0145 Explore P: 0.6440\n",
            "Episode: 224 Total reward: 26.0 Training loss: 3.0306 Explore P: 0.6423\n",
            "Episode: 225 Total reward: 15.0 Training loss: 0.0892 Explore P: 0.6414\n",
            "Episode: 226 Total reward: 10.0 Training loss: 3.1027 Explore P: 0.6407\n",
            "Episode: 227 Total reward: 8.0 Training loss: 5.8387 Explore P: 0.6402\n",
            "Episode: 228 Total reward: 25.0 Training loss: 3.0316 Explore P: 0.6387\n",
            "Episode: 229 Total reward: 25.0 Training loss: 3.0161 Explore P: 0.6371\n",
            "Episode: 230 Total reward: 24.0 Training loss: 8.7538 Explore P: 0.6356\n",
            "Episode: 231 Total reward: 18.0 Training loss: 0.1354 Explore P: 0.6345\n",
            "Episode: 232 Total reward: 18.0 Training loss: 0.1038 Explore P: 0.6333\n",
            "Episode: 233 Total reward: 11.0 Training loss: 8.9189 Explore P: 0.6327\n",
            "Episode: 234 Total reward: 15.0 Training loss: 3.1126 Explore P: 0.6317\n",
            "Episode: 235 Total reward: 26.0 Training loss: 3.0416 Explore P: 0.6301\n",
            "Episode: 236 Total reward: 9.0 Training loss: 5.8368 Explore P: 0.6296\n",
            "Episode: 237 Total reward: 12.0 Training loss: 3.0308 Explore P: 0.6288\n",
            "Episode: 238 Total reward: 17.0 Training loss: 3.1399 Explore P: 0.6278\n",
            "Episode: 239 Total reward: 11.0 Training loss: 8.7439 Explore P: 0.6271\n",
            "Episode: 240 Total reward: 19.0 Training loss: 0.3452 Explore P: 0.6259\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 241 Total reward: 8.0 Training loss: 0.3860 Explore P: 0.6254\n",
            "Episode: 242 Total reward: 17.0 Training loss: 3.2924 Explore P: 0.6244\n",
            "Episode: 243 Total reward: 10.0 Training loss: 0.2008 Explore P: 0.6238\n",
            "Episode: 244 Total reward: 15.0 Training loss: 0.1378 Explore P: 0.6228\n",
            "Episode: 245 Total reward: 11.0 Training loss: 3.2781 Explore P: 0.6222\n",
            "Episode: 246 Total reward: 12.0 Training loss: 3.2924 Explore P: 0.6214\n",
            "Episode: 247 Total reward: 23.0 Training loss: 3.3254 Explore P: 0.6200\n",
            "Episode: 248 Total reward: 9.0 Training loss: 3.3511 Explore P: 0.6195\n",
            "Episode: 249 Total reward: 11.0 Training loss: 3.2922 Explore P: 0.6188\n",
            "Episode: 250 Total reward: 9.0 Training loss: 10.2908 Explore P: 0.6183\n",
            "Episode: 251 Total reward: 10.0 Training loss: 0.1838 Explore P: 0.6177\n",
            "Episode: 252 Total reward: 8.0 Training loss: 6.2233 Explore P: 0.6172\n",
            "Episode: 253 Total reward: 17.0 Training loss: 0.0371 Explore P: 0.6161\n",
            "Episode: 254 Total reward: 33.0 Training loss: 0.2640 Explore P: 0.6141\n",
            "Episode: 255 Total reward: 14.0 Training loss: 6.2006 Explore P: 0.6133\n",
            "Episode: 256 Total reward: 10.0 Training loss: 6.4183 Explore P: 0.6127\n",
            "Episode: 257 Total reward: 19.0 Training loss: 3.3404 Explore P: 0.6115\n",
            "Episode: 258 Total reward: 15.0 Training loss: 0.1573 Explore P: 0.6106\n",
            "Episode: 259 Total reward: 14.0 Training loss: 0.3046 Explore P: 0.6098\n",
            "Episode: 260 Total reward: 9.0 Training loss: 0.2358 Explore P: 0.6093\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 261 Total reward: 8.0 Training loss: 0.5525 Explore P: 0.6088\n",
            "Episode: 262 Total reward: 9.0 Training loss: 3.6949 Explore P: 0.6082\n",
            "Episode: 263 Total reward: 13.0 Training loss: 0.3470 Explore P: 0.6075\n",
            "Episode: 264 Total reward: 24.0 Training loss: 0.0554 Explore P: 0.6060\n",
            "Episode: 265 Total reward: 16.0 Training loss: 10.0661 Explore P: 0.6051\n",
            "Episode: 266 Total reward: 29.0 Training loss: 3.5999 Explore P: 0.6034\n",
            "Episode: 267 Total reward: 12.0 Training loss: 3.5858 Explore P: 0.6027\n",
            "Episode: 268 Total reward: 27.0 Training loss: 3.6614 Explore P: 0.6011\n",
            "Episode: 269 Total reward: 10.0 Training loss: 3.6278 Explore P: 0.6005\n",
            "Episode: 270 Total reward: 9.0 Training loss: 3.6454 Explore P: 0.5999\n",
            "Episode: 271 Total reward: 28.0 Training loss: 0.1999 Explore P: 0.5983\n",
            "Episode: 272 Total reward: 11.0 Training loss: 3.5995 Explore P: 0.5976\n",
            "Episode: 273 Total reward: 11.0 Training loss: 0.6870 Explore P: 0.5970\n",
            "Episode: 274 Total reward: 15.0 Training loss: 3.6976 Explore P: 0.5961\n",
            "Episode: 275 Total reward: 15.0 Training loss: 3.7217 Explore P: 0.5952\n",
            "Episode: 276 Total reward: 17.0 Training loss: 6.9137 Explore P: 0.5942\n",
            "Episode: 277 Total reward: 19.0 Training loss: 0.0536 Explore P: 0.5931\n",
            "Episode: 278 Total reward: 31.0 Training loss: 6.7575 Explore P: 0.5913\n",
            "Episode: 279 Total reward: 9.0 Training loss: 3.5770 Explore P: 0.5908\n",
            "Episode: 280 Total reward: 14.0 Training loss: 0.1283 Explore P: 0.5900\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 281 Total reward: 10.0 Training loss: 7.9423 Explore P: 0.5894\n",
            "Episode: 282 Total reward: 7.0 Training loss: 8.0465 Explore P: 0.5890\n",
            "Episode: 283 Total reward: 14.0 Training loss: 0.6240 Explore P: 0.5882\n",
            "Episode: 284 Total reward: 20.0 Training loss: 4.1629 Explore P: 0.5870\n",
            "Episode: 285 Total reward: 23.0 Training loss: 4.2051 Explore P: 0.5857\n",
            "Episode: 286 Total reward: 17.0 Training loss: 0.3821 Explore P: 0.5847\n",
            "Episode: 287 Total reward: 15.0 Training loss: 0.1626 Explore P: 0.5839\n",
            "Episode: 288 Total reward: 37.0 Training loss: 4.1887 Explore P: 0.5818\n",
            "Episode: 289 Total reward: 44.0 Training loss: 0.0736 Explore P: 0.5792\n",
            "Episode: 290 Total reward: 24.0 Training loss: 0.3166 Explore P: 0.5779\n",
            "Episode: 291 Total reward: 30.0 Training loss: 0.2952 Explore P: 0.5762\n",
            "Episode: 292 Total reward: 11.0 Training loss: 7.8758 Explore P: 0.5756\n",
            "Episode: 293 Total reward: 12.0 Training loss: 4.2299 Explore P: 0.5749\n",
            "Episode: 294 Total reward: 14.0 Training loss: 0.2449 Explore P: 0.5741\n",
            "Episode: 295 Total reward: 16.0 Training loss: 8.0139 Explore P: 0.5732\n",
            "Episode: 296 Total reward: 10.0 Training loss: 0.6093 Explore P: 0.5726\n",
            "Episode: 297 Total reward: 18.0 Training loss: 4.2858 Explore P: 0.5716\n",
            "Episode: 298 Total reward: 10.0 Training loss: 0.1721 Explore P: 0.5710\n",
            "Episode: 299 Total reward: 10.0 Training loss: 4.2448 Explore P: 0.5705\n",
            "Episode: 300 Total reward: 11.0 Training loss: 0.2569 Explore P: 0.5699\n",
            "***** Updated target QNetwork *****\n",
            "Episode: 301 Total reward: 9.0 Training loss: 8.8867 Explore P: 0.5694\n",
            "Episode: 302 Total reward: 10.0 Training loss: 4.6694 Explore P: 0.5688\n",
            "Episode: 303 Total reward: 9.0 Training loss: 4.7527 Explore P: 0.5683\n",
            "Episode: 304 Total reward: 15.0 Training loss: 4.6686 Explore P: 0.5675\n",
            "Episode: 305 Total reward: 9.0 Training loss: 0.4159 Explore P: 0.5670\n",
            "Episode: 306 Total reward: 12.0 Training loss: 4.6265 Explore P: 0.5663\n",
            "Episode: 307 Total reward: 10.0 Training loss: 4.6182 Explore P: 0.5657\n",
            "Episode: 308 Total reward: 11.0 Training loss: 9.1250 Explore P: 0.5651\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3d215934da52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-3d215934da52>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRNGKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmainQN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'main'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mrewards_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmainQN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mplot_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-7762bd29c36d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(rng, env, mainQN)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# Sample mini-batch from memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-addcfa49bc7f>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, key, batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         idx = jax.random.choice(key,\n\u001b[0;32m---> 12\u001b[0;31m                                \u001b[0mjp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                                shape=(batch_size, ))\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36marange\u001b[0;34m(start, stop, step, dtype)\u001b[0m\n\u001b[1;32m   3108\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3109\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3110\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miota\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# avoids materializing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3111\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3112\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"start\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36miota\u001b[0;34m(dtype, size)\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonicalize_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m   \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcanonicalize_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0miota_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbroadcasted_iota\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mShape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    262\u001b[0m         args, used_axis_names(self, params) if self._dispatch_on_params else None)\n\u001b[1;32m    263\u001b[0m     \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapply_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m   \u001b[0;34m\"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m   \u001b[0mcompiled_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxla_primitive_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0munsafe_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/util.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trace_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_clear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_clear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/util.py\u001b[0m in \u001b[0;36mcached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlru_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}