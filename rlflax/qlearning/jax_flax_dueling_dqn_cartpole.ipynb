{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jax_qlearning_cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anirbanl/jax-code/blob/master/rlflax/qlearning/jax_flax_dueling_dqn_cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJIO3iDY31YN"
      },
      "source": [
        "# Based on sources:\n",
        "\n",
        "1. https://markelsanz14.medium.com/introduction-to-reinforcement-learning-part-4-double-dqn-and-dueling-dqn-b349c9a61ea1\n",
        "2. https://towardsdatascience.com/dueling-deep-q-networks-81ffab672751\n",
        "3. https://medium.com/@parsa_h_m/deep-reinforcement-learning-dqn-double-dqn-dueling-dqn-noisy-dqn-and-dqn-with-prioritized-551f621a9823\n",
        "4. https://github.com/higgsfield/RL-Adventure/blob/master/3.dueling%20dqn.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_jS9qg6EG4l",
        "outputId": "55793d3a-cde1-4db2-dbad-3179eba4a08a"
      },
      "source": [
        "!pip install jax jaxlib flax"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (0.2.13)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (0.1.66+cuda110)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.7/dist-packages (0.3.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from jax) (1.19.5)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.12)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.4.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax) (1.0.2)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (from flax) (0.0.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax) (3.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax) (1.15.0)\n",
            "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax->flax) (0.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.8.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.11.1)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.1.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt1r4--nEPJm"
      },
      "source": [
        "import gym\n",
        "gym.logger.set_level(40) # suppress warnings (please remove if gives error)\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import jax\n",
        "import jax.numpy as jp\n",
        "from jax.ops import index, index_add, index_update\n",
        "from jax import jit, grad, vmap, random, jacrev, jacobian, jacfwd, value_and_grad\n",
        "from functools import partial\n",
        "from jax.tree_util import tree_multimap  # Element-wise manipulation of collections of numpy arrays\n",
        "from flax import linen as nn           # The Linen API\n",
        "from flax.training import train_state  # Useful dataclass to keep train state\n",
        "import optax                           # Optimizers\n",
        "from typing import Sequence, Type"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK9XAT81EWW0"
      },
      "source": [
        "from collections import deque\n",
        "class Memory():\n",
        "    def __init__(self, max_size = 1000):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "    \n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "            \n",
        "    def sample(self, key, batch_size):\n",
        "        key, _ = jax.random.split(key)\n",
        "        idx = jax.random.choice(key,\n",
        "                               jp.arange(len(self.buffer)), \n",
        "                               shape=(batch_size, ))\n",
        "        # print(f\"\\nIds:{jp.mean(idx)}\\n\")\n",
        "        return [self.buffer[ii] for ii in idx]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtUX4AXiFWVK"
      },
      "source": [
        "train_episodes = 1000          # max number of episodes to learn from\n",
        "max_steps = 200                # max steps in an episode\n",
        "gamma = 0.99                   # future reward discount\n",
        "\n",
        "# Exploration parameters\n",
        "explore_start = 1.0            # exploration probability at start\n",
        "explore_stop = 0.01            # minimum exploration probability \n",
        "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
        "\n",
        "# Network parameters\n",
        "hidden_size = 64               # number of units in each Q-network hidden layer\n",
        "learning_rate = 1e-4         # Q-network learning rate\n",
        "\n",
        "# Memory parameters\n",
        "memory_size = 10000            # memory capacity\n",
        "batch_size = 20                # experience mini-batch size\n",
        "pretrain_length = batch_size   # number experiences to pretrain the memory"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSJfnThHFouD"
      },
      "source": [
        "#Define Q-network\n",
        "class QNetwork:\n",
        "    def __init__(self, rng, env, learning_rate=0.01, state_size=4, \n",
        "                 action_size=2, hidden_size=10, \n",
        "                 name='QNetwork'):\n",
        "        self.key = rng\n",
        "        self.env = env\n",
        "        # print(f\"QNetwork rng:{rng}\")\n",
        "\n",
        "        class Sequential(nn.Module):\n",
        "            layers: Sequence[nn.Module]\n",
        "\n",
        "            @nn.compact\n",
        "            def __call__(self, x):\n",
        "                for layer in self.layers:\n",
        "  \t                x = layer(x)\n",
        "                return x\n",
        "\n",
        "        class DuelingDQN(nn.Module):\n",
        "\n",
        "            def setup(self):\n",
        "                self.feature_model = Sequential(\n",
        "                    layers = [nn.Dense(hidden_size), nn.relu])\n",
        "                self.advantage_model = Sequential(\n",
        "                    layers = [nn.Dense(hidden_size), nn.relu, nn.Dense(action_size)])\n",
        "                self.value_model = Sequential(\n",
        "                    layers = [nn.Dense(hidden_size), nn.relu, nn.Dense(1)])\n",
        "\n",
        "            def __call__(self, x):\n",
        "                x = self.feature_model(x)\n",
        "                advantage = self.advantage_model(x)\n",
        "                value = self.value_model(x)\n",
        "                return value + advantage - jp.mean(advantage)\n",
        "\n",
        "        def create_train_state(rng, learning_rate, input_size):\n",
        "            \"\"\"Creates initial `TrainState`.\"\"\"\n",
        "            model = DuelingDQN()\n",
        "            params = model.init(rng, jp.ones((input_size, )))#['params']\n",
        "            # print(f\"Params:{params}\")\n",
        "            tx = optax.adam(learning_rate)\n",
        "            return train_state.TrainState.create(\n",
        "                apply_fn=model.apply, params=params, tx=tx)\n",
        "\n",
        "        self.ts = create_train_state(rng, learning_rate, state_size)\n",
        "\n",
        "        @jit\n",
        "        def train_step(ts, inputs, actions, targets):\n",
        "\n",
        "            def loss_fun(params, inputs, actions, targets):\n",
        "                output = ts.apply_fn(params, inputs)\n",
        "                selectedq = jp.sum(actions*output, axis=-1)\n",
        "                diff = selectedq - jax.lax.stop_gradient(targets)\n",
        "                return jp.mean(diff**2)\n",
        "\n",
        "            loss, g = value_and_grad(loss_fun)(ts.params, inputs, actions, targets)\n",
        "            return ts.apply_gradients(grads=g), loss\n",
        "\n",
        "        self.train_fn = train_step\n",
        "\n",
        "\n",
        "    def act(self, state, explore_p):\n",
        "        self.key, _ = jax.random.split(self.key)\n",
        "        # print(f\"Act key:{self.key}\")\n",
        "        uf = jax.random.uniform(self.key, (1,), minval=0.0, maxval=1.0)[0]\n",
        "        if explore_p > uf:\n",
        "            # Make a random action\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            # Get action from Q-network\n",
        "            qvalues = self.ts.apply_fn(self.ts.params, state)\n",
        "            action = jp.argmax(qvalues).item()\n",
        "        return action\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0fPvqRcLdf4"
      },
      "source": [
        "def init_memory(env):\n",
        "    # Initialize the simulation\n",
        "    env.reset()\n",
        "    # Take one random step to get the pole and cart moving\n",
        "    state, reward, done, _ = env.step(env.action_space.sample())\n",
        "    # print(f\"@@@@@@ Env init state:{state} @@@@@@\")\n",
        "\n",
        "    memory = Memory(max_size=memory_size)\n",
        "\n",
        "    # Make a bunch of random actions and store the experiences\n",
        "    for ii in range(pretrain_length):\n",
        "        # Uncomment the line below to watch the simulation\n",
        "        # env.render()\n",
        "\n",
        "        # Make a random action\n",
        "        action = env.action_space.sample()\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        # print(f\"@@@@@@ Env action:{action} @@@@@@\")\n",
        "        # print(f\"@@@@@@ Env next state:{next_state} @@@@@@\")\n",
        "\n",
        "        if done:\n",
        "            # The simulation fails so no next state\n",
        "            next_state = jp.zeros(state.shape)\n",
        "            # Add experience to memory\n",
        "            memory.add((state, action, reward, next_state))\n",
        "            \n",
        "            # Start new episode\n",
        "            env.reset()\n",
        "            # Take one random step to get the pole and cart moving\n",
        "            state, reward, done, _ = env.step(env.action_space.sample())\n",
        "        else:\n",
        "            # Add experience to memory\n",
        "            memory.add((state, action, reward, next_state))\n",
        "            state = next_state\n",
        "    return memory, state"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vpr3LNUMkaG"
      },
      "source": [
        "# Now train with experiences\n",
        "def one_hot(x, k, dtype=jp.float32):\n",
        "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
        "  return jp.array(x[:, None] == jp.arange(k), dtype)\n",
        "\n",
        "\n",
        "def train(rng, env, mainQN):\n",
        "    # print(f\"train rng:{rng}\")\n",
        "    rewards_list = []    \n",
        "    step = 0\n",
        "    memory, state = init_memory(env)\n",
        "    for ep in range(1, train_episodes):\n",
        "        total_reward = 0\n",
        "        t = 0\n",
        "        while t < max_steps:\n",
        "            step += 1\n",
        "            # Uncomment this next line to watch the training\n",
        "            # env.render() \n",
        "            \n",
        "            # Explore or Exploit\n",
        "            explore_p = explore_stop + (explore_start - explore_stop)*jp.exp(-decay_rate*step) \n",
        "            action = mainQN.act(state, explore_p)\n",
        "            \n",
        "            # Take action, get new state and reward\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            # print(f\"State:{state}\")\n",
        "            # print(f\"Reward:{reward}\")\n",
        "            # print(f\"Action:{action}\")\n",
        "            # print(f\"Done:{done}\\n\")\n",
        "\n",
        "            total_reward += reward\n",
        "            \n",
        "            if done:\n",
        "                # the episode ends so no next state\n",
        "                next_state = jp.zeros(state.shape)\n",
        "                t = max_steps\n",
        "                \n",
        "                print('Episode: {}'.format(ep),\n",
        "                    'Total reward: {}'.format(total_reward),\n",
        "                    'Training loss: {:.4f}'.format(loss),\n",
        "                    'Explore P: {:.4f}'.format(explore_p))\n",
        "                rewards_list.append((ep, total_reward))\n",
        "                \n",
        "                # Add experience to memory\n",
        "                memory.add((state, action, reward, next_state))\n",
        "                \n",
        "                # Start new episode\n",
        "                env.reset()\n",
        "                # Take one random step to get the pole and cart moving\n",
        "                state, reward, done, _ = env.step(env.action_space.sample())\n",
        "\n",
        "            else:\n",
        "                # Add experience to memory\n",
        "                memory.add((state, action, reward, next_state))\n",
        "                state = next_state\n",
        "                t += 1\n",
        "            \n",
        "            # Sample mini-batch from memory\n",
        "            batch = memory.sample(rng, batch_size)\n",
        "            states = jp.array([each[0] for each in batch])\n",
        "            actions = one_hot(jp.array([each[1] for each in batch]), 2)\n",
        "            rewards = jp.array([each[2] for each in batch])\n",
        "            next_states = jp.array([each[3] for each in batch])\n",
        "            \n",
        "            # Train network\n",
        "            target_Qs = mainQN.ts.apply_fn(mainQN.ts.params, next_states)\n",
        "            \n",
        "            # Set target_Qs to 0 for states where episode ends\n",
        "            episode_ends = (next_states == jp.zeros(states[0].shape)).all(axis=1)\n",
        "            new_target_Qs = index_update(target_Qs, index[episode_ends], (0, 0))\n",
        "            target_Qs = new_target_Qs\n",
        "            \n",
        "            targets = rewards + gamma * jp.max(target_Qs, axis=1)\n",
        "            # print(states.shape, targets.shape, targets)\n",
        "            mainQN.ts, loss = mainQN.train_fn(mainQN.ts, states, actions, targets)\n",
        "\n",
        "    return rewards_list\n",
        "\n",
        "def plot_scores(rewards_list):\n",
        "    def running_mean(x, N):\n",
        "        cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
        "        return (cumsum[N:] - cumsum[:-N]) / N\n",
        "    eps, rews = np.array(rewards_list).T\n",
        "    smoothed_rews = running_mean(rews, 10)\n",
        "    plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
        "    plt.plot(eps, rews, color='grey', alpha=0.3)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.show()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeurKJ6oNcZF",
        "outputId": "93c8f8c0-63ff-4838-805e-7ecb3675f3d2"
      },
      "source": [
        "def main():\n",
        "    seed = 0\n",
        "    env = gym.make('CartPole-v0')\n",
        "    env.seed(seed)\n",
        "    env.action_space.seed(seed)\n",
        "    print('observation space:', env.observation_space)\n",
        "    print('action space:', env.action_space.n)\n",
        "    rng = jax.random.PRNGKey(seed)\n",
        "    mainQN = QNetwork(rng, env, name='main', hidden_size=hidden_size, learning_rate=learning_rate)\n",
        "    rewards_list = train(rng, env, mainQN)\n",
        "    plot_scores(rewards_list)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
            "action space: 2\n",
            "Episode: 1 Total reward: 31.0 Training loss: 1.0755 Explore P: 0.9969\n",
            "Episode: 2 Total reward: 54.0 Training loss: 1.1272 Explore P: 0.9916\n",
            "Episode: 3 Total reward: 28.0 Training loss: 1.0321 Explore P: 0.9889\n",
            "Episode: 4 Total reward: 21.0 Training loss: 1.0920 Explore P: 0.9868\n",
            "Episode: 5 Total reward: 8.0 Training loss: 1.2192 Explore P: 0.9860\n",
            "Episode: 6 Total reward: 13.0 Training loss: 1.1947 Explore P: 0.9848\n",
            "Episode: 7 Total reward: 20.0 Training loss: 1.0323 Explore P: 0.9828\n",
            "Episode: 8 Total reward: 35.0 Training loss: 1.4423 Explore P: 0.9794\n",
            "Episode: 9 Total reward: 27.0 Training loss: 1.3803 Explore P: 0.9768\n",
            "Episode: 10 Total reward: 22.0 Training loss: 1.1606 Explore P: 0.9747\n",
            "Episode: 11 Total reward: 15.0 Training loss: 1.8741 Explore P: 0.9732\n",
            "Episode: 12 Total reward: 28.0 Training loss: 1.6710 Explore P: 0.9705\n",
            "Episode: 13 Total reward: 10.0 Training loss: 1.8584 Explore P: 0.9696\n",
            "Episode: 14 Total reward: 16.0 Training loss: 1.7576 Explore P: 0.9681\n",
            "Episode: 15 Total reward: 10.0 Training loss: 2.0772 Explore P: 0.9671\n",
            "Episode: 16 Total reward: 21.0 Training loss: 2.0026 Explore P: 0.9651\n",
            "Episode: 17 Total reward: 15.0 Training loss: 6.0408 Explore P: 0.9637\n",
            "Episode: 18 Total reward: 10.0 Training loss: 4.2056 Explore P: 0.9627\n",
            "Episode: 19 Total reward: 33.0 Training loss: 2.7284 Explore P: 0.9596\n",
            "Episode: 20 Total reward: 21.0 Training loss: 4.8622 Explore P: 0.9576\n",
            "Episode: 21 Total reward: 8.0 Training loss: 9.5753 Explore P: 0.9568\n",
            "Episode: 22 Total reward: 21.0 Training loss: 8.5751 Explore P: 0.9548\n",
            "Episode: 23 Total reward: 10.0 Training loss: 2.3432 Explore P: 0.9539\n",
            "Episode: 24 Total reward: 15.0 Training loss: 3.3483 Explore P: 0.9525\n",
            "Episode: 25 Total reward: 10.0 Training loss: 4.0226 Explore P: 0.9515\n",
            "Episode: 26 Total reward: 12.0 Training loss: 5.0237 Explore P: 0.9504\n",
            "Episode: 27 Total reward: 16.0 Training loss: 2.9378 Explore P: 0.9489\n",
            "Episode: 28 Total reward: 16.0 Training loss: 15.4454 Explore P: 0.9474\n",
            "Episode: 29 Total reward: 15.0 Training loss: 8.9545 Explore P: 0.9460\n",
            "Episode: 30 Total reward: 14.0 Training loss: 4.1658 Explore P: 0.9447\n",
            "Episode: 31 Total reward: 9.0 Training loss: 6.9405 Explore P: 0.9438\n",
            "Episode: 32 Total reward: 16.0 Training loss: 6.1465 Explore P: 0.9423\n",
            "Episode: 33 Total reward: 19.0 Training loss: 4.3574 Explore P: 0.9406\n",
            "Episode: 34 Total reward: 15.0 Training loss: 16.8464 Explore P: 0.9392\n",
            "Episode: 35 Total reward: 9.0 Training loss: 13.9244 Explore P: 0.9383\n",
            "Episode: 36 Total reward: 12.0 Training loss: 4.2801 Explore P: 0.9372\n",
            "Episode: 37 Total reward: 11.0 Training loss: 23.3024 Explore P: 0.9362\n",
            "Episode: 38 Total reward: 34.0 Training loss: 5.9704 Explore P: 0.9331\n",
            "Episode: 39 Total reward: 23.0 Training loss: 7.4357 Explore P: 0.9309\n",
            "Episode: 40 Total reward: 18.0 Training loss: 12.0221 Explore P: 0.9293\n",
            "Episode: 41 Total reward: 9.0 Training loss: 8.5438 Explore P: 0.9285\n",
            "Episode: 42 Total reward: 13.0 Training loss: 46.0423 Explore P: 0.9273\n",
            "Episode: 43 Total reward: 20.0 Training loss: 3.9695 Explore P: 0.9254\n",
            "Episode: 44 Total reward: 27.0 Training loss: 31.2436 Explore P: 0.9230\n",
            "Episode: 45 Total reward: 10.0 Training loss: 16.4892 Explore P: 0.9221\n",
            "Episode: 46 Total reward: 17.0 Training loss: 15.9647 Explore P: 0.9205\n",
            "Episode: 47 Total reward: 25.0 Training loss: 5.5154 Explore P: 0.9182\n",
            "Episode: 48 Total reward: 22.0 Training loss: 4.9542 Explore P: 0.9162\n",
            "Episode: 49 Total reward: 14.0 Training loss: 9.8756 Explore P: 0.9150\n",
            "Episode: 50 Total reward: 22.0 Training loss: 8.3530 Explore P: 0.9130\n",
            "Episode: 51 Total reward: 11.0 Training loss: 8.4110 Explore P: 0.9120\n",
            "Episode: 52 Total reward: 16.0 Training loss: 19.8922 Explore P: 0.9105\n",
            "Episode: 53 Total reward: 19.0 Training loss: 18.5638 Explore P: 0.9088\n",
            "Episode: 54 Total reward: 19.0 Training loss: 9.5262 Explore P: 0.9071\n",
            "Episode: 55 Total reward: 24.0 Training loss: 95.5796 Explore P: 0.9050\n",
            "Episode: 56 Total reward: 32.0 Training loss: 7.0350 Explore P: 0.9021\n",
            "Episode: 57 Total reward: 17.0 Training loss: 45.8084 Explore P: 0.9006\n",
            "Episode: 58 Total reward: 9.0 Training loss: 9.5456 Explore P: 0.8998\n",
            "Episode: 59 Total reward: 14.0 Training loss: 45.3930 Explore P: 0.8986\n",
            "Episode: 60 Total reward: 14.0 Training loss: 41.4355 Explore P: 0.8973\n",
            "Episode: 61 Total reward: 45.0 Training loss: 19.8576 Explore P: 0.8933\n",
            "Episode: 62 Total reward: 11.0 Training loss: 57.4208 Explore P: 0.8924\n",
            "Episode: 63 Total reward: 14.0 Training loss: 27.6897 Explore P: 0.8911\n",
            "Episode: 64 Total reward: 9.0 Training loss: 12.1536 Explore P: 0.8903\n",
            "Episode: 65 Total reward: 46.0 Training loss: 51.4933 Explore P: 0.8863\n",
            "Episode: 66 Total reward: 23.0 Training loss: 14.5171 Explore P: 0.8843\n",
            "Episode: 67 Total reward: 26.0 Training loss: 35.6388 Explore P: 0.8820\n",
            "Episode: 68 Total reward: 33.0 Training loss: 27.0655 Explore P: 0.8791\n",
            "Episode: 69 Total reward: 20.0 Training loss: 115.1167 Explore P: 0.8774\n",
            "Episode: 70 Total reward: 9.0 Training loss: 12.1869 Explore P: 0.8766\n",
            "Episode: 71 Total reward: 15.0 Training loss: 87.4874 Explore P: 0.8753\n",
            "Episode: 72 Total reward: 12.0 Training loss: 34.4301 Explore P: 0.8743\n",
            "Episode: 73 Total reward: 18.0 Training loss: 132.2488 Explore P: 0.8727\n",
            "Episode: 74 Total reward: 16.0 Training loss: 13.2809 Explore P: 0.8714\n",
            "Episode: 75 Total reward: 32.0 Training loss: 15.2042 Explore P: 0.8686\n",
            "Episode: 76 Total reward: 12.0 Training loss: 27.4722 Explore P: 0.8676\n",
            "Episode: 77 Total reward: 16.0 Training loss: 38.4908 Explore P: 0.8662\n",
            "Episode: 78 Total reward: 18.0 Training loss: 50.5850 Explore P: 0.8647\n",
            "Episode: 79 Total reward: 12.0 Training loss: 54.5844 Explore P: 0.8636\n",
            "Episode: 80 Total reward: 18.0 Training loss: 104.7400 Explore P: 0.8621\n",
            "Episode: 81 Total reward: 14.0 Training loss: 32.3186 Explore P: 0.8609\n",
            "Episode: 82 Total reward: 32.0 Training loss: 52.8981 Explore P: 0.8582\n",
            "Episode: 83 Total reward: 22.0 Training loss: 130.4665 Explore P: 0.8563\n",
            "Episode: 84 Total reward: 21.0 Training loss: 40.7504 Explore P: 0.8546\n",
            "Episode: 85 Total reward: 15.0 Training loss: 61.0801 Explore P: 0.8533\n",
            "Episode: 86 Total reward: 21.0 Training loss: 16.6459 Explore P: 0.8515\n",
            "Episode: 87 Total reward: 15.0 Training loss: 44.5080 Explore P: 0.8503\n",
            "Episode: 88 Total reward: 12.0 Training loss: 73.9039 Explore P: 0.8492\n",
            "Episode: 89 Total reward: 8.0 Training loss: 96.2312 Explore P: 0.8486\n",
            "Episode: 90 Total reward: 9.0 Training loss: 13.0917 Explore P: 0.8478\n",
            "Episode: 91 Total reward: 26.0 Training loss: 71.5490 Explore P: 0.8456\n",
            "Episode: 92 Total reward: 13.0 Training loss: 54.5336 Explore P: 0.8446\n",
            "Episode: 93 Total reward: 11.0 Training loss: 98.8805 Explore P: 0.8436\n",
            "Episode: 94 Total reward: 15.0 Training loss: 43.3911 Explore P: 0.8424\n",
            "Episode: 95 Total reward: 58.0 Training loss: 89.4039 Explore P: 0.8376\n",
            "Episode: 96 Total reward: 12.0 Training loss: 14.1275 Explore P: 0.8366\n",
            "Episode: 97 Total reward: 13.0 Training loss: 108.8643 Explore P: 0.8355\n",
            "Episode: 98 Total reward: 19.0 Training loss: 11.2162 Explore P: 0.8339\n",
            "Episode: 99 Total reward: 13.0 Training loss: 69.3505 Explore P: 0.8329\n",
            "Episode: 100 Total reward: 13.0 Training loss: 41.6780 Explore P: 0.8318\n",
            "Episode: 101 Total reward: 20.0 Training loss: 160.2904 Explore P: 0.8302\n",
            "Episode: 102 Total reward: 29.0 Training loss: 12.9990 Explore P: 0.8278\n",
            "Episode: 103 Total reward: 18.0 Training loss: 15.7034 Explore P: 0.8263\n",
            "Episode: 104 Total reward: 15.0 Training loss: 15.5320 Explore P: 0.8251\n",
            "Episode: 105 Total reward: 16.0 Training loss: 15.9746 Explore P: 0.8238\n",
            "Episode: 106 Total reward: 12.0 Training loss: 81.7001 Explore P: 0.8228\n",
            "Episode: 107 Total reward: 23.0 Training loss: 198.6173 Explore P: 0.8209\n",
            "Episode: 108 Total reward: 10.0 Training loss: 107.2417 Explore P: 0.8201\n",
            "Episode: 109 Total reward: 13.0 Training loss: 84.6466 Explore P: 0.8191\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}