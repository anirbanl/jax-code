{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jax_qlearning_cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOi+gGuJVWHYE7pQsnaOP5g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anirbanl/jax-code/blob/master/rlflax/jax_qlearning_cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_jS9qg6EG4l",
        "outputId": "801c16aa-61ad-4746-e872-b2c0ef8b2962"
      },
      "source": [
        "!pip install jax jaxlib flax"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (0.2.13)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (0.1.66+cuda110)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.7/dist-packages (0.3.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax) (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from jax) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.4.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.12)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax) (3.2.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax) (1.0.2)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (from flax) (0.0.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (1.3.1)\n",
            "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax->flax) (0.0.7)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.1.6)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.11.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt1r4--nEPJm"
      },
      "source": [
        "import gym\n",
        "gym.logger.set_level(40) # suppress warnings (please remove if gives error)\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import jax\n",
        "import jax.numpy as jp\n",
        "from jax.ops import index, index_add, index_update\n",
        "from jax import jit, grad, vmap, random, jacrev, jacobian, jacfwd, value_and_grad\n",
        "from functools import partial\n",
        "from jax.tree_util import tree_multimap  # Element-wise manipulation of collections of numpy arrays\n",
        "from flax import linen as nn           # The Linen API\n",
        "from flax.training import train_state  # Useful dataclass to keep train state\n",
        "import optax                           # Optimizers\n",
        "from typing import Sequence"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK9XAT81EWW0"
      },
      "source": [
        "from collections import deque\n",
        "class Memory():\n",
        "    def __init__(self, max_size = 1000):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "    \n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "            \n",
        "    def sample(self, batch_size):\n",
        "        idx = np.random.choice(np.arange(len(self.buffer)), \n",
        "                               size=batch_size, \n",
        "                               replace=False)\n",
        "        return [self.buffer[ii] for ii in idx]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtUX4AXiFWVK"
      },
      "source": [
        "train_episodes = 1000          # max number of episodes to learn from\n",
        "max_steps = 200                # max steps in an episode\n",
        "gamma = 0.99                   # future reward discount\n",
        "\n",
        "# Exploration parameters\n",
        "explore_start = 1.0            # exploration probability at start\n",
        "explore_stop = 0.01            # minimum exploration probability \n",
        "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
        "\n",
        "# Network parameters\n",
        "hidden_size = 64               # number of units in each Q-network hidden layer\n",
        "learning_rate = 1e-3         # Q-network learning rate\n",
        "\n",
        "# Memory parameters\n",
        "memory_size = 10000            # memory capacity\n",
        "batch_size = 20                # experience mini-batch size\n",
        "pretrain_length = batch_size   # number experiences to pretrain the memory"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSJfnThHFouD"
      },
      "source": [
        "#Define Q-network\n",
        "class QNetwork:\n",
        "    def __init__(self, rng, env, learning_rate=0.01, state_size=4, \n",
        "                 action_size=2, hidden_size=10, \n",
        "                 name='QNetwork'):\n",
        "        self.key = rng\n",
        "        self.env = env\n",
        "\n",
        "        class Model(nn.Module):\n",
        "            features: Sequence[int]\n",
        "\n",
        "            @nn.compact\n",
        "            def __call__(self, x):\n",
        "                x = nn.relu(nn.Dense(self.features[0])(x))\n",
        "                x = nn.relu(nn.Dense(self.features[1])(x))\n",
        "                x = nn.Dense(self.features[2])(x)\n",
        "                return x\n",
        "\n",
        "        def create_train_state(rng, learning_rate, s_size, h_size, a_size):\n",
        "            \"\"\"Creates initial `TrainState`.\"\"\"\n",
        "            model = Model(features=[hidden_size, hidden_size, a_size])\n",
        "            params = model.init(rng, jp.ones((s_size, )))#['params']\n",
        "            tx = optax.adam(learning_rate)\n",
        "            return train_state.TrainState.create(\n",
        "                apply_fn=model.apply, params=params, tx=tx)\n",
        "\n",
        "        self.ts = create_train_state(rng, learning_rate, state_size, hidden_size, action_size)\n",
        "\n",
        "        @jit\n",
        "        def train_step(ts, inputs, actions, targets):\n",
        "\n",
        "            def loss_fun(params, inputs, actions, targets):\n",
        "                output = ts.apply_fn(params, inputs)\n",
        "                selectedq = jp.sum(actions*output, axis=-1)\n",
        "                return jp.mean(jp.square(selectedq - targets))\n",
        "\n",
        "            loss, g = value_and_grad(loss_fun)(ts.params, inputs, actions, targets)\n",
        "            return ts.apply_gradients(grads=g), loss\n",
        "\n",
        "        self.train_fn = train_step\n",
        "\n",
        "\n",
        "    def act(self, state, explore_p):\n",
        "        self.key, _ = jax.random.split(self.key)\n",
        "        uf = jax.random.uniform(self.key, (1,), minval=0.0, maxval=1.0)[0]\n",
        "        if explore_p > uf:\n",
        "            # Make a random action\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            # Get action from Q-network\n",
        "            qvalues = self.ts.apply_fn(self.ts.params, state)\n",
        "            action = jp.argmax(qvalues).item()\n",
        "        return action\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0fPvqRcLdf4"
      },
      "source": [
        "def init_memory(env):\n",
        "    # Initialize the simulation\n",
        "    env.reset()\n",
        "    # Take one random step to get the pole and cart moving\n",
        "    state, reward, done, _ = env.step(env.action_space.sample())\n",
        "\n",
        "    memory = Memory(max_size=memory_size)\n",
        "\n",
        "    # Make a bunch of random actions and store the experiences\n",
        "    for ii in range(pretrain_length):\n",
        "        # Uncomment the line below to watch the simulation\n",
        "        # env.render()\n",
        "\n",
        "        # Make a random action\n",
        "        action = env.action_space.sample()\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        if done:\n",
        "            # The simulation fails so no next state\n",
        "            next_state = jp.zeros(state.shape)\n",
        "            # Add experience to memory\n",
        "            memory.add((state, action, reward, next_state))\n",
        "            \n",
        "            # Start new episode\n",
        "            env.reset()\n",
        "            # Take one random step to get the pole and cart moving\n",
        "            state, reward, done, _ = env.step(env.action_space.sample())\n",
        "        else:\n",
        "            # Add experience to memory\n",
        "            memory.add((state, action, reward, next_state))\n",
        "            state = next_state\n",
        "    return memory, state"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vpr3LNUMkaG"
      },
      "source": [
        "# Now train with experiences\n",
        "def one_hot(x, k, dtype=jp.float32):\n",
        "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
        "  return jp.array(x[:, None] == jp.arange(k), dtype)\n",
        "\n",
        "\n",
        "def train(env, mainQN):\n",
        "    rewards_list = []    \n",
        "    step = 0\n",
        "    memory, state = init_memory(env)\n",
        "    for ep in range(1, train_episodes):\n",
        "        total_reward = 0\n",
        "        t = 0\n",
        "        while t < max_steps:\n",
        "            step += 1\n",
        "            # Uncomment this next line to watch the training\n",
        "            # env.render() \n",
        "            \n",
        "            # Explore or Exploit\n",
        "            explore_p = explore_stop + (explore_start - explore_stop)*jp.exp(-decay_rate*step) \n",
        "            action = mainQN.act(state, explore_p)\n",
        "            \n",
        "            # Take action, get new state and reward\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            total_reward += reward\n",
        "            \n",
        "            if done:\n",
        "                # the episode ends so no next state\n",
        "                next_state = jp.zeros(state.shape)\n",
        "                t = max_steps\n",
        "                \n",
        "                print('Episode: {}'.format(ep),\n",
        "                    'Total reward: {}'.format(total_reward),\n",
        "                    'Training loss: {:.4f}'.format(loss),\n",
        "                    'Explore P: {:.4f}'.format(explore_p))\n",
        "                rewards_list.append((ep, total_reward))\n",
        "                \n",
        "                # Add experience to memory\n",
        "                memory.add((state, action, reward, next_state))\n",
        "                \n",
        "                # Start new episode\n",
        "                env.reset()\n",
        "                # Take one random step to get the pole and cart moving\n",
        "                state, reward, done, _ = env.step(env.action_space.sample())\n",
        "\n",
        "            else:\n",
        "                # Add experience to memory\n",
        "                memory.add((state, action, reward, next_state))\n",
        "                state = next_state\n",
        "                t += 1\n",
        "            \n",
        "            # Sample mini-batch from memory\n",
        "            batch = memory.sample(batch_size)\n",
        "            states = jp.array([each[0] for each in batch])\n",
        "            actions = one_hot(jp.array([each[1] for each in batch]), 2)\n",
        "            rewards = jp.array([each[2] for each in batch])\n",
        "            next_states = jp.array([each[3] for each in batch])\n",
        "            \n",
        "            # Train network\n",
        "            target_Qs = mainQN.ts.apply_fn(mainQN.ts.params, next_states)\n",
        "            \n",
        "            # Set target_Qs to 0 for states where episode ends\n",
        "            episode_ends = (next_states == jp.zeros(states[0].shape)).all(axis=1)\n",
        "            new_target_Qs = index_update(target_Qs, index[episode_ends], (0, 0))\n",
        "            target_Qs = new_target_Qs\n",
        "            \n",
        "            targets = rewards + gamma * jp.max(target_Qs, axis=1)\n",
        "            # print(states.shape, targets.shape, targets)\n",
        "            mainQN.ts, loss = mainQN.train_fn(mainQN.ts, states, actions, targets)\n",
        "\n",
        "    return rewards_list\n",
        "\n",
        "def plot_scores(rewards_list):\n",
        "    def running_mean(x, N):\n",
        "        cumsum = jp.cumsum(jp.insert(x, 0, 0)) \n",
        "        return (cumsum[N:] - cumsum[:-N]) / N\n",
        "    eps, rews = jp.array(rewards_list).T\n",
        "    smoothed_rews = running_mean(rews, 10)\n",
        "    plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
        "    plt.plot(eps, rews, color='grey', alpha=0.3)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.show()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeurKJ6oNcZF",
        "outputId": "dc3380a9-3193-43f2-8f78-b87be2161234"
      },
      "source": [
        "def main():\n",
        "    seed = 0\n",
        "    env = gym.make('CartPole-v0')\n",
        "    env.seed(seed)\n",
        "    print('observation space:', env.observation_space)\n",
        "    print('action space:', env.action_space.n)\n",
        "    rng = jax.random.PRNGKey(seed)\n",
        "    np.random.seed(seed)\n",
        "    mainQN = QNetwork(rng, env, name='main', hidden_size=hidden_size, learning_rate=learning_rate)\n",
        "    rewards_list = train(env, mainQN)\n",
        "    plot_scores(rewards_list)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
            "action space: 2\n",
            "Episode: 1 Total reward: 31.0 Training loss: 2.7614 Explore P: 0.9969\n",
            "Episode: 2 Total reward: 27.0 Training loss: 3.5193 Explore P: 0.9943\n",
            "Episode: 3 Total reward: 13.0 Training loss: 5.1785 Explore P: 0.9930\n",
            "Episode: 4 Total reward: 17.0 Training loss: 40.9322 Explore P: 0.9913\n",
            "Episode: 5 Total reward: 18.0 Training loss: 24.1495 Explore P: 0.9896\n",
            "Episode: 6 Total reward: 15.0 Training loss: 111.4834 Explore P: 0.9881\n",
            "Episode: 7 Total reward: 13.0 Training loss: 48.3164 Explore P: 0.9868\n",
            "Episode: 8 Total reward: 14.0 Training loss: 557.9758 Explore P: 0.9855\n",
            "Episode: 9 Total reward: 15.0 Training loss: 85.0130 Explore P: 0.9840\n",
            "Episode: 10 Total reward: 30.0 Training loss: 90.3582 Explore P: 0.9811\n",
            "Episode: 11 Total reward: 17.0 Training loss: 139.7248 Explore P: 0.9794\n",
            "Episode: 12 Total reward: 78.0 Training loss: 3594.9031 Explore P: 0.9719\n",
            "Episode: 13 Total reward: 37.0 Training loss: 3253.4463 Explore P: 0.9683\n",
            "Episode: 14 Total reward: 23.0 Training loss: 369.0736 Explore P: 0.9661\n",
            "Episode: 15 Total reward: 28.0 Training loss: 430.6585 Explore P: 0.9635\n",
            "Episode: 16 Total reward: 15.0 Training loss: 8282.0098 Explore P: 0.9620\n",
            "Episode: 17 Total reward: 56.0 Training loss: 389.7590 Explore P: 0.9567\n",
            "Episode: 18 Total reward: 24.0 Training loss: 3014.8706 Explore P: 0.9545\n",
            "Episode: 19 Total reward: 26.0 Training loss: 3629.5938 Explore P: 0.9520\n",
            "Episode: 20 Total reward: 71.0 Training loss: 10020.9912 Explore P: 0.9453\n",
            "Episode: 21 Total reward: 26.0 Training loss: 6711.7769 Explore P: 0.9429\n",
            "Episode: 22 Total reward: 13.0 Training loss: 3912.8208 Explore P: 0.9417\n",
            "Episode: 23 Total reward: 63.0 Training loss: 4262.4155 Explore P: 0.9358\n",
            "Episode: 24 Total reward: 32.0 Training loss: 87.5999 Explore P: 0.9329\n",
            "Episode: 25 Total reward: 11.0 Training loss: 51.9493 Explore P: 0.9319\n",
            "Episode: 26 Total reward: 13.0 Training loss: 3967.1289 Explore P: 0.9307\n",
            "Episode: 27 Total reward: 17.0 Training loss: 15682.3975 Explore P: 0.9291\n",
            "Episode: 28 Total reward: 9.0 Training loss: 18082.9805 Explore P: 0.9283\n",
            "Episode: 29 Total reward: 16.0 Training loss: 3714.4255 Explore P: 0.9268\n",
            "Episode: 30 Total reward: 23.0 Training loss: 50.2039 Explore P: 0.9247\n",
            "Episode: 31 Total reward: 28.0 Training loss: 2617.3965 Explore P: 0.9222\n",
            "Episode: 32 Total reward: 14.0 Training loss: 4749.8931 Explore P: 0.9209\n",
            "Episode: 33 Total reward: 13.0 Training loss: 2590.3586 Explore P: 0.9197\n",
            "Episode: 34 Total reward: 18.0 Training loss: 24.0344 Explore P: 0.9181\n",
            "Episode: 35 Total reward: 14.0 Training loss: 1941.3676 Explore P: 0.9168\n",
            "Episode: 36 Total reward: 39.0 Training loss: 24.1899 Explore P: 0.9133\n",
            "Episode: 37 Total reward: 14.0 Training loss: 1320.8619 Explore P: 0.9120\n",
            "Episode: 38 Total reward: 22.0 Training loss: 3368.7927 Explore P: 0.9100\n",
            "Episode: 39 Total reward: 16.0 Training loss: 1668.4530 Explore P: 0.9086\n",
            "Episode: 40 Total reward: 23.0 Training loss: 21.8816 Explore P: 0.9065\n",
            "Episode: 41 Total reward: 27.0 Training loss: 23.0139 Explore P: 0.9041\n",
            "Episode: 42 Total reward: 27.0 Training loss: 22.3143 Explore P: 0.9017\n",
            "Episode: 43 Total reward: 16.0 Training loss: 1579.3009 Explore P: 0.9003\n",
            "Episode: 44 Total reward: 11.0 Training loss: 4380.3940 Explore P: 0.8993\n",
            "Episode: 45 Total reward: 13.0 Training loss: 33.2522 Explore P: 0.8981\n",
            "Episode: 46 Total reward: 19.0 Training loss: 455.5243 Explore P: 0.8964\n",
            "Episode: 47 Total reward: 18.0 Training loss: 585.2682 Explore P: 0.8948\n",
            "Episode: 48 Total reward: 9.0 Training loss: 1077.7172 Explore P: 0.8940\n",
            "Episode: 49 Total reward: 13.0 Training loss: 738.9522 Explore P: 0.8929\n",
            "Episode: 50 Total reward: 17.0 Training loss: 2032.0758 Explore P: 0.8914\n",
            "Episode: 51 Total reward: 17.0 Training loss: 1981.9211 Explore P: 0.8899\n",
            "Episode: 52 Total reward: 12.0 Training loss: 582.6178 Explore P: 0.8888\n",
            "Episode: 53 Total reward: 19.0 Training loss: 34.4283 Explore P: 0.8872\n",
            "Episode: 54 Total reward: 21.0 Training loss: 31.3082 Explore P: 0.8853\n",
            "Episode: 55 Total reward: 21.0 Training loss: 30.0059 Explore P: 0.8835\n",
            "Episode: 56 Total reward: 23.0 Training loss: 20.4890 Explore P: 0.8815\n",
            "Episode: 57 Total reward: 14.0 Training loss: 332.3423 Explore P: 0.8803\n",
            "Episode: 58 Total reward: 16.0 Training loss: 2361.9500 Explore P: 0.8789\n",
            "Episode: 59 Total reward: 12.0 Training loss: 255.1911 Explore P: 0.8778\n",
            "Episode: 60 Total reward: 8.0 Training loss: 1587.9553 Explore P: 0.8771\n",
            "Episode: 61 Total reward: 26.0 Training loss: 408.6234 Explore P: 0.8749\n",
            "Episode: 62 Total reward: 14.0 Training loss: 1324.0087 Explore P: 0.8737\n",
            "Episode: 63 Total reward: 16.0 Training loss: 20.3402 Explore P: 0.8723\n",
            "Episode: 64 Total reward: 8.0 Training loss: 268.9499 Explore P: 0.8716\n",
            "Episode: 65 Total reward: 19.0 Training loss: 1264.1813 Explore P: 0.8700\n",
            "Episode: 66 Total reward: 24.0 Training loss: 1179.2334 Explore P: 0.8679\n",
            "Episode: 67 Total reward: 38.0 Training loss: 1678.4104 Explore P: 0.8647\n",
            "Episode: 68 Total reward: 28.0 Training loss: 613.0130 Explore P: 0.8623\n",
            "Episode: 69 Total reward: 15.0 Training loss: 1577.6133 Explore P: 0.8610\n",
            "Episode: 70 Total reward: 62.0 Training loss: 275.5693 Explore P: 0.8557\n",
            "Episode: 71 Total reward: 44.0 Training loss: 263.1360 Explore P: 0.8520\n",
            "Episode: 72 Total reward: 30.0 Training loss: 6.5583 Explore P: 0.8495\n",
            "Episode: 73 Total reward: 78.0 Training loss: 7.0714 Explore P: 0.8430\n",
            "Episode: 74 Total reward: 31.0 Training loss: 5.2319 Explore P: 0.8404\n",
            "Episode: 75 Total reward: 40.0 Training loss: 157.9536 Explore P: 0.8371\n",
            "Episode: 76 Total reward: 28.0 Training loss: 18.0090 Explore P: 0.8348\n",
            "Episode: 77 Total reward: 18.0 Training loss: 8.3043 Explore P: 0.8333\n",
            "Episode: 78 Total reward: 19.0 Training loss: 7.8613 Explore P: 0.8317\n",
            "Episode: 79 Total reward: 12.0 Training loss: 9.4417 Explore P: 0.8307\n",
            "Episode: 80 Total reward: 17.0 Training loss: 7.3511 Explore P: 0.8293\n",
            "Episode: 81 Total reward: 51.0 Training loss: 408.9456 Explore P: 0.8252\n",
            "Episode: 82 Total reward: 21.0 Training loss: 189.3163 Explore P: 0.8235\n",
            "Episode: 83 Total reward: 12.0 Training loss: 7.9604 Explore P: 0.8225\n",
            "Episode: 84 Total reward: 27.0 Training loss: 8.4887 Explore P: 0.8203\n",
            "Episode: 85 Total reward: 11.0 Training loss: 7.2362 Explore P: 0.8194\n",
            "Episode: 86 Total reward: 31.0 Training loss: 14.5976 Explore P: 0.8169\n",
            "Episode: 87 Total reward: 13.0 Training loss: 54.5421 Explore P: 0.8159\n",
            "Episode: 88 Total reward: 13.0 Training loss: 70.7488 Explore P: 0.8148\n",
            "Episode: 89 Total reward: 13.0 Training loss: 6.8073 Explore P: 0.8138\n",
            "Episode: 90 Total reward: 99.0 Training loss: 231.4224 Explore P: 0.8058\n",
            "Episode: 91 Total reward: 24.0 Training loss: 230.3250 Explore P: 0.8039\n",
            "Episode: 92 Total reward: 18.0 Training loss: 178.3589 Explore P: 0.8025\n",
            "Episode: 93 Total reward: 13.0 Training loss: 15.8643 Explore P: 0.8015\n",
            "Episode: 94 Total reward: 31.0 Training loss: 7.9542 Explore P: 0.7990\n",
            "Episode: 95 Total reward: 40.0 Training loss: 435.2633 Explore P: 0.7959\n",
            "Episode: 96 Total reward: 17.0 Training loss: 14.2882 Explore P: 0.7945\n",
            "Episode: 97 Total reward: 15.0 Training loss: 12.8562 Explore P: 0.7934\n",
            "Episode: 98 Total reward: 13.0 Training loss: 9.8992 Explore P: 0.7924\n",
            "Episode: 99 Total reward: 18.0 Training loss: 15.2463 Explore P: 0.7909\n",
            "Episode: 100 Total reward: 18.0 Training loss: 285.4599 Explore P: 0.7895\n",
            "Episode: 101 Total reward: 48.0 Training loss: 14.5056 Explore P: 0.7858\n",
            "Episode: 102 Total reward: 19.0 Training loss: 65.9190 Explore P: 0.7843\n",
            "Episode: 103 Total reward: 40.0 Training loss: 26.4530 Explore P: 0.7812\n",
            "Episode: 104 Total reward: 45.0 Training loss: 60.8134 Explore P: 0.7778\n",
            "Episode: 105 Total reward: 23.0 Training loss: 177.9929 Explore P: 0.7760\n",
            "Episode: 106 Total reward: 26.0 Training loss: 175.1249 Explore P: 0.7740\n",
            "Episode: 107 Total reward: 22.0 Training loss: 11.8406 Explore P: 0.7723\n",
            "Episode: 108 Total reward: 86.0 Training loss: 95.0821 Explore P: 0.7658\n",
            "Episode: 109 Total reward: 43.0 Training loss: 284.0309 Explore P: 0.7626\n",
            "Episode: 110 Total reward: 39.0 Training loss: 171.5884 Explore P: 0.7596\n",
            "Episode: 111 Total reward: 75.0 Training loss: 10.0666 Explore P: 0.7540\n",
            "Episode: 112 Total reward: 42.0 Training loss: 439.2826 Explore P: 0.7509\n",
            "Episode: 113 Total reward: 27.0 Training loss: 732.3701 Explore P: 0.7489\n",
            "Episode: 114 Total reward: 44.0 Training loss: 708.1288 Explore P: 0.7457\n",
            "Episode: 115 Total reward: 21.0 Training loss: 169.1897 Explore P: 0.7441\n",
            "Episode: 116 Total reward: 30.0 Training loss: 194.8696 Explore P: 0.7419\n",
            "Episode: 117 Total reward: 26.0 Training loss: 126.6687 Explore P: 0.7400\n",
            "Episode: 118 Total reward: 20.0 Training loss: 42.6161 Explore P: 0.7386\n",
            "Episode: 119 Total reward: 11.0 Training loss: 15.0254 Explore P: 0.7378\n",
            "Episode: 120 Total reward: 34.0 Training loss: 69.4528 Explore P: 0.7353\n",
            "Episode: 121 Total reward: 55.0 Training loss: 7.0019 Explore P: 0.7313\n",
            "Episode: 122 Total reward: 79.0 Training loss: 129.6310 Explore P: 0.7257\n",
            "Episode: 123 Total reward: 13.0 Training loss: 10.4904 Explore P: 0.7247\n",
            "Episode: 124 Total reward: 42.0 Training loss: 212.1271 Explore P: 0.7217\n",
            "Episode: 125 Total reward: 28.0 Training loss: 640.9942 Explore P: 0.7197\n",
            "Episode: 126 Total reward: 42.0 Training loss: 6.0410 Explore P: 0.7168\n",
            "Episode: 127 Total reward: 55.0 Training loss: 91.7151 Explore P: 0.7129\n",
            "Episode: 128 Total reward: 18.0 Training loss: 21.3798 Explore P: 0.7116\n",
            "Episode: 129 Total reward: 126.0 Training loss: 185.8486 Explore P: 0.7028\n",
            "Episode: 130 Total reward: 25.0 Training loss: 275.0005 Explore P: 0.7011\n",
            "Episode: 131 Total reward: 32.0 Training loss: 14.1723 Explore P: 0.6989\n",
            "Episode: 132 Total reward: 40.0 Training loss: 86.9794 Explore P: 0.6962\n",
            "Episode: 133 Total reward: 18.0 Training loss: 76.5094 Explore P: 0.6949\n",
            "Episode: 134 Total reward: 104.0 Training loss: 198.5932 Explore P: 0.6878\n",
            "Episode: 135 Total reward: 47.0 Training loss: 360.4689 Explore P: 0.6847\n",
            "Episode: 136 Total reward: 40.0 Training loss: 19.9013 Explore P: 0.6820\n",
            "Episode: 137 Total reward: 54.0 Training loss: 143.0275 Explore P: 0.6783\n",
            "Episode: 138 Total reward: 11.0 Training loss: 753.3068 Explore P: 0.6776\n",
            "Episode: 139 Total reward: 46.0 Training loss: 17.8553 Explore P: 0.6745\n",
            "Episode: 140 Total reward: 63.0 Training loss: 17.7050 Explore P: 0.6704\n",
            "Episode: 141 Total reward: 18.0 Training loss: 824.5234 Explore P: 0.6692\n",
            "Episode: 142 Total reward: 26.0 Training loss: 306.2711 Explore P: 0.6675\n",
            "Episode: 143 Total reward: 98.0 Training loss: 20.9963 Explore P: 0.6611\n",
            "Episode: 144 Total reward: 20.0 Training loss: 643.7728 Explore P: 0.6598\n",
            "Episode: 145 Total reward: 81.0 Training loss: 21.6737 Explore P: 0.6545\n",
            "Episode: 146 Total reward: 46.0 Training loss: 393.8643 Explore P: 0.6516\n",
            "Episode: 147 Total reward: 59.0 Training loss: 12.6844 Explore P: 0.6478\n",
            "Episode: 148 Total reward: 52.0 Training loss: 17.7707 Explore P: 0.6445\n",
            "Episode: 149 Total reward: 19.0 Training loss: 134.4854 Explore P: 0.6433\n",
            "Episode: 150 Total reward: 44.0 Training loss: 233.1161 Explore P: 0.6405\n",
            "Episode: 151 Total reward: 47.0 Training loss: 152.1743 Explore P: 0.6375\n",
            "Episode: 152 Total reward: 24.0 Training loss: 410.9959 Explore P: 0.6360\n",
            "Episode: 153 Total reward: 49.0 Training loss: 15.8380 Explore P: 0.6330\n",
            "Episode: 154 Total reward: 28.0 Training loss: 17.7883 Explore P: 0.6312\n",
            "Episode: 155 Total reward: 54.0 Training loss: 118.0167 Explore P: 0.6279\n",
            "Episode: 156 Total reward: 26.0 Training loss: 16.1133 Explore P: 0.6263\n",
            "Episode: 157 Total reward: 87.0 Training loss: 33.9492 Explore P: 0.6209\n",
            "Episode: 158 Total reward: 45.0 Training loss: 27.5469 Explore P: 0.6182\n",
            "Episode: 159 Total reward: 16.0 Training loss: 455.6692 Explore P: 0.6172\n",
            "Episode: 160 Total reward: 86.0 Training loss: 16.3832 Explore P: 0.6120\n",
            "Episode: 161 Total reward: 30.0 Training loss: 18.3930 Explore P: 0.6102\n",
            "Episode: 162 Total reward: 17.0 Training loss: 939.9792 Explore P: 0.6092\n",
            "Episode: 163 Total reward: 89.0 Training loss: 460.3595 Explore P: 0.6039\n",
            "Episode: 164 Total reward: 74.0 Training loss: 24.7326 Explore P: 0.5995\n",
            "Episode: 165 Total reward: 81.0 Training loss: 11.4063 Explore P: 0.5948\n",
            "Episode: 166 Total reward: 41.0 Training loss: 12.2995 Explore P: 0.5924\n",
            "Episode: 167 Total reward: 41.0 Training loss: 453.1805 Explore P: 0.5900\n",
            "Episode: 168 Total reward: 61.0 Training loss: 758.8215 Explore P: 0.5865\n",
            "Episode: 169 Total reward: 78.0 Training loss: 24.5888 Explore P: 0.5820\n",
            "Episode: 170 Total reward: 35.0 Training loss: 688.7375 Explore P: 0.5800\n",
            "Episode: 171 Total reward: 114.0 Training loss: 8.3604 Explore P: 0.5735\n",
            "Episode: 172 Total reward: 165.0 Training loss: 19.1954 Explore P: 0.5643\n",
            "Episode: 173 Total reward: 35.0 Training loss: 26.9939 Explore P: 0.5624\n",
            "Episode: 174 Total reward: 27.0 Training loss: 10.2925 Explore P: 0.5609\n",
            "Episode: 175 Total reward: 114.0 Training loss: 14.9641 Explore P: 0.5546\n",
            "Episode: 176 Total reward: 30.0 Training loss: 767.4484 Explore P: 0.5530\n",
            "Episode: 177 Total reward: 111.0 Training loss: 28.0243 Explore P: 0.5470\n",
            "Episode: 178 Total reward: 21.0 Training loss: 387.7192 Explore P: 0.5459\n",
            "Episode: 179 Total reward: 32.0 Training loss: 1915.0146 Explore P: 0.5442\n",
            "Episode: 180 Total reward: 149.0 Training loss: 130.8446 Explore P: 0.5363\n",
            "Episode: 181 Total reward: 66.0 Training loss: 1043.8816 Explore P: 0.5328\n",
            "Episode: 182 Total reward: 18.0 Training loss: 34.8512 Explore P: 0.5319\n",
            "Episode: 183 Total reward: 101.0 Training loss: 41.3397 Explore P: 0.5266\n",
            "Episode: 184 Total reward: 129.0 Training loss: 587.2875 Explore P: 0.5200\n",
            "Episode: 185 Total reward: 70.0 Training loss: 16.9439 Explore P: 0.5164\n",
            "Episode: 186 Total reward: 19.0 Training loss: 41.0321 Explore P: 0.5155\n",
            "Episode: 187 Total reward: 78.0 Training loss: 564.3657 Explore P: 0.5116\n",
            "Episode: 188 Total reward: 130.0 Training loss: 897.0719 Explore P: 0.5051\n",
            "Episode: 189 Total reward: 67.0 Training loss: 28.3694 Explore P: 0.5018\n",
            "Episode: 190 Total reward: 113.0 Training loss: 35.9482 Explore P: 0.4962\n",
            "Episode: 191 Total reward: 19.0 Training loss: 14.5402 Explore P: 0.4953\n",
            "Episode: 192 Total reward: 33.0 Training loss: 16.8727 Explore P: 0.4937\n",
            "Episode: 193 Total reward: 127.0 Training loss: 590.4586 Explore P: 0.4876\n",
            "Episode: 194 Total reward: 131.0 Training loss: 22.3187 Explore P: 0.4814\n",
            "Episode: 195 Total reward: 44.0 Training loss: 31.9405 Explore P: 0.4793\n",
            "Episode: 196 Total reward: 93.0 Training loss: 985.2725 Explore P: 0.4750\n",
            "Episode: 197 Total reward: 117.0 Training loss: 41.5005 Explore P: 0.4696\n",
            "Episode: 198 Total reward: 72.0 Training loss: 49.0770 Explore P: 0.4663\n",
            "Episode: 199 Total reward: 132.0 Training loss: 42.3149 Explore P: 0.4603\n",
            "Episode: 200 Total reward: 152.0 Training loss: 43.3599 Explore P: 0.4535\n",
            "Episode: 201 Total reward: 158.0 Training loss: 742.8991 Explore P: 0.4466\n",
            "Episode: 202 Total reward: 199.0 Training loss: 35.6621 Explore P: 0.4379\n",
            "Episode: 203 Total reward: 125.0 Training loss: 1090.2716 Explore P: 0.4326\n",
            "Episode: 204 Total reward: 193.0 Training loss: 980.0817 Explore P: 0.4246\n",
            "Episode: 205 Total reward: 186.0 Training loss: 943.4836 Explore P: 0.4169\n",
            "Episode: 206 Total reward: 156.0 Training loss: 10.8225 Explore P: 0.4106\n",
            "Episode: 207 Total reward: 121.0 Training loss: 21.4613 Explore P: 0.4058\n",
            "Episode: 208 Total reward: 37.0 Training loss: 25.9797 Explore P: 0.4043\n",
            "Episode: 209 Total reward: 88.0 Training loss: 84.8393 Explore P: 0.4009\n",
            "Episode: 210 Total reward: 135.0 Training loss: 26.5555 Explore P: 0.3956\n",
            "Episode: 211 Total reward: 199.0 Training loss: 397.2837 Explore P: 0.3880\n",
            "Episode: 212 Total reward: 199.0 Training loss: 60.3370 Explore P: 0.3806\n",
            "Episode: 213 Total reward: 14.0 Training loss: 685.2001 Explore P: 0.3801\n",
            "Episode: 214 Total reward: 199.0 Training loss: 3640.0898 Explore P: 0.3728\n",
            "Episode: 215 Total reward: 172.0 Training loss: 43.0918 Explore P: 0.3666\n",
            "Episode: 216 Total reward: 199.0 Training loss: 33.3645 Explore P: 0.3596\n",
            "Episode: 217 Total reward: 199.0 Training loss: 806.1400 Explore P: 0.3527\n",
            "Episode: 218 Total reward: 163.0 Training loss: 102.3774 Explore P: 0.3471\n",
            "Episode: 219 Total reward: 199.0 Training loss: 55.0030 Explore P: 0.3405\n",
            "Episode: 220 Total reward: 193.0 Training loss: 144.0423 Explore P: 0.3342\n",
            "Episode: 221 Total reward: 199.0 Training loss: 120.8008 Explore P: 0.3278\n",
            "Episode: 222 Total reward: 199.0 Training loss: 551.3190 Explore P: 0.3215\n",
            "Episode: 223 Total reward: 199.0 Training loss: 5742.1689 Explore P: 0.3154\n",
            "Episode: 224 Total reward: 199.0 Training loss: 98.8175 Explore P: 0.3094\n",
            "Episode: 225 Total reward: 199.0 Training loss: 141.2957 Explore P: 0.3035\n",
            "Episode: 226 Total reward: 199.0 Training loss: 118.6426 Explore P: 0.2977\n",
            "Episode: 227 Total reward: 199.0 Training loss: 119.3993 Explore P: 0.2920\n",
            "Episode: 228 Total reward: 199.0 Training loss: 158.2853 Explore P: 0.2865\n",
            "Episode: 229 Total reward: 199.0 Training loss: 156.5397 Explore P: 0.2810\n",
            "Episode: 230 Total reward: 199.0 Training loss: 6733.8228 Explore P: 0.2757\n",
            "Episode: 231 Total reward: 199.0 Training loss: 109.7313 Explore P: 0.2704\n",
            "Episode: 232 Total reward: 199.0 Training loss: 91.7124 Explore P: 0.2653\n",
            "Episode: 233 Total reward: 199.0 Training loss: 76.2159 Explore P: 0.2603\n",
            "Episode: 234 Total reward: 199.0 Training loss: 6827.2422 Explore P: 0.2554\n",
            "Episode: 235 Total reward: 199.0 Training loss: 120.6661 Explore P: 0.2505\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}