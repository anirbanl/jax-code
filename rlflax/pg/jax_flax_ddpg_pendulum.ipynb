{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jax_flax_ddpg_pendulum.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anirbanl/jax-code/blob/master/rlflax/pg/jax_flax_ddpg_pendulum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTvC7ne54FDq",
        "outputId": "32799866-3f37-43ea-912c-9adea94c3b3d"
      },
      "source": [
        "!pip install jax jaxlib flax"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (0.2.13)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (0.1.66+cuda110)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.7/dist-packages (0.3.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax) (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from jax) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.4.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.12)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (from flax) (0.0.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax) (3.2.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax) (1.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax) (1.15.0)\n",
            "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax->flax) (0.0.8)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (0.10.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.1.6)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.11.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtHJ0L-p4VLk"
      },
      "source": [
        "import gym\n",
        "gym.logger.set_level(40) # suppress warnings (please remove if gives error)\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import jax\n",
        "import jax.numpy as jp\n",
        "from jax.ops import index, index_add, index_update\n",
        "from jax import jit, grad, vmap, random, jacrev, jacobian, jacfwd, value_and_grad\n",
        "from functools import partial\n",
        "from jax.tree_util import tree_multimap  # Element-wise manipulation of collections of numpy arrays\n",
        "from flax import linen as nn           # The Linen API\n",
        "from flax.training import train_state  # Useful dataclass to keep train state\n",
        "import optax                           # Optimizers\n",
        "from typing import Sequence\n",
        "import copy"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RobhKFoq4jeh",
        "outputId": "ba3d4689-a3ae-4cc6-ad3f-1f8a06333729"
      },
      "source": [
        "env = gym.make('Pendulum-v0')\n",
        "env.seed(0)\n",
        "print('observation space:', env.observation_space)\n",
        "print('action space:', env.action_space)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "observation space: Box(-8.0, 8.0, (3,), float32)\n",
            "action space: Box(-2.0, 2.0, (1,), float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvNrvAxP5WgJ"
      },
      "source": [
        "#Normalize Action Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9My-BdM5Uf6"
      },
      "source": [
        "class NormalizedActions(gym.ActionWrapper):\n",
        "\n",
        "    def action(self, action):\n",
        "        low_bound   = self.action_space.low\n",
        "        upper_bound = self.action_space.high\n",
        "        \n",
        "        action = low_bound + (action + 1.0) * 0.5 * (upper_bound - low_bound)\n",
        "        action = np.clip(action, low_bound, upper_bound)\n",
        "        \n",
        "        return action\n",
        "\n",
        "    def reverse_action(self, action):\n",
        "        low_bound   = self.action_space.low\n",
        "        upper_bound = self.action_space.high\n",
        "        \n",
        "        action = 2 * (action - low_bound) / (upper_bound - low_bound) - 1\n",
        "        action = np.clip(action, low_bound, upper_bound)\n",
        "        \n",
        "        return actions"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f5PAxdZ7i8z"
      },
      "source": [
        "#Ornstein-Uhlenbeck process\n",
        "Adding time-correlated noise to the actions taken by the deterministic policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W34HoYC7jbN"
      },
      "source": [
        "class OUNoise(object):\n",
        "    def __init__(self, action_space, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000):\n",
        "        self.mu           = mu\n",
        "        self.theta        = theta\n",
        "        self.sigma        = max_sigma\n",
        "        self.max_sigma    = max_sigma\n",
        "        self.min_sigma    = min_sigma\n",
        "        self.decay_period = decay_period\n",
        "        self.action_dim   = action_space.shape[0]\n",
        "        self.low          = action_space.low\n",
        "        self.high         = action_space.high\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "        self.state = np.ones(self.action_dim) * self.mu\n",
        "        \n",
        "    def evolve_state(self):\n",
        "        x  = self.state\n",
        "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
        "        self.state = x + dx\n",
        "        return self.state\n",
        "    \n",
        "    def get_action(self, action, t=0):\n",
        "        ou_state = self.evolve_state()\n",
        "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
        "        return np.clip(action + ou_state, self.low, self.high)\n",
        "    \n",
        "#https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/ou_strategy.py"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPeYJW5T7vTc"
      },
      "source": [
        "def plot(frame_idx, rewards):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
        "    plt.plot(rewards)\n",
        "    plt.show()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsSb1qqymlXu",
        "outputId": "00196f1e-7f1f-4066-8cd6-bfe8a0a6aedc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "source": [
        "from collections import deque\n",
        "class Memory():\n",
        "    def __init__(self, rng, max_size = 1000):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "        self.key = rng\n",
        "    \n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "            \n",
        "    def sample(self, batch_size):\n",
        "        self.key, _ = jax.random.split(self.key)\n",
        "        idx = jax.random.choice(self.key,\n",
        "                               jp.arange(len(self.buffer)), \n",
        "                               shape=(batch_size, ))\n",
        "        return [self.buffer[ii] for ii in idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "'''\n",
        "def init_memory(env, memory_size=1000000):\n",
        "    # Initialize the simulation\n",
        "    env.reset()\n",
        "    # Take one random step to get the pole and cart moving\n",
        "    state, reward, done, _ = env.step(env.action_space.sample())\n",
        "\n",
        "    memory = Memory(max_size=memory_size)\n",
        "\n",
        "    # Make a bunch of random actions and store the experiences\n",
        "    for ii in range(pretrain_length):\n",
        "        # Uncomment the line below to watch the simulation\n",
        "        # env.render()\n",
        "\n",
        "        # Make a random action\n",
        "        action = env.action_space.sample()\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        if done:\n",
        "            # The simulation fails so no next state\n",
        "            next_state = jp.zeros(state.shape)\n",
        "            # Add experience to memory\n",
        "            memory.add((state, action, reward, next_state))\n",
        "            \n",
        "            # Start new episode\n",
        "            env.reset()\n",
        "            # Take one random step to get the pole and cart moving\n",
        "            state, reward, done, _ = env.step(env.action_space.sample())\n",
        "        else:\n",
        "            # Add experience to memory\n",
        "            memory.add((state, action, reward, next_state))\n",
        "            state = next_state\n",
        "    return memory, state\n",
        "'''"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef init_memory(env, memory_size=1000000):\\n    # Initialize the simulation\\n    env.reset()\\n    # Take one random step to get the pole and cart moving\\n    state, reward, done, _ = env.step(env.action_space.sample())\\n\\n    memory = Memory(max_size=memory_size)\\n\\n    # Make a bunch of random actions and store the experiences\\n    for ii in range(pretrain_length):\\n        # Uncomment the line below to watch the simulation\\n        # env.render()\\n\\n        # Make a random action\\n        action = env.action_space.sample()\\n        next_state, reward, done, _ = env.step(action)\\n\\n        if done:\\n            # The simulation fails so no next state\\n            next_state = jp.zeros(state.shape)\\n            # Add experience to memory\\n            memory.add((state, action, reward, next_state))\\n            \\n            # Start new episode\\n            env.reset()\\n            # Take one random step to get the pole and cart moving\\n            state, reward, done, _ = env.step(env.action_space.sample())\\n        else:\\n            # Add experience to memory\\n            memory.add((state, action, reward, next_state))\\n            state = next_state\\n    return memory, state\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VYTwI2G70N7"
      },
      "source": [
        "class Policy:\n",
        "    def __init__(self, rng, s_size=4, a_size=1, hidden_size=256, critic_lr=1e-3, actor_lr=1e-4):\n",
        "        super(Policy, self).__init__()\n",
        "        self.key = rng\n",
        "\n",
        "        class Actor(nn.Module):\n",
        "            features: Sequence[int]\n",
        "\n",
        "            @nn.compact\n",
        "            def __call__(self, x):\n",
        "                x = nn.relu(nn.Dense(self.features[0])(x))\n",
        "                x = nn.relu(nn.Dense(self.features[1])(x))\n",
        "                x = 2*nn.sigmoid(2*nn.Dense(self.features[2])(x))-1\n",
        "                return x\n",
        "\n",
        "        self.actor = Actor(features=[hidden_size, hidden_size, a_size])\n",
        "\n",
        "        class Critic(nn.Module):\n",
        "            features: Sequence[int]\n",
        "\n",
        "            @nn.compact\n",
        "            def __call__(self, x):\n",
        "                x = nn.relu(nn.Dense(self.features[0])(x))\n",
        "                x = nn.relu(nn.Dense(self.features[1])(x))\n",
        "                x = nn.Dense(self.features[2])(x)\n",
        "                return x\n",
        "\n",
        "        self.critic = Critic(features=[hidden_size, hidden_size, 1])\n",
        "\n",
        "        def create_train_state(rng, model, learning_rate, input_size):\n",
        "            \"\"\"Creates initial `TrainState`.\"\"\"\n",
        "            params = model.init(rng, jp.ones((input_size, )))#['params']\n",
        "            tx = optax.adam(learning_rate)\n",
        "            return train_state.TrainState.create(\n",
        "                apply_fn=model.apply, params=params, tx=tx)\n",
        "\n",
        "        self.actor_ts = create_train_state(rng, self.actor, actor_lr, s_size)\n",
        "        self.critic_ts = create_train_state(rng, self.critic, critic_lr, s_size + a_size)\n",
        "\n",
        "        @jit\n",
        "        def train_step(actor_ts, critic_ts, states, actions, targets):\n",
        "\n",
        "            def loss_fn(actor_params, critic_params):\n",
        "                critic_inputs_for_actor = jp.concatenate((states, actor_ts.apply_fn(actor_params, states)), axis=1)\n",
        "                actor_loss = -jp.mean(critic_ts.apply_fn(critic_params, critic_inputs_for_actor))\n",
        "                \n",
        "                critic_inputs_for_critic = jp.concatenate((states, actions), axis=1)\n",
        "                selectedq = critic_ts.apply_fn(critic_params, critic_inputs_for_critic)\n",
        "                diff = selectedq - jax.lax.stop_gradient(targets)\n",
        "                critic_loss = jp.mean(diff**2)\n",
        "\n",
        "                return actor_loss, critic_loss\n",
        "\n",
        "            actor_loss = lambda x: loss_fn(x, critic_ts.params)[0]\n",
        "            critic_loss = lambda y: loss_fn(actor_ts.params, y)[1]\n",
        "            al, ag = value_and_grad(actor_loss)(actor_ts.params)\n",
        "            cl, cg = value_and_grad(critic_loss)(critic_ts.params)\n",
        "            return actor_ts.apply_gradients(grads=ag), critic_ts.apply_gradients(grads=cg), al, cl\n",
        "\n",
        "        self.train_fn = train_step\n",
        "\n",
        "    def act(self, state):\n",
        "        action = self.actor_ts.apply_fn(self.actor_ts.params, state)\n",
        "        return action.item()\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvuYWNEhGcUB"
      },
      "source": [
        "def train(rng, env, policy, ou_noise, n_episodes=100, max_t=500, batch_size = 128, memory_size=1000000, gamma = 0.99, soft_tau=1e-2):\n",
        "    #memory, state = init_memory(env)\n",
        "    memory = Memory(rng, max_size=memory_size)\n",
        "    target_critic_params = copy.deepcopy(policy.critic_ts.params)\n",
        "    target_actor_params = copy.deepcopy(policy.actor_ts.params)\n",
        "    # print(f\"Current critic params:{policy.critic_ts.params}\")\n",
        "    # print(f\"Target critic params:{target_critic_params}\")\n",
        "    # print(f\"Current actor params:{policy.actor_ts.params}\")\n",
        "    # print(f\"Target actor params:{target_actor_params}\")\n",
        "    rewards_list = []\n",
        "\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        ou_noise.reset()\n",
        "        episode_reward = 0\n",
        "        for t in range(max_t):\n",
        "            # import time\n",
        "            action = policy.act(state)\n",
        "            # print(f\"action without ou noise:{action}\")\n",
        "            action = ou_noise.get_action(action, t)\n",
        "            # print(f\"action with ou noise:{action}\")\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            # print(f\"Episode:{i_episode} Step:{t} State:{state} Action:{action} Reward:{reward} Done:{done}\")\n",
        "            \n",
        "            memory.add((state, action, reward, next_state, done))\n",
        "\n",
        "            if len(memory) > batch_size:\n",
        "                # Sample mini-batch from memory\n",
        "                # time.sleep(20)\n",
        "                batch = memory.sample(batch_size)\n",
        "                states = jp.array([each[0] for each in batch])\n",
        "                actions = jp.array([each[1] for each in batch])\n",
        "                rewards = jp.array([each[2] for each in batch])\n",
        "                next_states = jp.array([each[3] for each in batch])\n",
        "\n",
        "                # Train network\n",
        "                target_next_actions = policy.actor_ts.apply_fn(target_actor_params, next_states)\n",
        "                critic_inputs_for_critic = jp.concatenate((next_states, target_next_actions), axis=1)\n",
        "                target_Qs = policy.critic_ts.apply_fn(target_critic_params, critic_inputs_for_critic)\n",
        "\n",
        "                # Set target_Qs to 0 for states where episode ends\n",
        "                episode_ends = (next_states == jp.zeros(states[0].shape)).all(axis=1)\n",
        "                new_target_Qs = index_update(target_Qs, index[episode_ends], 0)\n",
        "                target_Qs = new_target_Qs\n",
        "                \n",
        "                targets = rewards + gamma * target_Qs\n",
        "\n",
        "                # print(f\"Current critic params:{policy.critic_ts.params}\")\n",
        "                # print(f\"Target critic params:{target_critic_params}\")\n",
        "                # print(f\"Current actor params:{policy.actor_ts.params}\")\n",
        "                # print(f\"Target actor params:{target_actor_params}\")\n",
        "                policy.actor_ts, policy.critic_ts, al, cl = policy.train_fn(policy.actor_ts, policy.critic_ts, states, actions, targets)\n",
        "                # Update target network params\n",
        "                update_fn = lambda current, target : soft_tau * current + (1.0 - soft_tau) * target\n",
        "                target_critic_params = copy.deepcopy(jax.tree_multimap(update_fn, \n",
        "                                                                       policy.critic_ts.params,\n",
        "                                                                       target_critic_params))\n",
        "                target_actor_params = copy.deepcopy(jax.tree_multimap(update_fn,\n",
        "                                                                      policy.actor_ts.params,\n",
        "                                                                      target_actor_params))\n",
        "                # print(\"**** UPDATED TARGET ****\")\n",
        "                # print(f\"Current critic params:{policy.critic_ts.params}\")\n",
        "                # print(f\"Target critic params:{target_critic_params}\")\n",
        "                # print(f\"Current actor params:{policy.actor_ts.params}\")\n",
        "                # print(f\"Target actor params:{target_actor_params}\")\n",
        "\n",
        "\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break \n",
        "\n",
        "        rewards_list.append(episode_reward)\n",
        "        print(f\"Episode:{i_episode} Reward:{episode_reward} Average:{np.mean(rewards_list[-10:])}\")\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyVGHEv3o9wm",
        "outputId": "160193e0-bda1-4bb0-d89d-6decc9ecbb53"
      },
      "source": [
        "def main():\n",
        "    #env = NormalizedActions(gym.make(\"Pendulum-v0\").env)\n",
        "    env = NormalizedActions(gym.make(\"Pendulum-v0\"))\n",
        "    env.seed(0)\n",
        "    env.action_space.seed(0)\n",
        "    np.random.seed(0)\n",
        "    ou_noise = OUNoise(env.action_space)\n",
        "\n",
        "    state_dim  = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    hidden_dim = 256\n",
        "\n",
        "    print('observation space:', env.observation_space)\n",
        "    print('action space:', env.action_space)\n",
        "    rng = jax.random.PRNGKey(0)\n",
        "\n",
        "    pi = Policy(rng, state_dim, action_dim, hidden_dim)\n",
        "    scores = train(rng, env, pi, ou_noise)\n",
        "    plot_scores(scores)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "observation space: Box(-8.0, 8.0, (3,), float32)\n",
            "action space: Box(-2.0, 2.0, (1,), float32)\n",
            "Episode:1 Reward:-1566.9500056607076 Average:-1566.9500056607076\n",
            "Episode:2 Reward:-1477.2577702923757 Average:-1522.1038879765415\n",
            "Episode:3 Reward:-1614.7446501774293 Average:-1552.984142043504\n",
            "Episode:4 Reward:-1377.9845718852332 Average:-1509.2342495039363\n",
            "Episode:5 Reward:-1610.2781510766104 Average:-1529.4430298184711\n",
            "Episode:6 Reward:-1508.5267176007364 Average:-1525.9569777821819\n",
            "Episode:7 Reward:-966.1556651684064 Average:-1445.9853616944997\n",
            "Episode:8 Reward:-1614.6203250647993 Average:-1467.0647321157871\n",
            "Episode:9 Reward:-1201.047668118781 Average:-1437.5072805605641\n",
            "Episode:10 Reward:-1591.7333980631565 Average:-1452.9298923108236\n",
            "Episode:11 Reward:-1538.218097529041 Average:-1450.056701497657\n",
            "Episode:12 Reward:-1341.9226603727966 Average:-1436.523190505699\n",
            "Episode:13 Reward:-1603.9006352701397 Average:-1435.4387890149703\n",
            "Episode:14 Reward:-1610.3638134480907 Average:-1458.6767131712556\n",
            "Episode:15 Reward:-1362.9046240155017 Average:-1433.9393604651448\n",
            "Episode:16 Reward:-1338.6791215432813 Average:-1416.9546008593995\n",
            "Episode:17 Reward:-1034.8862740608572 Average:-1423.8276617486445\n",
            "Episode:18 Reward:-1131.473036629631 Average:-1375.5129329051276\n",
            "Episode:19 Reward:-1347.2095119578348 Average:-1390.1291172890333\n",
            "Episode:20 Reward:-1084.1325260222052 Average:-1339.369030084938\n",
            "Episode:21 Reward:-974.5000482285888 Average:-1282.9972251548927\n",
            "Episode:22 Reward:-1229.7595430509475 Average:-1271.7809134227077\n",
            "Episode:23 Reward:-1299.0207177435407 Average:-1241.292921670048\n",
            "Episode:24 Reward:-1251.8117612405133 Average:-1205.43771644929\n",
            "Episode:25 Reward:-1146.9957262363544 Average:-1183.8468266713755\n",
            "Episode:26 Reward:-1059.6352061426287 Average:-1155.94243513131\n",
            "Episode:27 Reward:-1025.4900114675556 Average:-1155.0028088719798\n",
            "Episode:28 Reward:-1301.0748220468925 Average:-1171.9629874137058\n",
            "Episode:29 Reward:-1593.8199578893657 Average:-1196.6240320068591\n",
            "Episode:30 Reward:-1341.4959285236744 Average:-1222.3603722570062\n",
            "Episode:31 Reward:-1418.5394688117303 Average:-1266.7643143153202\n",
            "Episode:32 Reward:-1440.090335729475 Average:-1287.7973935831728\n",
            "Episode:33 Reward:-1453.3026939891004 Average:-1303.2255912077292\n",
            "Episode:34 Reward:-1152.3216415961647 Average:-1293.2765792432942\n",
            "Episode:35 Reward:-1076.2132397152127 Average:-1286.19833059118\n",
            "Episode:36 Reward:-1410.718559257914 Average:-1321.3066659027083\n",
            "Episode:37 Reward:-1316.0413723480547 Average:-1350.3618019907583\n",
            "Episode:38 Reward:-1083.3303869411084 Average:-1328.58735848018\n",
            "Episode:39 Reward:-945.9642324489666 Average:-1263.80178593614\n",
            "Episode:40 Reward:-1358.361404365459 Average:-1265.4883335203185\n",
            "Episode:41 Reward:-1328.9901994443826 Average:-1256.5334065835839\n",
            "Episode:42 Reward:-1240.44353520773 Average:-1236.5687265314095\n",
            "Episode:43 Reward:-1250.1273401085402 Average:-1216.2511911433535\n",
            "Episode:44 Reward:-1468.4780469706548 Average:-1247.8668316808023\n",
            "Episode:45 Reward:-1267.4599327674425 Average:-1266.9915009860251\n",
            "Episode:46 Reward:-1213.128824205439 Average:-1247.2325274807777\n",
            "Episode:47 Reward:-1440.9035866341599 Average:-1259.7187489093883\n",
            "Episode:48 Reward:-1495.580045257282 Average:-1300.9437147410058\n",
            "Episode:49 Reward:-1370.9246840524256 Average:-1343.4397599013514\n",
            "Episode:50 Reward:-1475.013267822459 Average:-1355.1049462470514\n",
            "Episode:51 Reward:-1636.744765373575 Average:-1385.8804028399709\n",
            "Episode:52 Reward:-1569.7824282226586 Average:-1418.8142921414637\n",
            "Episode:53 Reward:-1616.962603352461 Average:-1455.4978184658557\n",
            "Episode:54 Reward:-1613.8776297497416 Average:-1470.0377767437647\n",
            "Episode:55 Reward:-1561.5397492316506 Average:-1499.4457583901853\n",
            "Episode:56 Reward:-1577.7072867813906 Average:-1535.9036046477806\n",
            "Episode:57 Reward:-1483.9710899290951 Average:-1540.210354977274\n",
            "Episode:58 Reward:-1569.9943548998517 Average:-1547.6517859415308\n",
            "Episode:59 Reward:-1527.6364966834572 Average:-1563.322967204634\n",
            "Episode:60 Reward:-1451.5873737091965 Average:-1560.980377793308\n",
            "Episode:61 Reward:-1479.5420494266455 Average:-1545.2601061986147\n",
            "Episode:62 Reward:-1452.4745234316597 Average:-1533.529315719515\n",
            "Episode:63 Reward:-1546.417061418186 Average:-1526.4747615260874\n",
            "Episode:64 Reward:-1521.8725052248647 Average:-1517.2742490735998\n",
            "Episode:65 Reward:-1553.0167369561448 Average:-1516.421947846049\n",
            "Episode:66 Reward:-1506.8926188397616 Average:-1509.3404810518862\n",
            "Episode:67 Reward:-1366.4671732070058 Average:-1497.5900893796772\n",
            "Episode:68 Reward:-1616.8686985950824 Average:-1502.2775237492003\n",
            "Episode:69 Reward:-1547.0636699664105 Average:-1504.2202410774958\n",
            "Episode:70 Reward:-1462.150514815097 Average:-1505.2765551880857\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}