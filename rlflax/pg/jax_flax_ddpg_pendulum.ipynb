{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jax_flax_ddpg_pendulum.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO7BAvEtTP4zSiH2k7FFct1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anirbanl/jax-code/blob/master/rlflax/pg/jax_flax_ddpg_pendulum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTvC7ne54FDq",
        "outputId": "3692adb7-278f-4856-98ce-69b33a765344"
      },
      "source": [
        "!pip install jax jaxlib flax"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (0.2.13)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (0.1.66+cuda110)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.7/dist-packages (0.3.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax) (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from jax) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.4.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from jaxlib) (1.12)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (from flax) (0.0.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax) (3.2.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax) (1.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax) (1.15.0)\n",
            "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax->flax) (0.0.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.8.1)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.1.6)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.11.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtHJ0L-p4VLk"
      },
      "source": [
        "import gym\n",
        "gym.logger.set_level(40) # suppress warnings (please remove if gives error)\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import jax\n",
        "import jax.numpy as jp\n",
        "from jax.ops import index, index_add, index_update\n",
        "from jax import jit, grad, vmap, random, jacrev, jacobian, jacfwd, value_and_grad\n",
        "from functools import partial\n",
        "from jax.tree_util import tree_multimap  # Element-wise manipulation of collections of numpy arrays\n",
        "from flax import linen as nn           # The Linen API\n",
        "from flax.training import train_state  # Useful dataclass to keep train state\n",
        "import optax                           # Optimizers\n",
        "from typing import Sequence\n",
        "import copy"
      ],
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RobhKFoq4jeh",
        "outputId": "2dfac745-ec9f-43a9-b341-0b7ce036ee7e"
      },
      "source": [
        "env = gym.make('Pendulum-v0')\n",
        "env.seed(0)\n",
        "print('observation space:', env.observation_space)\n",
        "print('action space:', env.action_space)"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "observation space: Box(-8.0, 8.0, (3,), float32)\n",
            "action space: Box(-2.0, 2.0, (1,), float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvNrvAxP5WgJ"
      },
      "source": [
        "#Normalize Action Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9My-BdM5Uf6"
      },
      "source": [
        "class NormalizedActions(gym.ActionWrapper):\n",
        "\n",
        "    def action(self, action):\n",
        "        low_bound   = self.action_space.low\n",
        "        upper_bound = self.action_space.high\n",
        "        \n",
        "        action = low_bound + (action + 1.0) * 0.5 * (upper_bound - low_bound)\n",
        "        action = np.clip(action, low_bound, upper_bound)\n",
        "        \n",
        "        return action\n",
        "\n",
        "    def reverse_action(self, action):\n",
        "        low_bound   = self.action_space.low\n",
        "        upper_bound = self.action_space.high\n",
        "        \n",
        "        action = 2 * (action - low_bound) / (upper_bound - low_bound) - 1\n",
        "        action = np.clip(action, low_bound, upper_bound)\n",
        "        \n",
        "        return actions"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f5PAxdZ7i8z"
      },
      "source": [
        "#Ornstein-Uhlenbeck process\n",
        "Adding time-correlated noise to the actions taken by the deterministic policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W34HoYC7jbN"
      },
      "source": [
        "class OUNoise(object):\n",
        "    def __init__(self, action_space, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000):\n",
        "        self.mu           = mu\n",
        "        self.theta        = theta\n",
        "        self.sigma        = max_sigma\n",
        "        self.max_sigma    = max_sigma\n",
        "        self.min_sigma    = min_sigma\n",
        "        self.decay_period = decay_period\n",
        "        self.action_dim   = action_space.shape[0]\n",
        "        self.low          = action_space.low\n",
        "        self.high         = action_space.high\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "        self.state = np.ones(self.action_dim) * self.mu\n",
        "        \n",
        "    def evolve_state(self):\n",
        "        x  = self.state\n",
        "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
        "        self.state = x + dx\n",
        "        return self.state\n",
        "    \n",
        "    def get_action(self, action, t=0):\n",
        "        ou_state = self.evolve_state()\n",
        "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
        "        return np.clip(action + ou_state, self.low, self.high)\n",
        "    \n",
        "#https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/ou_strategy.py"
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPeYJW5T7vTc"
      },
      "source": [
        "def plot(frame_idx, rewards):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
        "    plt.plot(rewards)\n",
        "    plt.show()"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsSb1qqymlXu"
      },
      "source": [
        "from collections import deque\n",
        "class Memory():\n",
        "    def __init__(self, rng, max_size = 1000):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "        self.key = rng\n",
        "    \n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "            \n",
        "    def sample(self, batch_size):\n",
        "        self.key, _ = jax.random.split(self.key)\n",
        "        idx = jax.random.choice(self.key,\n",
        "                               jp.arange(len(self.buffer)), \n",
        "                               shape=(batch_size, ))\n",
        "        return [self.buffer[ii] for ii in idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "def init_memory(env, memory_size=1000000):\n",
        "    # Initialize the simulation\n",
        "    env.reset()\n",
        "    # Take one random step to get the pole and cart moving\n",
        "    state, reward, done, _ = env.step(env.action_space.sample())\n",
        "\n",
        "    memory = Memory(max_size=memory_size)\n",
        "\n",
        "    # Make a bunch of random actions and store the experiences\n",
        "    for ii in range(pretrain_length):\n",
        "        # Uncomment the line below to watch the simulation\n",
        "        # env.render()\n",
        "\n",
        "        # Make a random action\n",
        "        action = env.action_space.sample()\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        if done:\n",
        "            # The simulation fails so no next state\n",
        "            next_state = jp.zeros(state.shape)\n",
        "            # Add experience to memory\n",
        "            memory.add((state, action, reward, next_state))\n",
        "            \n",
        "            # Start new episode\n",
        "            env.reset()\n",
        "            # Take one random step to get the pole and cart moving\n",
        "            state, reward, done, _ = env.step(env.action_space.sample())\n",
        "        else:\n",
        "            # Add experience to memory\n",
        "            memory.add((state, action, reward, next_state))\n",
        "            state = next_state\n",
        "    return memory, state"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VYTwI2G70N7"
      },
      "source": [
        "class Policy:\n",
        "    def __init__(self, rng, s_size=4, a_size=1, hidden_size=256, critic_lr=1e-3, actor_lr=1e-4):\n",
        "        super(Policy, self).__init__()\n",
        "        self.key = rng\n",
        "\n",
        "        class Actor(nn.Module):\n",
        "            features: Sequence[int]\n",
        "\n",
        "            @nn.compact\n",
        "            def __call__(self, x):\n",
        "                x = nn.relu(nn.Dense(self.features[0])(x))\n",
        "                x = nn.relu(nn.Dense(self.features[1])(x))\n",
        "                x = nn.tanh(nn.Dense(self.features[2])(x))\n",
        "                return x\n",
        "\n",
        "        self.actor = Actor(features=[hidden_size, hidden_size, a_size])\n",
        "\n",
        "        class Critic(nn.Module):\n",
        "            features: Sequence[int]\n",
        "\n",
        "            @nn.compact\n",
        "            def __call__(self, x):\n",
        "                x = nn.relu(nn.Dense(self.features[0])(x))\n",
        "                x = nn.relu(nn.Dense(self.features[1])(x))\n",
        "                x = nn.Dense(self.features[2])(x)\n",
        "                return x\n",
        "\n",
        "        self.critic = Critic(features=[hidden_size, hidden_size, 1])\n",
        "\n",
        "        def create_train_state(rng, model, learning_rate, input_size):\n",
        "            \"\"\"Creates initial `TrainState`.\"\"\"\n",
        "            params = model.init(rng, jp.ones((input_size, )))#['params']\n",
        "            tx = optax.adam(learning_rate)\n",
        "            return train_state.TrainState.create(\n",
        "                apply_fn=model.apply, params=params, tx=tx)\n",
        "\n",
        "        self.actor_ts = create_train_state(rng, self.actor, actor_lr, s_size)\n",
        "        self.critic_ts = create_train_state(rng, self.critic, critic_lr, s_size + a_size)\n",
        "\n",
        "        @jit\n",
        "        def train_step(actor_ts, critic_ts, states, actions, targets):\n",
        "\n",
        "            def loss_fn(actor_params, critic_params):\n",
        "                critic_inputs_for_actor = jp.concatenate((states, actor_ts.apply_fn(actor_params, states)), axis=1)\n",
        "                actor_loss = -jp.mean(critic_ts.apply_fn(critic_params, critic_inputs_for_actor))\n",
        "                \n",
        "                critic_inputs_for_critic = jp.concatenate((states, actions), axis=1)\n",
        "                selectedq = critic_ts.apply_fn(critic_params, critic_inputs_for_critic)\n",
        "                diff = selectedq - jax.lax.stop_gradient(targets)\n",
        "                critic_loss = jp.mean(diff**2)\n",
        "\n",
        "                return actor_loss, critic_loss\n",
        "\n",
        "            actor_loss = lambda x: loss_fn(x, critic_ts.params)[0]\n",
        "            critic_loss = lambda y: loss_fn(actor_ts.params, y)[1]\n",
        "            al, ag = value_and_grad(actor_loss)(actor_ts.params)\n",
        "            cl, cg = value_and_grad(critic_loss)(critic_ts.params)\n",
        "            return actor_ts.apply_gradients(grads=ag), critic_ts.apply_gradients(grads=cg), al, cl\n",
        "\n",
        "        self.train_fn = train_step\n",
        "\n",
        "    def act(self, state):\n",
        "        action = self.actor_ts.apply_fn(self.actor_ts.params, state)\n",
        "        return action.item()\n"
      ],
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvuYWNEhGcUB"
      },
      "source": [
        "def train(rng, env, policy, ou_noise, n_episodes=100, max_t=1000, batch_size = 128, memory_size=1000000, print_every=1, gamma = 0.99, soft_tau=1e-2):\n",
        "    #memory, state = init_memory(env)\n",
        "    memory = Memory(rng, max_size=memory_size)\n",
        "    target_critic_params = copy.deepcopy(policy.critic_ts.params)\n",
        "    target_actor_params = copy.deepcopy(policy.actor_ts.params)\n",
        "\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        ou_noise.reset()\n",
        "        episode_rewards = []\n",
        "        for t in range(max_t):\n",
        "            # print(f\"Episode:{i_episode} Step:{t}\")\n",
        "            action = policy.act(state)\n",
        "            # print(f\"action without ou noise:{action}\")\n",
        "            action = ou_noise.get_action(action, t)\n",
        "            # print(f\"action with ou noise:{action}\")\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            memory.add((state, action, reward, next_state, done))\n",
        "\n",
        "            if len(memory) > batch_size:\n",
        "                # Sample mini-batch from memory\n",
        "                batch = memory.sample(batch_size)\n",
        "                states = jp.array([each[0] for each in batch])\n",
        "                actions = jp.array([each[1] for each in batch])\n",
        "                rewards = jp.array([each[2] for each in batch])\n",
        "                next_states = jp.array([each[3] for each in batch])\n",
        "\n",
        "                # Train network\n",
        "                target_next_actions = policy.actor_ts.apply_fn(target_actor_params, next_states)\n",
        "                critic_inputs_for_critic = jp.concatenate((next_states, target_next_actions), axis=1)\n",
        "                target_Qs = policy.critic_ts.apply_fn(target_critic_params, critic_inputs_for_critic)\n",
        "\n",
        "                # Set target_Qs to 0 for states where episode ends\n",
        "                episode_ends = (next_states == jp.zeros(states[0].shape)).all(axis=1)\n",
        "                new_target_Qs = index_update(target_Qs, index[episode_ends], 0)\n",
        "                target_Qs = new_target_Qs\n",
        "                \n",
        "                targets = rewards + gamma * target_Qs\n",
        "\n",
        "                policy.actor_ts, policy.critic_ts, al, cl = policy.train_fn(policy.actor_ts, policy.critic_ts, states, actions, targets)\n",
        "                # Update target network params\n",
        "                update_fn = lambda current, target : soft_tau * current + (1.0 - soft_tau) * target\n",
        "                target_critic_params = copy.deepcopy(jax.tree_multimap(update_fn, \n",
        "                                                                       policy.critic_ts.params,\n",
        "                                                                       target_critic_params))\n",
        "                target_actor_params = copy.deepcopy(jax.tree_multimap(update_fn,\n",
        "                                                                      policy.actor_ts.params,\n",
        "                                                                      target_actor_params))\n",
        "\n",
        "\n",
        "            episode_rewards.append(reward)\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break \n",
        "\n",
        "        scores_deque.append(sum(episode_rewards))\n",
        "\n",
        "        if i_episode % print_every == 0:\n",
        "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyVGHEv3o9wm",
        "outputId": "a6005471-1092-49f5-bd18-49d4ab5168cd"
      },
      "source": [
        "def main():\n",
        "    env = NormalizedActions(gym.make(\"Pendulum-v0\"))\n",
        "    env.seed(0)\n",
        "    env.action_space.seed(0)\n",
        "    ou_noise = OUNoise(env.action_space)\n",
        "\n",
        "    state_dim  = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    hidden_dim = 256\n",
        "\n",
        "    print('observation space:', env.observation_space)\n",
        "    print('action space:', env.action_space)\n",
        "    rng = jax.random.PRNGKey(0)\n",
        "\n",
        "    pi = Policy(rng, state_dim, action_dim, hidden_dim)\n",
        "    scores = train(rng, env, pi, ou_noise)\n",
        "    plot_scores(scores)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "observation space: Box(-8.0, 8.0, (3,), float32)\n",
            "action space: Box(-2.0, 2.0, (1,), float32)\n",
            "Episode 1\tAverage Score: -1389.48\n",
            "Episode 2\tAverage Score: -1325.56\n",
            "Episode 3\tAverage Score: -1409.93\n",
            "Episode 4\tAverage Score: -1402.17\n",
            "Episode 5\tAverage Score: -1435.47\n",
            "Episode 6\tAverage Score: -1454.26\n",
            "Episode 7\tAverage Score: -1455.44\n",
            "Episode 8\tAverage Score: -1469.93\n",
            "Episode 9\tAverage Score: -1452.99\n",
            "Episode 10\tAverage Score: -1470.61\n",
            "Episode 11\tAverage Score: -1483.17\n",
            "Episode 12\tAverage Score: -1481.98\n",
            "Episode 13\tAverage Score: -1491.55\n",
            "Episode 14\tAverage Score: -1487.92\n",
            "Episode 15\tAverage Score: -1469.94\n",
            "Episode 16\tAverage Score: -1456.47\n",
            "Episode 17\tAverage Score: -1438.90\n",
            "Episode 18\tAverage Score: -1422.89\n",
            "Episode 19\tAverage Score: -1417.64\n",
            "Episode 20\tAverage Score: -1403.61\n",
            "Episode 21\tAverage Score: -1390.44\n",
            "Episode 22\tAverage Score: -1379.54\n",
            "Episode 23\tAverage Score: -1385.17\n",
            "Episode 24\tAverage Score: -1383.28\n",
            "Episode 25\tAverage Score: -1391.70\n",
            "Episode 26\tAverage Score: -1397.68\n",
            "Episode 27\tAverage Score: -1385.40\n",
            "Episode 28\tAverage Score: -1375.46\n",
            "Episode 29\tAverage Score: -1367.98\n",
            "Episode 30\tAverage Score: -1367.21\n",
            "Episode 31\tAverage Score: -1365.55\n",
            "Episode 32\tAverage Score: -1365.52\n",
            "Episode 33\tAverage Score: -1368.04\n",
            "Episode 34\tAverage Score: -1371.46\n",
            "Episode 35\tAverage Score: -1371.44\n",
            "Episode 36\tAverage Score: -1371.47\n",
            "Episode 37\tAverage Score: -1375.29\n",
            "Episode 38\tAverage Score: -1368.74\n",
            "Episode 39\tAverage Score: -1367.91\n",
            "Episode 40\tAverage Score: -1367.52\n",
            "Episode 41\tAverage Score: -1371.32\n",
            "Episode 42\tAverage Score: -1374.53\n",
            "Episode 43\tAverage Score: -1377.62\n",
            "Episode 44\tAverage Score: -1379.76\n",
            "Episode 45\tAverage Score: -1378.33\n",
            "Episode 46\tAverage Score: -1378.69\n",
            "Episode 47\tAverage Score: -1380.33\n",
            "Episode 48\tAverage Score: -1383.73\n",
            "Episode 49\tAverage Score: -1381.69\n",
            "Episode 50\tAverage Score: -1378.35\n",
            "Episode 51\tAverage Score: -1380.29\n",
            "Episode 52\tAverage Score: -1383.18\n",
            "Episode 53\tAverage Score: -1381.48\n",
            "Episode 54\tAverage Score: -1385.76\n",
            "Episode 55\tAverage Score: -1383.06\n",
            "Episode 56\tAverage Score: -1383.58\n",
            "Episode 57\tAverage Score: -1383.46\n",
            "Episode 58\tAverage Score: -1385.98\n",
            "Episode 59\tAverage Score: -1389.07\n",
            "Episode 60\tAverage Score: -1391.03\n",
            "Episode 61\tAverage Score: -1391.87\n",
            "Episode 62\tAverage Score: -1392.54\n",
            "Episode 63\tAverage Score: -1396.05\n",
            "Episode 64\tAverage Score: -1401.15\n",
            "Episode 65\tAverage Score: -1405.94\n",
            "Episode 66\tAverage Score: -1409.71\n",
            "Episode 67\tAverage Score: -1411.31\n",
            "Episode 68\tAverage Score: -1416.61\n",
            "Episode 69\tAverage Score: -1422.60\n",
            "Episode 70\tAverage Score: -1427.00\n",
            "Episode 71\tAverage Score: -1425.69\n",
            "Episode 72\tAverage Score: -1425.03\n",
            "Episode 73\tAverage Score: -1430.70\n",
            "Episode 74\tAverage Score: -1433.22\n",
            "Episode 75\tAverage Score: -1435.92\n",
            "Episode 76\tAverage Score: -1431.48\n",
            "Episode 77\tAverage Score: -1436.27\n",
            "Episode 78\tAverage Score: -1439.30\n",
            "Episode 79\tAverage Score: -1440.48\n",
            "Episode 80\tAverage Score: -1442.14\n",
            "Episode 81\tAverage Score: -1446.57\n",
            "Episode 82\tAverage Score: -1448.20\n",
            "Episode 83\tAverage Score: -1452.76\n",
            "Episode 84\tAverage Score: -1451.53\n",
            "Episode 85\tAverage Score: -1454.17\n",
            "Episode 86\tAverage Score: -1456.22\n",
            "Episode 87\tAverage Score: -1460.66\n",
            "Episode 88\tAverage Score: -1464.01\n",
            "Episode 89\tAverage Score: -1465.98\n",
            "Episode 90\tAverage Score: -1468.84\n",
            "Episode 91\tAverage Score: -1470.04\n",
            "Episode 92\tAverage Score: -1472.16\n",
            "Episode 93\tAverage Score: -1475.94\n",
            "Episode 94\tAverage Score: -1476.49\n",
            "Episode 95\tAverage Score: -1471.89\n",
            "Episode 96\tAverage Score: -1476.10\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}