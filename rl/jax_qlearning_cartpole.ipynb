{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jax_qlearning_cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxW3Rc4uTpUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "gym.logger.set_level(40) # suppress warnings (please remove if gives error)\n",
        "import numpy as onp\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from jax import jacobian, lax\n",
        "import jax\n",
        "import jax.numpy as np\n",
        "from jax.ops import index, index_add, index_update\n",
        "from jax import jit, grad, vmap, random, jacrev, jacobian, jacfwd, value_and_grad\n",
        "from functools import partial\n",
        "from jax.experimental import stax # neural network library\n",
        "from jax.experimental.stax import GeneralConv, Conv, ConvTranspose, Dense, MaxPool, Relu, Flatten, LogSoftmax, LeakyRelu, Dropout, Tanh, Sigmoid, BatchNorm, Softmax # neural network layers\n",
        "from jax.nn import softmax, sigmoid\n",
        "from jax.nn.initializers import zeros\n",
        "from jax.experimental import optimizers\n",
        "from jax.tree_util import tree_multimap  # Element-wise manipulation of collections of numpy arrays"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hwH1GvQL2hG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "class Memory():\n",
        "    def __init__(self, max_size = 1000):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "    \n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "            \n",
        "    def sample(self, batch_size):\n",
        "        idx = onp.random.choice(onp.arange(len(self.buffer)), \n",
        "                               size=batch_size, \n",
        "                               replace=False)\n",
        "        return [self.buffer[ii] for ii in idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSsnZSGbLy2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_episodes = 1000          # max number of episodes to learn from\n",
        "max_steps = 200                # max steps in an episode\n",
        "gamma = 0.99                   # future reward discount\n",
        "\n",
        "# Exploration parameters\n",
        "explore_start = 1.0            # exploration probability at start\n",
        "explore_stop = 0.01            # minimum exploration probability \n",
        "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
        "\n",
        "# Network parameters\n",
        "hidden_size = 64               # number of units in each Q-network hidden layer\n",
        "learning_rate = 0.0001         # Q-network learning rate\n",
        "\n",
        "# Memory parameters\n",
        "memory_size = 10000            # memory capacity\n",
        "batch_size = 20                # experience mini-batch size\n",
        "pretrain_length = batch_size   # number experiences to pretrain the memory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy0RC7e8CqTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define Q-network\n",
        "class QNetwork:\n",
        "    def __init__(self, rng, env, learning_rate=0.01, state_size=4, \n",
        "                 action_size=2, hidden_size=10, \n",
        "                 name='QNetwork'):\n",
        "        self.key = rng\n",
        "        self.init_fun, self.apply_fun = stax.serial(\n",
        "            Dense(hidden_size), Relu,\n",
        "            Dense(hidden_size), Relu,\n",
        "            Dense(action_size)\n",
        "        )\n",
        "        self.in_shape = (-1, state_size)\n",
        "        _, self.net_params = self.init_fun(self.key, self.in_shape)\n",
        "        self.opt_init, self.opt_update, self.get_params = optimizers.adam(step_size=learning_rate)\n",
        "        self.opt_state = self.opt_init(self.net_params)\n",
        "        self.env = env\n",
        "        self.loss = np.inf\n",
        "\n",
        "    def loss_fun(self, params, inputs, actions, targets):\n",
        "        output = self.apply_fun(params, inputs)\n",
        "        selectedq = np.sum(actions*output, axis=-1)\n",
        "        # print(inputs.shape, actions.shape, output.shape, selectedq.shape, targets.shape)\n",
        "        return np.mean(np.square(selectedq - targets))\n",
        "\n",
        "    def output(self, inputs):\n",
        "        return self.apply_fun(self.net_params, inputs)\n",
        "\n",
        "    def update_key(self):\n",
        "        self.key, _ = jax.random.split(self.key)\n",
        "        return self.key\n",
        "\n",
        "    def act(self, state, explore_p):\n",
        "        uf = jax.random.uniform(self.update_key(), (1,), minval=0.0, maxval=1.0)[0]\n",
        "        # print(self.key, uf)\n",
        "        if explore_p > uf:\n",
        "            # Make a random action\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            # Get action from Q-network\n",
        "            action = np.argmax(self.output(state)).item()\n",
        "        return action\n",
        "\n",
        "    def step(self, i, inputs, actions, targets):\n",
        "        params = self.get_params(self.opt_state)\n",
        "        self.loss, g = value_and_grad(self.loss_fun)(params, inputs, actions, targets)\n",
        "        self.opt_state = self.opt_update(i, g, self.opt_state)\n",
        "        self.net_params = self.get_params(self.opt_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erkuutwTL9Be",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_memory(env):\n",
        "    # Initialize the simulation\n",
        "    env.reset()\n",
        "    # Take one random step to get the pole and cart moving\n",
        "    state, reward, done, _ = env.step(env.action_space.sample())\n",
        "\n",
        "    memory = Memory(max_size=memory_size)\n",
        "\n",
        "    # Make a bunch of random actions and store the experiences\n",
        "    for ii in range(pretrain_length):\n",
        "        # Uncomment the line below to watch the simulation\n",
        "        # env.render()\n",
        "\n",
        "        # Make a random action\n",
        "        action = env.action_space.sample()\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        if done:\n",
        "            # The simulation fails so no next state\n",
        "            next_state = np.zeros(state.shape)\n",
        "            # Add experience to memory\n",
        "            memory.add((state, action, reward, next_state))\n",
        "            \n",
        "            # Start new episode\n",
        "            env.reset()\n",
        "            # Take one random step to get the pole and cart moving\n",
        "            state, reward, done, _ = env.step(env.action_space.sample())\n",
        "        else:\n",
        "            # Add experience to memory\n",
        "            memory.add((state, action, reward, next_state))\n",
        "            state = next_state\n",
        "    return memory, state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAr-Z17PCfG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now train with experiences\n",
        "def one_hot(x, k, dtype=np.float32):\n",
        "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
        "  return np.array(x[:, None] == np.arange(k), dtype)\n",
        "\n",
        "\n",
        "def train(env, mainQN):\n",
        "    rewards_list = []    \n",
        "    step = 0\n",
        "    memory, state = init_memory(env)\n",
        "    for ep in range(1, train_episodes):\n",
        "        total_reward = 0\n",
        "        t = 0\n",
        "        while t < max_steps:\n",
        "            step += 1\n",
        "            # Uncomment this next line to watch the training\n",
        "            # env.render() \n",
        "            \n",
        "            # Explore or Exploit\n",
        "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step) \n",
        "            action = mainQN.act(state, explore_p)\n",
        "            \n",
        "            # Take action, get new state and reward\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            total_reward += reward\n",
        "            \n",
        "            if done:\n",
        "                # the episode ends so no next state\n",
        "                next_state = np.zeros(state.shape)\n",
        "                t = max_steps\n",
        "                \n",
        "                print('Episode: {}'.format(ep),\n",
        "                    'Total reward: {}'.format(total_reward),\n",
        "                    'Training loss: {:.4f}'.format(loss),\n",
        "                    'Explore P: {:.4f}'.format(explore_p))\n",
        "                rewards_list.append((ep, total_reward))\n",
        "                \n",
        "                # Add experience to memory\n",
        "                memory.add((state, action, reward, next_state))\n",
        "                \n",
        "                # Start new episode\n",
        "                env.reset()\n",
        "                # Take one random step to get the pole and cart moving\n",
        "                state, reward, done, _ = env.step(env.action_space.sample())\n",
        "\n",
        "            else:\n",
        "                # Add experience to memory\n",
        "                memory.add((state, action, reward, next_state))\n",
        "                state = next_state\n",
        "                t += 1\n",
        "            \n",
        "            # Sample mini-batch from memory\n",
        "            batch = memory.sample(batch_size)\n",
        "            states = np.array([each[0] for each in batch])\n",
        "            actions = one_hot(np.array([each[1] for each in batch]), 2)\n",
        "            rewards = np.array([each[2] for each in batch])\n",
        "            next_states = np.array([each[3] for each in batch])\n",
        "            \n",
        "            # Train network\n",
        "            target_Qs = mainQN.output(next_states)\n",
        "            \n",
        "            # Set target_Qs to 0 for states where episode ends\n",
        "            episode_ends = (next_states == np.zeros(states[0].shape)).all(axis=1)\n",
        "            new_target_Qs = index_update(target_Qs, index[episode_ends], (0, 0))\n",
        "            target_Qs = new_target_Qs\n",
        "            \n",
        "            targets = rewards + gamma * np.max(target_Qs, axis=1)\n",
        "            # print(states.shape, targets.shape, targets)\n",
        "            mainQN.step((ep-1)*train_episodes+step-1, states, actions, targets)\n",
        "\n",
        "            loss = mainQN.loss\n",
        "\n",
        "    return rewards_list\n",
        "\n",
        "def plot_scores(rewards_list):\n",
        "    def running_mean(x, N):\n",
        "        cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
        "        return (cumsum[N:] - cumsum[:-N]) / N\n",
        "    eps, rews = np.array(rewards_list).T\n",
        "    smoothed_rews = running_mean(rews, 10)\n",
        "    plt.plot(eps[-len(smoothed_rews):], smoothed_rews)\n",
        "    plt.plot(eps, rews, color='grey', alpha=0.3)\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKDTyxjF7Aj3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1b6b1caa-552e-46da-a1dd-573768d522cb"
      },
      "source": [
        "def main():\n",
        "    seed = 0\n",
        "    env = gym.make('CartPole-v0')\n",
        "    env.seed(seed)\n",
        "    print('observation space:', env.observation_space)\n",
        "    print('action space:', env.action_space.n)\n",
        "    rng = jax.random.PRNGKey(seed)\n",
        "    onp.random.seed(seed)\n",
        "    mainQN = QNetwork(rng, env, name='main', hidden_size=hidden_size, learning_rate=learning_rate)\n",
        "    rewards_list = train(env, mainQN)\n",
        "    plot_scores(rewards_list)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "observation space: Box(4,)\n",
            "action space: 2\n",
            "Episode: 1 Total reward: 29.0 Training loss: 1.0450 Explore P: 0.9971\n",
            "Episode: 2 Total reward: 30.0 Training loss: 1.0738 Explore P: 0.9942\n",
            "Episode: 3 Total reward: 7.0 Training loss: 0.9976 Explore P: 0.9935\n",
            "Episode: 4 Total reward: 18.0 Training loss: 1.1242 Explore P: 0.9917\n",
            "Episode: 5 Total reward: 30.0 Training loss: 1.2784 Explore P: 0.9888\n",
            "Episode: 6 Total reward: 23.0 Training loss: 1.1642 Explore P: 0.9865\n",
            "Episode: 7 Total reward: 11.0 Training loss: 1.2198 Explore P: 0.9855\n",
            "Episode: 8 Total reward: 11.0 Training loss: 1.7343 Explore P: 0.9844\n",
            "Episode: 9 Total reward: 15.0 Training loss: 3.4449 Explore P: 0.9829\n",
            "Episode: 10 Total reward: 43.0 Training loss: 1.9305 Explore P: 0.9787\n",
            "Episode: 11 Total reward: 38.0 Training loss: 2.1615 Explore P: 0.9751\n",
            "Episode: 12 Total reward: 11.0 Training loss: 1.9250 Explore P: 0.9740\n",
            "Episode: 13 Total reward: 52.0 Training loss: 2.7796 Explore P: 0.9690\n",
            "Episode: 14 Total reward: 13.0 Training loss: 3.1416 Explore P: 0.9678\n",
            "Episode: 15 Total reward: 35.0 Training loss: 2.8437 Explore P: 0.9644\n",
            "Episode: 16 Total reward: 14.0 Training loss: 5.5025 Explore P: 0.9631\n",
            "Episode: 17 Total reward: 23.0 Training loss: 4.8509 Explore P: 0.9609\n",
            "Episode: 18 Total reward: 20.0 Training loss: 13.1131 Explore P: 0.9590\n",
            "Episode: 19 Total reward: 9.0 Training loss: 14.4375 Explore P: 0.9581\n",
            "Episode: 20 Total reward: 37.0 Training loss: 24.4290 Explore P: 0.9546\n",
            "Episode: 21 Total reward: 23.0 Training loss: 4.4757 Explore P: 0.9525\n",
            "Episode: 22 Total reward: 13.0 Training loss: 10.3741 Explore P: 0.9512\n",
            "Episode: 23 Total reward: 28.0 Training loss: 16.4753 Explore P: 0.9486\n",
            "Episode: 24 Total reward: 15.0 Training loss: 20.9019 Explore P: 0.9472\n",
            "Episode: 25 Total reward: 13.0 Training loss: 28.1428 Explore P: 0.9460\n",
            "Episode: 26 Total reward: 31.0 Training loss: 39.8858 Explore P: 0.9431\n",
            "Episode: 27 Total reward: 15.0 Training loss: 8.2056 Explore P: 0.9417\n",
            "Episode: 28 Total reward: 27.0 Training loss: 89.3975 Explore P: 0.9392\n",
            "Episode: 29 Total reward: 20.0 Training loss: 40.4180 Explore P: 0.9373\n",
            "Episode: 30 Total reward: 30.0 Training loss: 7.7751 Explore P: 0.9345\n",
            "Episode: 31 Total reward: 22.0 Training loss: 38.8394 Explore P: 0.9325\n",
            "Episode: 32 Total reward: 33.0 Training loss: 117.4630 Explore P: 0.9295\n",
            "Episode: 33 Total reward: 31.0 Training loss: 28.6960 Explore P: 0.9266\n",
            "Episode: 34 Total reward: 10.0 Training loss: 54.9156 Explore P: 0.9257\n",
            "Episode: 35 Total reward: 24.0 Training loss: 12.9062 Explore P: 0.9235\n",
            "Episode: 36 Total reward: 13.0 Training loss: 90.0259 Explore P: 0.9223\n",
            "Episode: 37 Total reward: 16.0 Training loss: 60.9075 Explore P: 0.9209\n",
            "Episode: 38 Total reward: 20.0 Training loss: 159.1671 Explore P: 0.9191\n",
            "Episode: 39 Total reward: 12.0 Training loss: 31.6991 Explore P: 0.9180\n",
            "Episode: 40 Total reward: 14.0 Training loss: 77.6158 Explore P: 0.9167\n",
            "Episode: 41 Total reward: 15.0 Training loss: 11.1833 Explore P: 0.9153\n",
            "Episode: 42 Total reward: 13.0 Training loss: 67.6548 Explore P: 0.9142\n",
            "Episode: 43 Total reward: 20.0 Training loss: 118.2231 Explore P: 0.9124\n",
            "Episode: 44 Total reward: 19.0 Training loss: 185.9958 Explore P: 0.9106\n",
            "Episode: 45 Total reward: 21.0 Training loss: 116.9280 Explore P: 0.9087\n",
            "Episode: 46 Total reward: 11.0 Training loss: 170.4321 Explore P: 0.9078\n",
            "Episode: 47 Total reward: 18.0 Training loss: 83.5017 Explore P: 0.9061\n",
            "Episode: 48 Total reward: 17.0 Training loss: 16.9523 Explore P: 0.9046\n",
            "Episode: 49 Total reward: 51.0 Training loss: 361.8317 Explore P: 0.9001\n",
            "Episode: 50 Total reward: 15.0 Training loss: 173.1757 Explore P: 0.8987\n",
            "Episode: 51 Total reward: 43.0 Training loss: 126.7570 Explore P: 0.8949\n",
            "Episode: 52 Total reward: 15.0 Training loss: 11.2920 Explore P: 0.8936\n",
            "Episode: 53 Total reward: 27.0 Training loss: 12.7169 Explore P: 0.8912\n",
            "Episode: 54 Total reward: 10.0 Training loss: 82.9126 Explore P: 0.8903\n",
            "Episode: 55 Total reward: 8.0 Training loss: 197.1572 Explore P: 0.8896\n",
            "Episode: 56 Total reward: 20.0 Training loss: 9.6282 Explore P: 0.8879\n",
            "Episode: 57 Total reward: 11.0 Training loss: 130.7961 Explore P: 0.8869\n",
            "Episode: 58 Total reward: 9.0 Training loss: 9.9186 Explore P: 0.8861\n",
            "Episode: 59 Total reward: 34.0 Training loss: 13.2697 Explore P: 0.8831\n",
            "Episode: 60 Total reward: 10.0 Training loss: 11.7322 Explore P: 0.8823\n",
            "Episode: 61 Total reward: 17.0 Training loss: 8.1171 Explore P: 0.8808\n",
            "Episode: 62 Total reward: 14.0 Training loss: 235.4362 Explore P: 0.8796\n",
            "Episode: 63 Total reward: 30.0 Training loss: 8.8577 Explore P: 0.8770\n",
            "Episode: 64 Total reward: 22.0 Training loss: 7.1589 Explore P: 0.8751\n",
            "Episode: 65 Total reward: 40.0 Training loss: 89.5322 Explore P: 0.8716\n",
            "Episode: 66 Total reward: 10.0 Training loss: 122.2987 Explore P: 0.8708\n",
            "Episode: 67 Total reward: 11.0 Training loss: 8.4923 Explore P: 0.8698\n",
            "Episode: 68 Total reward: 30.0 Training loss: 138.7038 Explore P: 0.8672\n",
            "Episode: 69 Total reward: 23.0 Training loss: 67.6484 Explore P: 0.8653\n",
            "Episode: 70 Total reward: 44.0 Training loss: 88.1813 Explore P: 0.8615\n",
            "Episode: 71 Total reward: 21.0 Training loss: 228.1780 Explore P: 0.8597\n",
            "Episode: 72 Total reward: 29.0 Training loss: 71.3511 Explore P: 0.8573\n",
            "Episode: 73 Total reward: 11.0 Training loss: 60.7199 Explore P: 0.8563\n",
            "Episode: 74 Total reward: 18.0 Training loss: 6.4984 Explore P: 0.8548\n",
            "Episode: 75 Total reward: 10.0 Training loss: 7.4263 Explore P: 0.8540\n",
            "Episode: 76 Total reward: 12.0 Training loss: 3.5008 Explore P: 0.8529\n",
            "Episode: 77 Total reward: 71.0 Training loss: 3.1238 Explore P: 0.8470\n",
            "Episode: 78 Total reward: 15.0 Training loss: 119.7227 Explore P: 0.8457\n",
            "Episode: 79 Total reward: 39.0 Training loss: 2.7845 Explore P: 0.8425\n",
            "Episode: 80 Total reward: 23.0 Training loss: 62.4263 Explore P: 0.8406\n",
            "Episode: 81 Total reward: 10.0 Training loss: 83.3769 Explore P: 0.8397\n",
            "Episode: 82 Total reward: 14.0 Training loss: 101.9249 Explore P: 0.8386\n",
            "Episode: 83 Total reward: 32.0 Training loss: 3.0785 Explore P: 0.8359\n",
            "Episode: 84 Total reward: 37.0 Training loss: 59.4250 Explore P: 0.8329\n",
            "Episode: 85 Total reward: 23.0 Training loss: 5.7877 Explore P: 0.8310\n",
            "Episode: 86 Total reward: 12.0 Training loss: 74.7884 Explore P: 0.8300\n",
            "Episode: 87 Total reward: 29.0 Training loss: 185.8417 Explore P: 0.8276\n",
            "Episode: 88 Total reward: 15.0 Training loss: 185.1469 Explore P: 0.8264\n",
            "Episode: 89 Total reward: 16.0 Training loss: 1.9747 Explore P: 0.8251\n",
            "Episode: 90 Total reward: 12.0 Training loss: 100.8500 Explore P: 0.8241\n",
            "Episode: 91 Total reward: 14.0 Training loss: 1.9672 Explore P: 0.8230\n",
            "Episode: 92 Total reward: 45.0 Training loss: 2.9261 Explore P: 0.8193\n",
            "Episode: 93 Total reward: 13.0 Training loss: 54.7816 Explore P: 0.8183\n",
            "Episode: 94 Total reward: 14.0 Training loss: 164.3041 Explore P: 0.8171\n",
            "Episode: 95 Total reward: 12.0 Training loss: 2.9559 Explore P: 0.8162\n",
            "Episode: 96 Total reward: 20.0 Training loss: 144.1160 Explore P: 0.8146\n",
            "Episode: 97 Total reward: 13.0 Training loss: 5.3346 Explore P: 0.8135\n",
            "Episode: 98 Total reward: 41.0 Training loss: 188.5283 Explore P: 0.8102\n",
            "Episode: 99 Total reward: 10.0 Training loss: 2.5751 Explore P: 0.8094\n",
            "Episode: 100 Total reward: 14.0 Training loss: 48.9643 Explore P: 0.8083\n",
            "Episode: 101 Total reward: 22.0 Training loss: 79.7882 Explore P: 0.8066\n",
            "Episode: 102 Total reward: 8.0 Training loss: 57.8546 Explore P: 0.8059\n",
            "Episode: 103 Total reward: 11.0 Training loss: 1.6315 Explore P: 0.8050\n",
            "Episode: 104 Total reward: 16.0 Training loss: 55.4129 Explore P: 0.8038\n",
            "Episode: 105 Total reward: 14.0 Training loss: 58.4554 Explore P: 0.8027\n",
            "Episode: 106 Total reward: 32.0 Training loss: 2.2878 Explore P: 0.8001\n",
            "Episode: 107 Total reward: 9.0 Training loss: 90.2070 Explore P: 0.7994\n",
            "Episode: 108 Total reward: 25.0 Training loss: 2.4711 Explore P: 0.7975\n",
            "Episode: 109 Total reward: 31.0 Training loss: 73.0650 Explore P: 0.7950\n",
            "Episode: 110 Total reward: 15.0 Training loss: 1.9052 Explore P: 0.7938\n",
            "Episode: 111 Total reward: 38.0 Training loss: 88.4511 Explore P: 0.7909\n",
            "Episode: 112 Total reward: 20.0 Training loss: 79.5139 Explore P: 0.7893\n",
            "Episode: 113 Total reward: 9.0 Training loss: 1.4415 Explore P: 0.7886\n",
            "Episode: 114 Total reward: 14.0 Training loss: 2.2667 Explore P: 0.7875\n",
            "Episode: 115 Total reward: 11.0 Training loss: 136.3665 Explore P: 0.7867\n",
            "Episode: 116 Total reward: 16.0 Training loss: 1.5402 Explore P: 0.7854\n",
            "Episode: 117 Total reward: 15.0 Training loss: 1.5260 Explore P: 0.7843\n",
            "Episode: 118 Total reward: 17.0 Training loss: 65.2970 Explore P: 0.7829\n",
            "Episode: 119 Total reward: 16.0 Training loss: 2.1961 Explore P: 0.7817\n",
            "Episode: 120 Total reward: 21.0 Training loss: 46.9132 Explore P: 0.7801\n",
            "Episode: 121 Total reward: 10.0 Training loss: 56.2298 Explore P: 0.7793\n",
            "Episode: 122 Total reward: 18.0 Training loss: 234.0879 Explore P: 0.7779\n",
            "Episode: 123 Total reward: 16.0 Training loss: 121.8802 Explore P: 0.7767\n",
            "Episode: 124 Total reward: 16.0 Training loss: 117.5935 Explore P: 0.7755\n",
            "Episode: 125 Total reward: 12.0 Training loss: 1.3632 Explore P: 0.7746\n",
            "Episode: 126 Total reward: 14.0 Training loss: 221.6678 Explore P: 0.7735\n",
            "Episode: 127 Total reward: 26.0 Training loss: 1.2096 Explore P: 0.7715\n",
            "Episode: 128 Total reward: 26.0 Training loss: 1.8243 Explore P: 0.7695\n",
            "Episode: 129 Total reward: 9.0 Training loss: 2.1650 Explore P: 0.7689\n",
            "Episode: 130 Total reward: 41.0 Training loss: 2.0941 Explore P: 0.7657\n",
            "Episode: 131 Total reward: 27.0 Training loss: 0.8729 Explore P: 0.7637\n",
            "Episode: 132 Total reward: 11.0 Training loss: 103.0730 Explore P: 0.7629\n",
            "Episode: 133 Total reward: 13.0 Training loss: 44.3116 Explore P: 0.7619\n",
            "Episode: 134 Total reward: 30.0 Training loss: 38.8895 Explore P: 0.7596\n",
            "Episode: 135 Total reward: 15.0 Training loss: 48.1422 Explore P: 0.7585\n",
            "Episode: 136 Total reward: 12.0 Training loss: 40.6994 Explore P: 0.7576\n",
            "Episode: 137 Total reward: 11.0 Training loss: 45.8494 Explore P: 0.7568\n",
            "Episode: 138 Total reward: 27.0 Training loss: 40.4283 Explore P: 0.7548\n",
            "Episode: 139 Total reward: 16.0 Training loss: 1.8678 Explore P: 0.7536\n",
            "Episode: 140 Total reward: 19.0 Training loss: 45.1753 Explore P: 0.7522\n",
            "Episode: 141 Total reward: 16.0 Training loss: 1.8045 Explore P: 0.7510\n",
            "Episode: 142 Total reward: 27.0 Training loss: 43.5109 Explore P: 0.7490\n",
            "Episode: 143 Total reward: 8.0 Training loss: 44.9731 Explore P: 0.7484\n",
            "Episode: 144 Total reward: 19.0 Training loss: 1.7556 Explore P: 0.7470\n",
            "Episode: 145 Total reward: 17.0 Training loss: 0.9179 Explore P: 0.7458\n",
            "Episode: 146 Total reward: 10.0 Training loss: 40.0204 Explore P: 0.7450\n",
            "Episode: 147 Total reward: 22.0 Training loss: 45.4850 Explore P: 0.7434\n",
            "Episode: 148 Total reward: 16.0 Training loss: 1.6058 Explore P: 0.7422\n",
            "Episode: 149 Total reward: 24.0 Training loss: 39.0323 Explore P: 0.7405\n",
            "Episode: 150 Total reward: 19.0 Training loss: 169.7851 Explore P: 0.7391\n",
            "Episode: 151 Total reward: 10.0 Training loss: 1.5078 Explore P: 0.7384\n",
            "Episode: 152 Total reward: 34.0 Training loss: 1.7241 Explore P: 0.7359\n",
            "Episode: 153 Total reward: 15.0 Training loss: 1.6874 Explore P: 0.7348\n",
            "Episode: 154 Total reward: 11.0 Training loss: 2.2372 Explore P: 0.7340\n",
            "Episode: 155 Total reward: 11.0 Training loss: 1.5545 Explore P: 0.7332\n",
            "Episode: 156 Total reward: 8.0 Training loss: 33.3843 Explore P: 0.7326\n",
            "Episode: 157 Total reward: 12.0 Training loss: 37.1010 Explore P: 0.7318\n",
            "Episode: 158 Total reward: 18.0 Training loss: 37.1811 Explore P: 0.7305\n",
            "Episode: 159 Total reward: 16.0 Training loss: 40.7588 Explore P: 0.7293\n",
            "Episode: 160 Total reward: 13.0 Training loss: 71.2854 Explore P: 0.7284\n",
            "Episode: 161 Total reward: 14.0 Training loss: 41.2224 Explore P: 0.7274\n",
            "Episode: 162 Total reward: 9.0 Training loss: 83.4971 Explore P: 0.7267\n",
            "Episode: 163 Total reward: 14.0 Training loss: 34.2066 Explore P: 0.7257\n",
            "Episode: 164 Total reward: 9.0 Training loss: 75.0669 Explore P: 0.7251\n",
            "Episode: 165 Total reward: 24.0 Training loss: 65.2563 Explore P: 0.7234\n",
            "Episode: 166 Total reward: 15.0 Training loss: 1.5445 Explore P: 0.7223\n",
            "Episode: 167 Total reward: 14.0 Training loss: 0.2450 Explore P: 0.7213\n",
            "Episode: 168 Total reward: 15.0 Training loss: 1.2105 Explore P: 0.7202\n",
            "Episode: 169 Total reward: 11.0 Training loss: 30.7670 Explore P: 0.7195\n",
            "Episode: 170 Total reward: 10.0 Training loss: 1.2876 Explore P: 0.7188\n",
            "Episode: 171 Total reward: 10.0 Training loss: 1.2639 Explore P: 0.7180\n",
            "Episode: 172 Total reward: 13.0 Training loss: 92.5318 Explore P: 0.7171\n",
            "Episode: 173 Total reward: 10.0 Training loss: 29.7207 Explore P: 0.7164\n",
            "Episode: 174 Total reward: 31.0 Training loss: 0.7989 Explore P: 0.7142\n",
            "Episode: 175 Total reward: 14.0 Training loss: 30.6738 Explore P: 0.7132\n",
            "Episode: 176 Total reward: 17.0 Training loss: 1.3824 Explore P: 0.7121\n",
            "Episode: 177 Total reward: 23.0 Training loss: 1.7252 Explore P: 0.7104\n",
            "Episode: 178 Total reward: 16.0 Training loss: 56.3320 Explore P: 0.7093\n",
            "Episode: 179 Total reward: 10.0 Training loss: 1.9412 Explore P: 0.7086\n",
            "Episode: 180 Total reward: 14.0 Training loss: 32.3022 Explore P: 0.7076\n",
            "Episode: 181 Total reward: 11.0 Training loss: 28.4927 Explore P: 0.7069\n",
            "Episode: 182 Total reward: 24.0 Training loss: 34.7375 Explore P: 0.7052\n",
            "Episode: 183 Total reward: 12.0 Training loss: 63.4096 Explore P: 0.7044\n",
            "Episode: 184 Total reward: 12.0 Training loss: 1.4830 Explore P: 0.7035\n",
            "Episode: 185 Total reward: 9.0 Training loss: 1.1270 Explore P: 0.7029\n",
            "Episode: 186 Total reward: 28.0 Training loss: 74.4842 Explore P: 0.7010\n",
            "Episode: 187 Total reward: 13.0 Training loss: 76.7367 Explore P: 0.7001\n",
            "Episode: 188 Total reward: 16.0 Training loss: 51.4975 Explore P: 0.6990\n",
            "Episode: 189 Total reward: 18.0 Training loss: 26.3398 Explore P: 0.6977\n",
            "Episode: 190 Total reward: 16.0 Training loss: 1.0675 Explore P: 0.6966\n",
            "Episode: 191 Total reward: 9.0 Training loss: 1.3336 Explore P: 0.6960\n",
            "Episode: 192 Total reward: 16.0 Training loss: 51.7997 Explore P: 0.6949\n",
            "Episode: 193 Total reward: 8.0 Training loss: 24.5053 Explore P: 0.6944\n",
            "Episode: 194 Total reward: 10.0 Training loss: 26.9870 Explore P: 0.6937\n",
            "Episode: 195 Total reward: 18.0 Training loss: 24.5958 Explore P: 0.6925\n",
            "Episode: 196 Total reward: 10.0 Training loss: 96.4036 Explore P: 0.6918\n",
            "Episode: 197 Total reward: 31.0 Training loss: 46.2355 Explore P: 0.6897\n",
            "Episode: 198 Total reward: 8.0 Training loss: 1.3932 Explore P: 0.6891\n",
            "Episode: 199 Total reward: 26.0 Training loss: 20.7566 Explore P: 0.6874\n",
            "Episode: 200 Total reward: 10.0 Training loss: 0.6417 Explore P: 0.6867\n",
            "Episode: 201 Total reward: 55.0 Training loss: 79.2376 Explore P: 0.6830\n",
            "Episode: 202 Total reward: 12.0 Training loss: 52.1661 Explore P: 0.6822\n",
            "Episode: 203 Total reward: 10.0 Training loss: 1.1350 Explore P: 0.6815\n",
            "Episode: 204 Total reward: 15.0 Training loss: 22.1963 Explore P: 0.6805\n",
            "Episode: 205 Total reward: 28.0 Training loss: 27.9344 Explore P: 0.6786\n",
            "Episode: 206 Total reward: 20.0 Training loss: 41.6453 Explore P: 0.6773\n",
            "Episode: 207 Total reward: 10.0 Training loss: 20.8889 Explore P: 0.6766\n",
            "Episode: 208 Total reward: 12.0 Training loss: 21.8600 Explore P: 0.6758\n",
            "Episode: 209 Total reward: 13.0 Training loss: 1.3870 Explore P: 0.6749\n",
            "Episode: 210 Total reward: 15.0 Training loss: 78.5367 Explore P: 0.6739\n",
            "Episode: 211 Total reward: 23.0 Training loss: 23.1748 Explore P: 0.6724\n",
            "Episode: 212 Total reward: 9.0 Training loss: 1.1124 Explore P: 0.6718\n",
            "Episode: 213 Total reward: 10.0 Training loss: 1.1577 Explore P: 0.6712\n",
            "Episode: 214 Total reward: 15.0 Training loss: 34.7979 Explore P: 0.6702\n",
            "Episode: 215 Total reward: 12.0 Training loss: 0.7173 Explore P: 0.6694\n",
            "Episode: 216 Total reward: 13.0 Training loss: 1.1614 Explore P: 0.6685\n",
            "Episode: 217 Total reward: 17.0 Training loss: 16.0195 Explore P: 0.6674\n",
            "Episode: 218 Total reward: 17.0 Training loss: 36.9473 Explore P: 0.6663\n",
            "Episode: 219 Total reward: 9.0 Training loss: 1.0569 Explore P: 0.6657\n",
            "Episode: 220 Total reward: 20.0 Training loss: 49.1787 Explore P: 0.6644\n",
            "Episode: 221 Total reward: 11.0 Training loss: 1.0274 Explore P: 0.6637\n",
            "Episode: 222 Total reward: 12.0 Training loss: 20.8572 Explore P: 0.6629\n",
            "Episode: 223 Total reward: 27.0 Training loss: 1.0892 Explore P: 0.6611\n",
            "Episode: 224 Total reward: 13.0 Training loss: 1.3017 Explore P: 0.6603\n",
            "Episode: 225 Total reward: 20.0 Training loss: 1.4380 Explore P: 0.6590\n",
            "Episode: 226 Total reward: 21.0 Training loss: 37.9829 Explore P: 0.6576\n",
            "Episode: 227 Total reward: 13.0 Training loss: 0.5911 Explore P: 0.6568\n",
            "Episode: 228 Total reward: 24.0 Training loss: 20.5709 Explore P: 0.6552\n",
            "Episode: 229 Total reward: 41.0 Training loss: 37.1943 Explore P: 0.6526\n",
            "Episode: 230 Total reward: 9.0 Training loss: 0.9718 Explore P: 0.6520\n",
            "Episode: 231 Total reward: 14.0 Training loss: 43.2521 Explore P: 0.6511\n",
            "Episode: 232 Total reward: 10.0 Training loss: 28.1916 Explore P: 0.6505\n",
            "Episode: 233 Total reward: 13.0 Training loss: 11.3958 Explore P: 0.6496\n",
            "Episode: 234 Total reward: 22.0 Training loss: 1.0326 Explore P: 0.6482\n",
            "Episode: 235 Total reward: 16.0 Training loss: 1.6170 Explore P: 0.6472\n",
            "Episode: 236 Total reward: 13.0 Training loss: 13.2159 Explore P: 0.6464\n",
            "Episode: 237 Total reward: 9.0 Training loss: 23.8095 Explore P: 0.6458\n",
            "Episode: 238 Total reward: 12.0 Training loss: 0.6128 Explore P: 0.6451\n",
            "Episode: 239 Total reward: 17.0 Training loss: 23.5242 Explore P: 0.6440\n",
            "Episode: 240 Total reward: 17.0 Training loss: 15.2855 Explore P: 0.6429\n",
            "Episode: 241 Total reward: 12.0 Training loss: 37.0282 Explore P: 0.6421\n",
            "Episode: 242 Total reward: 22.0 Training loss: 15.6120 Explore P: 0.6407\n",
            "Episode: 243 Total reward: 12.0 Training loss: 13.4886 Explore P: 0.6400\n",
            "Episode: 244 Total reward: 11.0 Training loss: 18.1602 Explore P: 0.6393\n",
            "Episode: 245 Total reward: 39.0 Training loss: 18.4176 Explore P: 0.6368\n",
            "Episode: 246 Total reward: 40.0 Training loss: 10.5097 Explore P: 0.6343\n",
            "Episode: 247 Total reward: 14.0 Training loss: 52.4644 Explore P: 0.6335\n",
            "Episode: 248 Total reward: 9.0 Training loss: 19.9999 Explore P: 0.6329\n",
            "Episode: 249 Total reward: 15.0 Training loss: 14.0963 Explore P: 0.6320\n",
            "Episode: 250 Total reward: 7.0 Training loss: 17.2202 Explore P: 0.6315\n",
            "Episode: 251 Total reward: 14.0 Training loss: 14.8765 Explore P: 0.6307\n",
            "Episode: 252 Total reward: 10.0 Training loss: 36.1209 Explore P: 0.6301\n",
            "Episode: 253 Total reward: 24.0 Training loss: 63.7004 Explore P: 0.6286\n",
            "Episode: 254 Total reward: 11.0 Training loss: 11.6570 Explore P: 0.6279\n",
            "Episode: 255 Total reward: 38.0 Training loss: 26.4897 Explore P: 0.6255\n",
            "Episode: 256 Total reward: 30.0 Training loss: 16.0917 Explore P: 0.6237\n",
            "Episode: 257 Total reward: 12.0 Training loss: 1.1071 Explore P: 0.6230\n",
            "Episode: 258 Total reward: 70.0 Training loss: 13.4338 Explore P: 0.6187\n",
            "Episode: 259 Total reward: 20.0 Training loss: 1.1999 Explore P: 0.6175\n",
            "Episode: 260 Total reward: 85.0 Training loss: 1.5753 Explore P: 0.6123\n",
            "Episode: 261 Total reward: 33.0 Training loss: 9.5001 Explore P: 0.6103\n",
            "Episode: 262 Total reward: 30.0 Training loss: 26.9273 Explore P: 0.6085\n",
            "Episode: 263 Total reward: 27.0 Training loss: 31.6013 Explore P: 0.6069\n",
            "Episode: 264 Total reward: 64.0 Training loss: 11.1932 Explore P: 0.6031\n",
            "Episode: 265 Total reward: 21.0 Training loss: 12.4319 Explore P: 0.6019\n",
            "Episode: 266 Total reward: 44.0 Training loss: 1.4242 Explore P: 0.5993\n",
            "Episode: 267 Total reward: 33.0 Training loss: 28.7996 Explore P: 0.5973\n",
            "Episode: 268 Total reward: 30.0 Training loss: 0.8314 Explore P: 0.5956\n",
            "Episode: 269 Total reward: 21.0 Training loss: 1.4728 Explore P: 0.5944\n",
            "Episode: 270 Total reward: 33.0 Training loss: 8.8014 Explore P: 0.5924\n",
            "Episode: 271 Total reward: 73.0 Training loss: 16.3455 Explore P: 0.5882\n",
            "Episode: 272 Total reward: 33.0 Training loss: 13.7253 Explore P: 0.5863\n",
            "Episode: 273 Total reward: 15.0 Training loss: 1.4307 Explore P: 0.5854\n",
            "Episode: 274 Total reward: 58.0 Training loss: 0.7338 Explore P: 0.5821\n",
            "Episode: 275 Total reward: 55.0 Training loss: 6.7574 Explore P: 0.5790\n",
            "Episode: 276 Total reward: 70.0 Training loss: 17.4589 Explore P: 0.5750\n",
            "Episode: 277 Total reward: 58.0 Training loss: 13.4837 Explore P: 0.5717\n",
            "Episode: 278 Total reward: 27.0 Training loss: 20.6833 Explore P: 0.5702\n",
            "Episode: 279 Total reward: 30.0 Training loss: 21.6093 Explore P: 0.5685\n",
            "Episode: 280 Total reward: 61.0 Training loss: 1.0634 Explore P: 0.5651\n",
            "Episode: 281 Total reward: 52.0 Training loss: 10.2776 Explore P: 0.5623\n",
            "Episode: 282 Total reward: 75.0 Training loss: 0.8775 Explore P: 0.5581\n",
            "Episode: 283 Total reward: 22.0 Training loss: 1.1274 Explore P: 0.5569\n",
            "Episode: 284 Total reward: 24.0 Training loss: 1.9035 Explore P: 0.5556\n",
            "Episode: 285 Total reward: 62.0 Training loss: 17.7567 Explore P: 0.5522\n",
            "Episode: 286 Total reward: 35.0 Training loss: 12.8522 Explore P: 0.5503\n",
            "Episode: 287 Total reward: 21.0 Training loss: 50.2386 Explore P: 0.5492\n",
            "Episode: 288 Total reward: 55.0 Training loss: 34.1020 Explore P: 0.5463\n",
            "Episode: 289 Total reward: 17.0 Training loss: 14.5307 Explore P: 0.5453\n",
            "Episode: 290 Total reward: 44.0 Training loss: 8.4514 Explore P: 0.5430\n",
            "Episode: 291 Total reward: 23.0 Training loss: 14.9649 Explore P: 0.5418\n",
            "Episode: 292 Total reward: 26.0 Training loss: 11.3587 Explore P: 0.5404\n",
            "Episode: 293 Total reward: 30.0 Training loss: 1.1461 Explore P: 0.5388\n",
            "Episode: 294 Total reward: 40.0 Training loss: 37.2502 Explore P: 0.5367\n",
            "Episode: 295 Total reward: 23.0 Training loss: 17.8710 Explore P: 0.5355\n",
            "Episode: 296 Total reward: 36.0 Training loss: 1.3388 Explore P: 0.5336\n",
            "Episode: 297 Total reward: 54.0 Training loss: 0.9418 Explore P: 0.5308\n",
            "Episode: 298 Total reward: 25.0 Training loss: 1.4553 Explore P: 0.5295\n",
            "Episode: 299 Total reward: 45.0 Training loss: 26.2144 Explore P: 0.5271\n",
            "Episode: 300 Total reward: 91.0 Training loss: 16.1980 Explore P: 0.5225\n",
            "Episode: 301 Total reward: 37.0 Training loss: 1.7507 Explore P: 0.5206\n",
            "Episode: 302 Total reward: 65.0 Training loss: 15.2586 Explore P: 0.5173\n",
            "Episode: 303 Total reward: 42.0 Training loss: 19.3157 Explore P: 0.5151\n",
            "Episode: 304 Total reward: 44.0 Training loss: 5.2111 Explore P: 0.5129\n",
            "Episode: 305 Total reward: 52.0 Training loss: 1.8589 Explore P: 0.5103\n",
            "Episode: 306 Total reward: 16.0 Training loss: 63.6814 Explore P: 0.5095\n",
            "Episode: 307 Total reward: 47.0 Training loss: 1.7497 Explore P: 0.5072\n",
            "Episode: 308 Total reward: 77.0 Training loss: 49.7624 Explore P: 0.5033\n",
            "Episode: 309 Total reward: 42.0 Training loss: 1.2128 Explore P: 0.5013\n",
            "Episode: 310 Total reward: 38.0 Training loss: 21.7526 Explore P: 0.4994\n",
            "Episode: 311 Total reward: 52.0 Training loss: 28.0803 Explore P: 0.4969\n",
            "Episode: 312 Total reward: 39.0 Training loss: 2.0072 Explore P: 0.4950\n",
            "Episode: 313 Total reward: 36.0 Training loss: 26.1746 Explore P: 0.4932\n",
            "Episode: 314 Total reward: 77.0 Training loss: 1.4533 Explore P: 0.4895\n",
            "Episode: 315 Total reward: 22.0 Training loss: 54.2184 Explore P: 0.4885\n",
            "Episode: 316 Total reward: 54.0 Training loss: 43.8683 Explore P: 0.4859\n",
            "Episode: 317 Total reward: 51.0 Training loss: 1.5632 Explore P: 0.4835\n",
            "Episode: 318 Total reward: 81.0 Training loss: 2.6080 Explore P: 0.4797\n",
            "Episode: 319 Total reward: 22.0 Training loss: 46.1858 Explore P: 0.4786\n",
            "Episode: 320 Total reward: 59.0 Training loss: 2.8024 Explore P: 0.4759\n",
            "Episode: 321 Total reward: 32.0 Training loss: 73.4399 Explore P: 0.4744\n",
            "Episode: 322 Total reward: 39.0 Training loss: 2.7651 Explore P: 0.4726\n",
            "Episode: 323 Total reward: 53.0 Training loss: 34.9465 Explore P: 0.4701\n",
            "Episode: 324 Total reward: 84.0 Training loss: 11.4468 Explore P: 0.4663\n",
            "Episode: 325 Total reward: 48.0 Training loss: 1.8015 Explore P: 0.4641\n",
            "Episode: 326 Total reward: 24.0 Training loss: 2.7940 Explore P: 0.4630\n",
            "Episode: 327 Total reward: 39.0 Training loss: 1.7490 Explore P: 0.4612\n",
            "Episode: 328 Total reward: 30.0 Training loss: 49.1477 Explore P: 0.4599\n",
            "Episode: 329 Total reward: 37.0 Training loss: 68.1348 Explore P: 0.4582\n",
            "Episode: 330 Total reward: 47.0 Training loss: 38.0437 Explore P: 0.4561\n",
            "Episode: 331 Total reward: 57.0 Training loss: 27.9602 Explore P: 0.4536\n",
            "Episode: 332 Total reward: 61.0 Training loss: 23.9416 Explore P: 0.4509\n",
            "Episode: 333 Total reward: 69.0 Training loss: 2.0996 Explore P: 0.4479\n",
            "Episode: 334 Total reward: 70.0 Training loss: 1.5870 Explore P: 0.4448\n",
            "Episode: 335 Total reward: 77.0 Training loss: 37.1588 Explore P: 0.4415\n",
            "Episode: 336 Total reward: 48.0 Training loss: 49.4464 Explore P: 0.4394\n",
            "Episode: 337 Total reward: 134.0 Training loss: 69.0667 Explore P: 0.4337\n",
            "Episode: 338 Total reward: 28.0 Training loss: 1.4951 Explore P: 0.4325\n",
            "Episode: 339 Total reward: 40.0 Training loss: 58.6428 Explore P: 0.4308\n",
            "Episode: 340 Total reward: 82.0 Training loss: 38.0252 Explore P: 0.4274\n",
            "Episode: 341 Total reward: 33.0 Training loss: 34.1498 Explore P: 0.4260\n",
            "Episode: 342 Total reward: 32.0 Training loss: 1.6187 Explore P: 0.4247\n",
            "Episode: 343 Total reward: 66.0 Training loss: 1.6318 Explore P: 0.4220\n",
            "Episode: 344 Total reward: 46.0 Training loss: 5.1066 Explore P: 0.4201\n",
            "Episode: 345 Total reward: 49.0 Training loss: 2.6330 Explore P: 0.4181\n",
            "Episode: 346 Total reward: 71.0 Training loss: 21.7055 Explore P: 0.4152\n",
            "Episode: 347 Total reward: 22.0 Training loss: 65.7690 Explore P: 0.4143\n",
            "Episode: 348 Total reward: 49.0 Training loss: 2.5861 Explore P: 0.4123\n",
            "Episode: 349 Total reward: 31.0 Training loss: 1.8206 Explore P: 0.4111\n",
            "Episode: 350 Total reward: 11.0 Training loss: 20.8202 Explore P: 0.4106\n",
            "Episode: 351 Total reward: 78.0 Training loss: 1.8334 Explore P: 0.4075\n",
            "Episode: 352 Total reward: 39.0 Training loss: 106.1917 Explore P: 0.4060\n",
            "Episode: 353 Total reward: 53.0 Training loss: 1.5326 Explore P: 0.4039\n",
            "Episode: 354 Total reward: 39.0 Training loss: 3.0711 Explore P: 0.4023\n",
            "Episode: 355 Total reward: 49.0 Training loss: 34.3839 Explore P: 0.4004\n",
            "Episode: 356 Total reward: 39.0 Training loss: 2.0876 Explore P: 0.3989\n",
            "Episode: 357 Total reward: 50.0 Training loss: 41.5542 Explore P: 0.3970\n",
            "Episode: 358 Total reward: 37.0 Training loss: 99.3944 Explore P: 0.3955\n",
            "Episode: 359 Total reward: 60.0 Training loss: 2.9315 Explore P: 0.3932\n",
            "Episode: 360 Total reward: 32.0 Training loss: 28.2111 Explore P: 0.3920\n",
            "Episode: 361 Total reward: 36.0 Training loss: 37.0358 Explore P: 0.3906\n",
            "Episode: 362 Total reward: 97.0 Training loss: 17.0472 Explore P: 0.3869\n",
            "Episode: 363 Total reward: 40.0 Training loss: 2.1984 Explore P: 0.3854\n",
            "Episode: 364 Total reward: 51.0 Training loss: 27.7267 Explore P: 0.3835\n",
            "Episode: 365 Total reward: 31.0 Training loss: 35.8148 Explore P: 0.3824\n",
            "Episode: 366 Total reward: 39.0 Training loss: 2.1717 Explore P: 0.3809\n",
            "Episode: 367 Total reward: 66.0 Training loss: 1.5007 Explore P: 0.3785\n",
            "Episode: 368 Total reward: 63.0 Training loss: 3.2551 Explore P: 0.3762\n",
            "Episode: 369 Total reward: 131.0 Training loss: 41.5723 Explore P: 0.3714\n",
            "Episode: 370 Total reward: 73.0 Training loss: 0.9879 Explore P: 0.3688\n",
            "Episode: 371 Total reward: 41.0 Training loss: 89.9714 Explore P: 0.3673\n",
            "Episode: 372 Total reward: 80.0 Training loss: 22.9063 Explore P: 0.3645\n",
            "Episode: 373 Total reward: 65.0 Training loss: 46.7131 Explore P: 0.3622\n",
            "Episode: 374 Total reward: 87.0 Training loss: 2.0196 Explore P: 0.3591\n",
            "Episode: 375 Total reward: 44.0 Training loss: 41.3276 Explore P: 0.3576\n",
            "Episode: 376 Total reward: 77.0 Training loss: 71.4877 Explore P: 0.3549\n",
            "Episode: 377 Total reward: 94.0 Training loss: 2.7828 Explore P: 0.3517\n",
            "Episode: 378 Total reward: 39.0 Training loss: 1.5645 Explore P: 0.3504\n",
            "Episode: 379 Total reward: 64.0 Training loss: 1.8053 Explore P: 0.3482\n",
            "Episode: 380 Total reward: 64.0 Training loss: 3.4886 Explore P: 0.3460\n",
            "Episode: 381 Total reward: 101.0 Training loss: 2.3251 Explore P: 0.3427\n",
            "Episode: 382 Total reward: 63.0 Training loss: 47.5402 Explore P: 0.3406\n",
            "Episode: 383 Total reward: 55.0 Training loss: 3.0924 Explore P: 0.3388\n",
            "Episode: 384 Total reward: 57.0 Training loss: 2.5770 Explore P: 0.3369\n",
            "Episode: 385 Total reward: 44.0 Training loss: 110.2104 Explore P: 0.3354\n",
            "Episode: 386 Total reward: 38.0 Training loss: 17.9994 Explore P: 0.3342\n",
            "Episode: 387 Total reward: 60.0 Training loss: 48.4219 Explore P: 0.3323\n",
            "Episode: 388 Total reward: 39.0 Training loss: 38.1749 Explore P: 0.3310\n",
            "Episode: 389 Total reward: 81.0 Training loss: 61.7736 Explore P: 0.3284\n",
            "Episode: 390 Total reward: 70.0 Training loss: 56.5882 Explore P: 0.3262\n",
            "Episode: 391 Total reward: 50.0 Training loss: 55.4615 Explore P: 0.3246\n",
            "Episode: 392 Total reward: 71.0 Training loss: 2.8701 Explore P: 0.3224\n",
            "Episode: 393 Total reward: 107.0 Training loss: 78.2240 Explore P: 0.3191\n",
            "Episode: 394 Total reward: 37.0 Training loss: 9.4504 Explore P: 0.3179\n",
            "Episode: 395 Total reward: 115.0 Training loss: 1.6399 Explore P: 0.3144\n",
            "Episode: 396 Total reward: 53.0 Training loss: 2.2932 Explore P: 0.3128\n",
            "Episode: 397 Total reward: 54.0 Training loss: 81.6228 Explore P: 0.3112\n",
            "Episode: 398 Total reward: 74.0 Training loss: 1.7213 Explore P: 0.3090\n",
            "Episode: 399 Total reward: 166.0 Training loss: 67.1458 Explore P: 0.3040\n",
            "Episode: 400 Total reward: 121.0 Training loss: 96.5188 Explore P: 0.3005\n",
            "Episode: 401 Total reward: 84.0 Training loss: 2.4113 Explore P: 0.2981\n",
            "Episode: 402 Total reward: 102.0 Training loss: 2.1577 Explore P: 0.2951\n",
            "Episode: 403 Total reward: 106.0 Training loss: 43.3399 Explore P: 0.2921\n",
            "Episode: 404 Total reward: 88.0 Training loss: 0.5996 Explore P: 0.2897\n",
            "Episode: 405 Total reward: 76.0 Training loss: 40.1396 Explore P: 0.2876\n",
            "Episode: 406 Total reward: 76.0 Training loss: 78.9788 Explore P: 0.2854\n",
            "Episode: 407 Total reward: 94.0 Training loss: 1.0676 Explore P: 0.2829\n",
            "Episode: 408 Total reward: 92.0 Training loss: 80.6400 Explore P: 0.2804\n",
            "Episode: 409 Total reward: 102.0 Training loss: 1.7999 Explore P: 0.2776\n",
            "Episode: 410 Total reward: 49.0 Training loss: 2.9675 Explore P: 0.2763\n",
            "Episode: 411 Total reward: 71.0 Training loss: 2.6866 Explore P: 0.2744\n",
            "Episode: 412 Total reward: 59.0 Training loss: 7.6934 Explore P: 0.2729\n",
            "Episode: 413 Total reward: 108.0 Training loss: 1.7065 Explore P: 0.2701\n",
            "Episode: 414 Total reward: 63.0 Training loss: 37.0909 Explore P: 0.2684\n",
            "Episode: 415 Total reward: 79.0 Training loss: 2.5418 Explore P: 0.2664\n",
            "Episode: 416 Total reward: 75.0 Training loss: 2.3099 Explore P: 0.2645\n",
            "Episode: 417 Total reward: 60.0 Training loss: 113.6062 Explore P: 0.2630\n",
            "Episode: 418 Total reward: 46.0 Training loss: 1.8862 Explore P: 0.2618\n",
            "Episode: 419 Total reward: 51.0 Training loss: 2.2425 Explore P: 0.2605\n",
            "Episode: 420 Total reward: 136.0 Training loss: 82.8118 Explore P: 0.2571\n",
            "Episode: 421 Total reward: 77.0 Training loss: 72.6714 Explore P: 0.2552\n",
            "Episode: 422 Total reward: 135.0 Training loss: 1.9356 Explore P: 0.2519\n",
            "Episode: 423 Total reward: 41.0 Training loss: 2.1658 Explore P: 0.2510\n",
            "Episode: 424 Total reward: 48.0 Training loss: 2.2257 Explore P: 0.2498\n",
            "Episode: 425 Total reward: 147.0 Training loss: 52.2292 Explore P: 0.2463\n",
            "Episode: 426 Total reward: 79.0 Training loss: 65.1753 Explore P: 0.2444\n",
            "Episode: 427 Total reward: 93.0 Training loss: 4.7322 Explore P: 0.2423\n",
            "Episode: 428 Total reward: 139.0 Training loss: 0.8366 Explore P: 0.2391\n",
            "Episode: 429 Total reward: 161.0 Training loss: 54.8100 Explore P: 0.2354\n",
            "Episode: 430 Total reward: 130.0 Training loss: 2.3849 Explore P: 0.2325\n",
            "Episode: 431 Total reward: 139.0 Training loss: 1.7638 Explore P: 0.2294\n",
            "Episode: 432 Total reward: 148.0 Training loss: 151.8096 Explore P: 0.2262\n",
            "Episode: 433 Total reward: 199.0 Training loss: 75.9947 Explore P: 0.2219\n",
            "Episode: 434 Total reward: 92.0 Training loss: 95.2724 Explore P: 0.2200\n",
            "Episode: 435 Total reward: 53.0 Training loss: 145.0503 Explore P: 0.2189\n",
            "Episode: 436 Total reward: 199.0 Training loss: 76.7402 Explore P: 0.2148\n",
            "Episode: 437 Total reward: 96.0 Training loss: 2.7048 Explore P: 0.2128\n",
            "Episode: 438 Total reward: 132.0 Training loss: 2.1907 Explore P: 0.2102\n",
            "Episode: 439 Total reward: 83.0 Training loss: 3.0076 Explore P: 0.2085\n",
            "Episode: 440 Total reward: 82.0 Training loss: 3.1954 Explore P: 0.2069\n",
            "Episode: 441 Total reward: 143.0 Training loss: 3.3337 Explore P: 0.2041\n",
            "Episode: 442 Total reward: 109.0 Training loss: 4.6519 Explore P: 0.2020\n",
            "Episode: 443 Total reward: 199.0 Training loss: 1.3024 Explore P: 0.1982\n",
            "Episode: 444 Total reward: 152.0 Training loss: 81.1534 Explore P: 0.1954\n",
            "Episode: 445 Total reward: 191.0 Training loss: 1.4195 Explore P: 0.1919\n",
            "Episode: 446 Total reward: 126.0 Training loss: 1.0390 Explore P: 0.1896\n",
            "Episode: 447 Total reward: 152.0 Training loss: 1.0276 Explore P: 0.1869\n",
            "Episode: 448 Total reward: 142.0 Training loss: 2.1584 Explore P: 0.1844\n",
            "Episode: 449 Total reward: 152.0 Training loss: 1.1081 Explore P: 0.1817\n",
            "Episode: 450 Total reward: 163.0 Training loss: 94.1320 Explore P: 0.1790\n",
            "Episode: 451 Total reward: 134.0 Training loss: 0.9782 Explore P: 0.1767\n",
            "Episode: 452 Total reward: 199.0 Training loss: 2.8010 Explore P: 0.1734\n",
            "Episode: 453 Total reward: 103.0 Training loss: 2.5966 Explore P: 0.1718\n",
            "Episode: 454 Total reward: 199.0 Training loss: 2.1341 Explore P: 0.1686\n",
            "Episode: 455 Total reward: 153.0 Training loss: 2.5393 Explore P: 0.1662\n",
            "Episode: 456 Total reward: 199.0 Training loss: 1.1290 Explore P: 0.1631\n",
            "Episode: 457 Total reward: 199.0 Training loss: 2.2340 Explore P: 0.1601\n",
            "Episode: 458 Total reward: 199.0 Training loss: 0.9186 Explore P: 0.1571\n",
            "Episode: 459 Total reward: 199.0 Training loss: 1.0961 Explore P: 0.1542\n",
            "Episode: 460 Total reward: 136.0 Training loss: 0.9871 Explore P: 0.1523\n",
            "Episode: 461 Total reward: 199.0 Training loss: 1.2711 Explore P: 0.1495\n",
            "Episode: 462 Total reward: 199.0 Training loss: 2.0102 Explore P: 0.1467\n",
            "Episode: 463 Total reward: 199.0 Training loss: 152.4172 Explore P: 0.1440\n",
            "Episode: 464 Total reward: 199.0 Training loss: 1.3790 Explore P: 0.1414\n",
            "Episode: 465 Total reward: 199.0 Training loss: 1.1663 Explore P: 0.1388\n",
            "Episode: 466 Total reward: 199.0 Training loss: 1.5767 Explore P: 0.1363\n",
            "Episode: 467 Total reward: 99.0 Training loss: 352.5474 Explore P: 0.1350\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}